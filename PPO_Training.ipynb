{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training for Stock Trading\n",
    "\n",
    "This notebook implements Proximal Policy Optimization (PPO) for training a reinforcement learning agent to trade stocks.\n",
    "\n",
    "## Overview\n",
    "- **Environment**: Stock trading environment using historical price data\n",
    "- **Algorithm**: PPO (Proximal Policy Optimization)\n",
    "- **Model**: Actor-Critic network (MLP or CNN)\n",
    "- **Features**: Volume, extra features (volatility, ATR-like), chronological train/val split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T17:52:49.221616Z",
     "start_time": "2025-12-28T17:52:49.208262Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Import our stock trading environment package\n",
    "from stock_trading_env import (\n",
    "    StocksEnv,\n",
    "    load_many_from_dir,\n",
    "    split_many_by_ratio,\n",
    "    Actions,\n",
    ")\n",
    "\n",
    "# ---------- Reproducibility ----------\n",
    "def set_seed(seed: int, deterministic: bool = True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Pick a default seed (match your CLI default if you had one)\n",
    "SEED = 0\n",
    "set_seed(SEED, deterministic=True)\n",
    "\n",
    "# ---------- Device ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Output directories ----------\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "os.makedirs(\"saves\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"Seed: {SEED} (deterministic=True)\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "Seed: 0 (deterministic=True)\n",
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available: False\n",
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PPO Model Definitions\n",
    "\n",
    "Define the Actor-Critic networks for PPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T17:52:56.306904Z",
     "start_time": "2025-12-28T17:52:56.294835Z"
    }
   },
   "source": [
    "# Cell 2: PPO Actor-Critic models (DISCRETE actions) — matches earlier repo logic\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ActorCriticMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    PPO Actor-Critic for DISCRETE actions (vector observation).\n",
    "\n",
    "    Input:  x (B, obs_dim)\n",
    "    Output: logits (B, n_actions), value (B,)\n",
    "      - logits are unnormalized scores for torch.distributions.Categorical(logits=logits)\n",
    "      - value is V(s) for GAE/advantage estimation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, n_actions: int, hidden: int = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Linear(hidden, n_actions)\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Orthogonal init is common for PPO\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        # Smaller gain for the final policy head helps early training stability\n",
    "        nn.init.orthogonal_(self.policy_head.weight, gain=0.01)\n",
    "        nn.init.constant_(self.policy_head.bias, 0.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = self.shared(x)\n",
    "        logits = self.policy_head(z)\n",
    "        value = self.value_head(z).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class ActorCriticConv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    PPO Actor-Critic for State1D observations (CNN over time bars).\n",
    "\n",
    "    Expected env State1D shape (C, T) where:\n",
    "      - C = (3 + (1 if volumes else 0)) + 2  -> typically 6 if volumes=True\n",
    "      - T = bars_count (e.g., 10)\n",
    "\n",
    "    Input:  x (B, C, T)\n",
    "    Output: logits (B, n_actions), value (B,)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions: int, bars_count: int, volumes: bool = True, hidden: int = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Match env.State1D channel logic:\n",
    "        # base channels: 3 + (1 if volumes else 0)\n",
    "        # +2 for (have_position, unrealized_return)\n",
    "        in_channels = (3 + (1 if volumes else 0)) + 2\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # -> (B, 64 * bars_count)\n",
    "        )\n",
    "\n",
    "        conv_out = 64 * bars_count\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(conv_out, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Linear(hidden, n_actions)\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        nn.init.orthogonal_(self.policy_head.weight, gain=0.01)\n",
    "        nn.init.constant_(self.policy_head.bias, 0.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        feat = self.conv(x)\n",
    "        z = self.shared(feat)\n",
    "        logits = self.policy_head(z)\n",
    "        value = self.value_head(z).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "print(\"✓ PPO models defined (MLP + Conv1D)\")\n",
    "print(\"  - Use ActorCriticMLP for vector obs\")\n",
    "print(\"  - Use ActorCriticConv1D only if env returns State1D (C,T), e.g. (6, bars)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PPO models defined (MLP + Conv1D)\n",
      "  - Use ActorCriticMLP for vector obs\n",
      "  - Use ActorCriticConv1D only if env returns State1D (C,T), e.g. (6, bars)\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO Buffer and Utilities\n",
    "\n",
    "Define the rollout buffer for collecting experience and computing GAE advantages.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T18:03:19.258982Z",
     "start_time": "2025-12-28T18:03:19.239092Z"
    }
   },
   "source": [
    "# Cell 3 (updated): RolloutBuffer + PPOBatch\n",
    "# Fixes/robustness:\n",
    "# - Safer dtype/device handling\n",
    "# - Enforces add() obs shape\n",
    "# - Handles truncated/terminated properly via done float\n",
    "# - Supports 1D (vector) and 2D (C,T) observations cleanly\n",
    "# - Compute GAE in float32, normalize advantages safely\n",
    "# - Optional pin_memory for faster CPU->GPU transfer\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PPOBatch:\n",
    "    \"\"\"One minibatch used during PPO updates.\"\"\"\n",
    "    obs: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "    old_logprobs: torch.Tensor\n",
    "    advantages: torch.Tensor\n",
    "    returns: torch.Tensor\n",
    "    old_values: torch.Tensor\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Fixed-length rollout buffer for PPO.\n",
    "\n",
    "    Stores:\n",
    "      obs[t], action[t], reward[t], done[t], value[t], logprob[t]\n",
    "\n",
    "    After calling compute_gae():\n",
    "      advantages[t], returns[t] are filled.\n",
    "\n",
    "    Notes:\n",
    "      - done should be True when episode ended for ANY reason:\n",
    "        done = terminated OR truncated (Gymnasium).\n",
    "      - obs can be vector (obs_dim,) or State1D (C,T).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_shape: Union[int, Tuple[int, ...]],\n",
    "        size: int,\n",
    "        device: Union[str, torch.device] = \"cpu\",\n",
    "        obs_dtype: np.dtype = np.float32,\n",
    "        pin_memory: bool = False,\n",
    "    ):\n",
    "        if isinstance(obs_shape, int):\n",
    "            obs_shape = (obs_shape,)\n",
    "        self.obs_shape = tuple(obs_shape)\n",
    "        self.size = int(size)\n",
    "        self.device = torch.device(device) if not isinstance(device, torch.device) else device\n",
    "        self.pin_memory = bool(pin_memory) and (self.device.type == \"cuda\")\n",
    "\n",
    "        # Storage (numpy)\n",
    "        self.obs = np.zeros((self.size, *self.obs_shape), dtype=obs_dtype)\n",
    "        self.actions = np.zeros((self.size,), dtype=np.int64)\n",
    "        self.rewards = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((self.size,), dtype=np.float32)   # 1.0 if done else 0.0\n",
    "        self.values = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.logprobs = np.zeros((self.size,), dtype=np.float32)\n",
    "\n",
    "        # Computed after rollout\n",
    "        self.advantages = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.returns = np.zeros((self.size,), dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "        value: float,\n",
    "        logprob: float,\n",
    "    ):\n",
    "        \"\"\"Add one transition.\"\"\"\n",
    "        if self.ptr >= self.size:\n",
    "            raise RuntimeError(\"RolloutBuffer is full. Call reset() before adding more.\")\n",
    "\n",
    "        obs_arr = np.asarray(obs, dtype=self.obs.dtype)\n",
    "\n",
    "        # Enforce shape to catch silent bugs early (common in notebooks)\n",
    "        if obs_arr.shape != self.obs_shape:\n",
    "            raise ValueError(\n",
    "                f\"Obs shape mismatch: got {obs_arr.shape}, expected {self.obs_shape}. \"\n",
    "                \"Check env output (vector vs State1D) and model input.\"\n",
    "            )\n",
    "\n",
    "        self.obs[self.ptr] = obs_arr\n",
    "        self.actions[self.ptr] = int(action)\n",
    "        self.rewards[self.ptr] = float(reward)\n",
    "        self.dones[self.ptr] = 1.0 if bool(done) else 0.0\n",
    "        self.values[self.ptr] = float(value)\n",
    "        self.logprobs[self.ptr] = float(logprob)\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.full = (self.ptr == self.size)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_gae(\n",
    "        self,\n",
    "        last_value: float,\n",
    "        gamma: float = 0.99,\n",
    "        lam: float = 0.95,\n",
    "        normalize_adv: bool = True,\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute GAE(lambda) advantages and returns.\n",
    "\n",
    "        last_value should be V(s_T) for the *last obs after the rollout*,\n",
    "        used to bootstrap if the last transition was not terminal.\n",
    "        \"\"\"\n",
    "        if not self.full:\n",
    "            raise RuntimeError(\"RolloutBuffer not full. Collect 'size' steps before compute_gae().\")\n",
    "\n",
    "        last_value = float(last_value)\n",
    "\n",
    "        adv = 0.0\n",
    "        for t in reversed(range(self.size)):\n",
    "            done = self.dones[t]\n",
    "            mask = 1.0 - done  # 0 if terminal else 1\n",
    "\n",
    "            next_value = last_value if (t == self.size - 1) else float(self.values[t + 1])\n",
    "            delta = float(self.rewards[t]) + gamma * next_value * mask - float(self.values[t])\n",
    "\n",
    "            adv = delta + gamma * lam * mask * adv\n",
    "            self.advantages[t] = adv\n",
    "\n",
    "        # returns target for critic\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "        if normalize_adv:\n",
    "            m = float(self.advantages.mean())\n",
    "            s = float(self.advantages.std())\n",
    "            self.advantages = (self.advantages - m) / (s + eps)\n",
    "\n",
    "    def get_batches(self, batch_size: int, shuffle: bool = True) -> Iterator[PPOBatch]:\n",
    "        \"\"\"Yield mini-batches as torch tensors.\"\"\"\n",
    "        if not self.full:\n",
    "            raise RuntimeError(\"RolloutBuffer not full. Collect rollout before batching.\")\n",
    "\n",
    "        if batch_size <= 0:\n",
    "            raise ValueError(\"batch_size must be > 0\")\n",
    "\n",
    "        idxs = np.arange(self.size)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "\n",
    "        for start in range(0, self.size, batch_size):\n",
    "            b_idx = idxs[start : start + batch_size]\n",
    "\n",
    "            # CPU tensors first (optionally pinned), then move to device\n",
    "            obs_cpu = torch.from_numpy(self.obs[b_idx]).float()\n",
    "            if self.pin_memory:\n",
    "                obs_cpu = obs_cpu.pin_memory()\n",
    "            obs_t = obs_cpu.to(self.device, non_blocking=self.pin_memory)\n",
    "\n",
    "            actions_t = torch.from_numpy(self.actions[b_idx]).long().to(self.device)\n",
    "            old_logp_t = torch.from_numpy(self.logprobs[b_idx]).float().to(self.device)\n",
    "            adv_t = torch.from_numpy(self.advantages[b_idx]).float().to(self.device)\n",
    "            ret_t = torch.from_numpy(self.returns[b_idx]).float().to(self.device)\n",
    "            old_v_t = torch.from_numpy(self.values[b_idx]).float().to(self.device)\n",
    "\n",
    "            yield PPOBatch(\n",
    "                obs=obs_t,\n",
    "                actions=actions_t,\n",
    "                old_logprobs=old_logp_t,\n",
    "                advantages=adv_t,\n",
    "                returns=ret_t,\n",
    "                old_values=old_v_t,\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"✓ PPO buffer defined (updated/robust)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PPO buffer defined (updated/robust)\n"
     ]
    }
   ],
   "execution_count": 24
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T17:58:18.434371Z",
     "start_time": "2025-12-28T17:58:18.410851Z"
    }
   },
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "@torch.no_grad()\n",
    "def policy_act(model, obs, device: torch.device, greedy: bool = False):\n",
    "    \"\"\"\n",
    "    Sample (or greedy-select) action from current policy.\n",
    "    Returns: action(int), logprob(float), value(float)\n",
    "    Works for both:\n",
    "      - vector obs: (obs_dim,)\n",
    "      - State1D obs: (C, T)\n",
    "    \"\"\"\n",
    "    obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    logits, value = model(obs_t)\n",
    "    dist = Categorical(logits=logits)\n",
    "\n",
    "    if greedy:\n",
    "        action_t = torch.argmax(logits, dim=1)\n",
    "    else:\n",
    "        action_t = dist.sample()\n",
    "\n",
    "    logprob_t = dist.log_prob(action_t)\n",
    "    return int(action_t.item()), float(logprob_t.item()), float(value.item())\n",
    "\n",
    "\n",
    "def explained_variance(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"1 - Var[y_true - y_pred] / Var[y_true]. Diagnostic for critic fit.\"\"\"\n",
    "    var_y = float(np.var(y_true))\n",
    "    if var_y < 1e-12:\n",
    "        return 0.0\n",
    "    return float(1.0 - np.var(y_true - y_pred) / (var_y + 1e-12))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_run_ppo(env, model, episodes: int = 50, device=\"cpu\", greedy: bool = True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a PPO policy on the given environment.\n",
    "    Returns mean metrics across episodes.\n",
    "\n",
    "    IMPORTANT: This assumes the env executes actions at OPEN(t+1), consistent with your repo env.\n",
    "    \"\"\"\n",
    "    device = torch.device(device) if not isinstance(device, torch.device) else device\n",
    "\n",
    "    # Ensure deterministic eval behavior (dropout/bn etc.)\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    base_env = env.unwrapped if hasattr(env, \"unwrapped\") else env\n",
    "    st = getattr(base_env, \"_state\", None)\n",
    "    if st is None:\n",
    "        # restore mode before raising\n",
    "        if was_training:\n",
    "            model.train()\n",
    "        raise AttributeError(\"validation_run_ppo expected env.unwrapped to have attribute '_state'.\")\n",
    "\n",
    "    stats = {\n",
    "        \"episode_reward\": [],\n",
    "        \"episode_steps\": [],\n",
    "        \"num_trades\": [],\n",
    "        \"win_rate\": [],\n",
    "        \"avg_trade_return\": [],\n",
    "        \"avg_hold_steps\": [],\n",
    "        \"sum_trade_return\": [],\n",
    "    }\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        # Manual trade tracking (OPEN(t+1) execution)\n",
    "        in_pos = False\n",
    "        entry_price = None\n",
    "        hold_steps = 0\n",
    "        trade_returns = []\n",
    "        trade_hold_steps = []\n",
    "\n",
    "        while not done:\n",
    "            action, _lp, _v = policy_act(model, obs, device=device, greedy=greedy)\n",
    "\n",
    "            # Safer enum conversion\n",
    "            try:\n",
    "                act_enum = Actions(action)\n",
    "            except Exception:\n",
    "                # If action is invalid, treat as \"Skip\" (safe fallback)\n",
    "                act_enum = Actions.Skip\n",
    "\n",
    "            # Access state after reset/step (base_env._state can change reference)\n",
    "            st = base_env._state\n",
    "            next_idx = st._offset + 1\n",
    "\n",
    "            exec_open = None\n",
    "            if 0 <= next_idx < st._prices.open.shape[0]:\n",
    "                exec_open = float(st._prices.open[next_idx])\n",
    "\n",
    "            prev_have_pos_env = bool(st.have_position)\n",
    "\n",
    "            # Bookkeeping BEFORE env.step(), using OPEN(t+1)\n",
    "            if exec_open is not None:\n",
    "                if act_enum == Actions.Buy and not in_pos:\n",
    "                    in_pos = True\n",
    "                    entry_price = exec_open\n",
    "                    hold_steps = 0\n",
    "                elif act_enum == Actions.Close and in_pos:\n",
    "                    if entry_price is not None and entry_price > 0:\n",
    "                        tr = 100.0 * (exec_open - entry_price) / entry_price\n",
    "                    else:\n",
    "                        tr = 0.0\n",
    "                    trade_returns.append(tr)\n",
    "                    trade_hold_steps.append(hold_steps)\n",
    "                    in_pos = False\n",
    "                    entry_price = None\n",
    "                    hold_steps = 0\n",
    "\n",
    "            # Step environment\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = bool(terminated or truncated)\n",
    "            total_reward += float(reward)\n",
    "            steps += 1\n",
    "\n",
    "            # Detect forced close by env (stop-loss / done close, etc.)\n",
    "            now_have_pos_env = bool(base_env._state.have_position)\n",
    "            if in_pos and prev_have_pos_env and (not now_have_pos_env) and act_enum != Actions.Close:\n",
    "                # Use exec_open if available (OPEN(t+1)), else fallback to current close\n",
    "                exit_price = exec_open if exec_open is not None else float(base_env._state._cur_close())\n",
    "                if entry_price is not None and entry_price > 0:\n",
    "                    tr = 100.0 * (exit_price - entry_price) / entry_price\n",
    "                else:\n",
    "                    tr = 0.0\n",
    "                trade_returns.append(tr)\n",
    "                trade_hold_steps.append(hold_steps)\n",
    "                in_pos = False\n",
    "                entry_price = None\n",
    "                hold_steps = 0\n",
    "\n",
    "            if in_pos:\n",
    "                hold_steps += 1\n",
    "\n",
    "        # If episode ends while holding, close at last close for reporting\n",
    "        if in_pos and entry_price is not None and entry_price > 0:\n",
    "            last_close = float(base_env._state._cur_close())\n",
    "            tr = 100.0 * (last_close - entry_price) / entry_price\n",
    "            trade_returns.append(tr)\n",
    "            trade_hold_steps.append(hold_steps)\n",
    "\n",
    "        # Episode metrics\n",
    "        stats[\"episode_reward\"].append(total_reward)\n",
    "        stats[\"episode_steps\"].append(steps)\n",
    "\n",
    "        n_trades = len(trade_returns)\n",
    "        stats[\"num_trades\"].append(float(n_trades))\n",
    "\n",
    "        if n_trades > 0:\n",
    "            wins = sum(1 for x in trade_returns if x > 0.0)\n",
    "            stats[\"win_rate\"].append(float(wins / n_trades))\n",
    "            stats[\"avg_trade_return\"].append(float(np.mean(trade_returns)))\n",
    "            stats[\"avg_hold_steps\"].append(float(np.mean(trade_hold_steps)))\n",
    "            stats[\"sum_trade_return\"].append(float(np.sum(trade_returns)))\n",
    "        else:\n",
    "            stats[\"win_rate\"].append(0.0)\n",
    "            stats[\"avg_trade_return\"].append(0.0)\n",
    "            stats[\"avg_hold_steps\"].append(0.0)\n",
    "            stats[\"sum_trade_return\"].append(0.0)\n",
    "\n",
    "    # Restore model mode\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return {k: float(np.mean(v)) for k, v in stats.items()}\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined (updated/robust)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined (updated/robust)\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T18:00:09.542328Z",
     "start_time": "2025-12-28T18:00:09.530952Z"
    }
   },
   "source": [
    "# ===== Training Configuration (matched to your CLI run) =====\n",
    "config = {\n",
    "    # Data\n",
    "    \"data_dir\": \"yf_data\",\n",
    "    \"run_name\": \"ppo_aapl_final\",   # CLI: -r ppo_aapl_final\n",
    "    \"seed\": 0,                      # set to whatever your train_ppo.py used (0 is common)\n",
    "\n",
    "    # Device\n",
    "    \"use_cuda\": True,               # CLI: --cuda\n",
    "\n",
    "    # PPO Hyperparameters\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_eps\": 0.2,\n",
    "    \"lr\": 3e-4,                     # CLI: --lr 3e-4\n",
    "    \"rollout_steps\": 1024,          # CLI: --rollout_steps 1024\n",
    "    \"minibatch\": 256,               # CLI: --minibatch 256\n",
    "    \"epochs\": 5,                    # CLI: --epochs 5\n",
    "    \"value_coef\": 0.5,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"target_kl\": 0.02,\n",
    "\n",
    "    # Environment\n",
    "    \"bars\": 10,                     # CLI: --bars 10\n",
    "    \"volumes\": True,                # CLI: --volumes\n",
    "    \"extra_features\": True,         # env default True; keep True unless your script changed it\n",
    "    \"reward_mode\": \"close_pnl\",     # CLI: --reward_mode close_pnl\n",
    "    \"state_1d\": False,              # keep False unless you used a CNN before\n",
    "    \"time_limit\": 1000,             # CLI: --time_limit 1000\n",
    "\n",
    "    # Data Split\n",
    "    \"split\": True,                  # CLI: --split\n",
    "    \"train_ratio\": 0.8,             # CLI: --train_ratio 0.8\n",
    "    \"min_train\": 300,               # CLI: --min_train 300\n",
    "    \"min_val\": 300,                 # CLI: --min_val 300\n",
    "\n",
    "    # Training Control\n",
    "    \"max_rollouts\": 500,            # CLI: --max_rollouts 500\n",
    "    \"total_steps\": 10_000_000,      # CLI: --total_steps 10000000\n",
    "\n",
    "    # Validation & Checkpointing\n",
    "    \"val_every_rollouts\": 10,       # CLI: --val_every_rollouts 10\n",
    "    \"save_every_rollouts\": 10,      # CLI: --save_every_rollouts 10\n",
    "    \"early_stop\": True,             # CLI: --early_stop\n",
    "    \"patience\": 20,                 # CLI: --patience 20\n",
    "    \"min_rollouts\": 50,             # CLI: --min_rollouts 50\n",
    "    \"min_delta\": 0.01,              # CLI: --min_delta 0.01\n",
    "}\n",
    "\n",
    "# Set device (matches --cuda behavior)\n",
    "device = torch.device(\"cuda\" if (config[\"use_cuda\"] and torch.cuda.is_available()) else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "print(\"✓ Configuration set\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "✓ Configuration set\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Prepare Data\n",
    "\n",
    "Load stock price data and split into train/validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T18:02:42.983594Z",
     "start_time": "2025-12-28T18:02:42.929692Z"
    }
   },
   "source": [
    "# Load all price data\n",
    "prices_all = load_many_from_dir(config[\"data_dir\"])\n",
    "print(f\"Loaded {len(prices_all)} instruments: {list(prices_all.keys())}\")\n",
    "\n",
    "# Optional: if you want to exactly match an AAPL-only run\n",
    "# TARGET = \"AAPL\"\n",
    "# if TARGET in prices_all:\n",
    "#     prices_all = {TARGET: prices_all[TARGET]}\n",
    "#     print(f\"Filtered to single instrument: {TARGET}\")\n",
    "\n",
    "# Split into train/validation (chronological, no leakage)\n",
    "if config[\"split\"]:\n",
    "    prices_train, prices_val = split_many_by_ratio(\n",
    "        prices_all,\n",
    "        train_ratio=config[\"train_ratio\"],\n",
    "        min_train=config[\"min_train\"],\n",
    "        min_val=config[\"min_val\"],\n",
    "    )\n",
    "    print(f\"Train instruments: {len(prices_train)}\")\n",
    "    print(f\"Validation instruments: {len(prices_val)}\")\n",
    "else:\n",
    "    prices_train = prices_all\n",
    "    prices_val = prices_all\n",
    "    print(\"No split: using same data for train and validation (in-sample)\")\n",
    "\n",
    "# Sanity checks (strongly recommended)\n",
    "for k in prices_train.keys():\n",
    "    tr_len = len(prices_train[k].close)\n",
    "    va_len = len(prices_val[k].close)\n",
    "    if config[\"split\"]:\n",
    "        assert tr_len >= config[\"min_train\"], f\"{k}: train too short ({tr_len})\"\n",
    "        assert va_len >= config[\"min_val\"], f\"{k}: val too short ({va_len})\"\n",
    "    print(f\"  {k}: train_len={tr_len}, val_len={va_len}\")\n",
    "\n",
    "print(\"✓ Data loaded and split\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 instruments: ['AAPL_1d_2020-01-01_to_2025-12-23', 'TSLA_1d_2020-01-01_to_2025-11-30', 'TSLA_1d_2020-01-01_to_2025-12-31']\n",
      "Train instruments: 2\n",
      "Validation instruments: 2\n",
      "  AAPL_1d_2020-01-01_to_2025-12-23: train_len=1201, val_len=301\n",
      "  TSLA_1d_2020-01-01_to_2025-11-30: train_len=1186, val_len=300\n",
      "✓ Data loaded and split\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Environments\n",
    "\n",
    "Set up training and validation environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T18:05:00.980184Z",
     "start_time": "2025-12-28T18:05:00.970645Z"
    }
   },
   "source": [
    "# Create training environment (match CLI behavior)\n",
    "env_train_base = StocksEnv(\n",
    "    prices_train,\n",
    "    bars_count=config[\"bars\"],\n",
    "    volumes=config[\"volumes\"],\n",
    "    extra_features=config[\"extra_features\"],\n",
    "    reward_mode=config[\"reward_mode\"],\n",
    "    state_1d=config[\"state_1d\"],\n",
    "    # IMPORTANT: do NOT override reset_on_close/reward_on_close unless you *know*\n",
    "    # you used non-default values in train_ppo.py\n",
    "    # reset_on_close=True (default in env)\n",
    "    # reward_on_close=True (default in env)\n",
    ")\n",
    "env_train = gym.wrappers.TimeLimit(env_train_base, max_episode_steps=config[\"time_limit\"])\n",
    "\n",
    "# Create validation environment (use same settings + same TimeLimit)\n",
    "env_val_base = StocksEnv(\n",
    "    prices_val,\n",
    "    bars_count=config[\"bars\"],\n",
    "    volumes=config[\"volumes\"],\n",
    "    extra_features=config[\"extra_features\"],\n",
    "    reward_mode=config[\"reward_mode\"],\n",
    "    state_1d=config[\"state_1d\"],\n",
    ")\n",
    "env_val = gym.wrappers.TimeLimit(env_val_base, max_episode_steps=config[\"time_limit\"])\n",
    "\n",
    "obs_shape = env_train.observation_space.shape\n",
    "n_actions = env_train.action_space.n\n",
    "\n",
    "print(f\"Observation space: {obs_shape}\")\n",
    "print(f\"Action space: {n_actions} actions\")\n",
    "print(\"✓ Environments created\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: (45,)\n",
      "Action space: 3 actions\n",
      "✓ Environments created\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Model and Optimizer\n",
    "\n",
    "Create the PPO model and optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T18:07:28.997593Z",
     "start_time": "2025-12-28T18:07:28.965275Z"
    }
   },
   "source": [
    "# Cell 8: Build model + optimizer + TensorBoard writer (compatible with updated Cell 2)\n",
    "\n",
    "# Infer observation shape from env\n",
    "# - Vector state: obs_shape = (obs_dim,)\n",
    "# - State1D:      obs_shape = (C, T)\n",
    "if config[\"state_1d\"]:\n",
    "    C, T = obs_shape  # (channels, time bars)\n",
    "\n",
    "    # Our updated ActorCriticConv1D computes in_channels internally from `volumes`\n",
    "    # (C should equal (3 + (1 if volumes else 0)) + 2, usually 6 if volumes=True)\n",
    "    expected_C = (3 + (1 if config[\"volumes\"] else 0)) + 2\n",
    "    if C != expected_C:\n",
    "        raise ValueError(\n",
    "            f\"State1D channel mismatch: env returned C={C}, expected C={expected_C}. \"\n",
    "            \"Check env volumes/state_1d settings.\"\n",
    "        )\n",
    "\n",
    "    model = ActorCriticConv1D(\n",
    "        n_actions=n_actions,\n",
    "        bars_count=T,\n",
    "        volumes=config[\"volumes\"],\n",
    "        hidden=256,\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Created Conv1D model: C={C}, T={T}\")\n",
    "else:\n",
    "    obs_dim = obs_shape[0]\n",
    "    model = ActorCriticMLP(obs_dim=obs_dim, n_actions=n_actions, hidden=256).to(device)\n",
    "    print(f\"Created MLP model: obs_dim={obs_dim}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "# TensorBoard writer (clean run directory like scripts usually do)\n",
    "run_dir = os.path.join(\"runs\", config[\"run_name\"])\n",
    "writer = SummaryWriter(log_dir=run_dir)\n",
    "print(f\"TensorBoard log dir: {run_dir}\")\n",
    "print(\"Model params:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "print(\"✓ Model and optimizer initialized\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MLP model: obs_dim=45\n",
      "TensorBoard log dir: runs\\ppo_aapl_final\n",
      "Model params: 78596\n",
      "✓ Model and optimizer initialized\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Main PPO training loop: collect rollouts, compute advantages, update policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T18:15:45.825011Z",
     "start_time": "2025-12-28T18:09:09.633613Z"
    }
   },
   "source": [
    "# ===== Training bookkeeping =====\n",
    "obs, info = env_train.reset(seed=config[\"seed\"])\n",
    "episode_reward = 0.0\n",
    "episode_steps = 0\n",
    "episode_count = 0\n",
    "\n",
    "global_step = 0\n",
    "rollout_idx = 0\n",
    "t0 = time.time()\n",
    "\n",
    "best_val_reward = -1e9\n",
    "no_improve = 0\n",
    "\n",
    "print(f\"[PPO] device={device} obs_shape={obs_shape} actions={n_actions}\")\n",
    "print(f\"[PPO] reward_mode={config['reward_mode']} volumes={config['volumes']} extra_features={config['extra_features']}\")\n",
    "print(f\"[PPO] split={config['split']} max_rollouts={config['max_rollouts']} early_stop={config['early_stop']}\")\n",
    "print(f\"[PPO] logs: runs/  checkpoints: saves/\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Main training loop\n",
    "while (global_step < config[\"total_steps\"]) and (rollout_idx < config[\"max_rollouts\"]):\n",
    "    rollout_idx += 1\n",
    "    buf = RolloutBuffer(obs_shape=obs_shape, size=config[\"rollout_steps\"], device=device)\n",
    "\n",
    "    # ===== Collect Rollout =====\n",
    "    for _ in range(config[\"rollout_steps\"]):\n",
    "        global_step += 1\n",
    "\n",
    "        action, logprob, value = policy_act(model, obs, device=device, greedy=False)\n",
    "        next_obs, reward, terminated, truncated, info = env_train.step(action)\n",
    "\n",
    "        episode_done = bool(terminated or truncated)\n",
    "\n",
    "        # IMPORTANT FIX: mask GAE on ANY episode end (terminated OR truncated)\n",
    "        buf_done = episode_done\n",
    "\n",
    "        buf.add(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            reward=float(reward),\n",
    "            done=buf_done,\n",
    "            value=value,\n",
    "            logprob=logprob,\n",
    "        )\n",
    "\n",
    "        episode_reward += float(reward)\n",
    "        episode_steps += 1\n",
    "        obs = next_obs\n",
    "\n",
    "        if episode_done:\n",
    "            episode_count += 1\n",
    "            writer.add_scalar(\"train/episode_reward\", episode_reward, global_step)\n",
    "            writer.add_scalar(\"train/episode_steps\", episode_steps, global_step)\n",
    "\n",
    "            obs, info = env_train.reset()\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = 0\n",
    "\n",
    "        if global_step >= config[\"total_steps\"]:\n",
    "            break\n",
    "\n",
    "    # If we didn't fill the buffer (e.g., hit total_steps), stop cleanly\n",
    "    if not buf.full:\n",
    "        print(f\"[PPO] stopping: reached total_steps mid-rollout (filled {buf.ptr}/{buf.size}).\")\n",
    "        break\n",
    "\n",
    "    # Rollout reward heartbeat\n",
    "    roll_sum = float(buf.rewards.sum())\n",
    "    roll_mean = float(buf.rewards.mean())\n",
    "    writer.add_scalar(\"train/rollout_reward_sum\", roll_sum, global_step)\n",
    "    writer.add_scalar(\"train/rollout_reward_mean\", roll_mean, global_step)\n",
    "\n",
    "    # Bootstrap last value for GAE (only used if last transition not terminal)\n",
    "    with torch.no_grad():\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        _, last_v = model(obs_t)\n",
    "        last_value = float(last_v.item())\n",
    "\n",
    "    buf.compute_gae(\n",
    "        last_value=last_value,\n",
    "        gamma=config[\"gamma\"],\n",
    "        lam=config[\"gae_lambda\"],\n",
    "        normalize_adv=True,\n",
    "    )\n",
    "\n",
    "    # ===== PPO Update =====\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    approx_kls = []\n",
    "    clipfracs = []\n",
    "\n",
    "    for _epoch in range(config[\"epochs\"]):\n",
    "        for batch in buf.get_batches(batch_size=config[\"minibatch\"], shuffle=True):\n",
    "            logits, values = model(batch.obs)\n",
    "            dist = Categorical(logits=logits)\n",
    "\n",
    "            new_logp = dist.log_prob(batch.actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            # PPO ratio\n",
    "            ratio = torch.exp(new_logp - batch.old_logprobs)\n",
    "\n",
    "            # Policy loss (clipped surrogate)\n",
    "            unclipped = ratio * batch.advantages\n",
    "            clipped = torch.clamp(ratio, 1.0 - config[\"clip_eps\"], 1.0 + config[\"clip_eps\"]) * batch.advantages\n",
    "            loss_pi = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "            # Value loss\n",
    "            loss_v = (batch.returns - values).pow(2).mean()\n",
    "\n",
    "            # Total loss\n",
    "            loss = loss_pi + config[\"value_coef\"] * loss_v - config[\"entropy_coef\"] * entropy\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Better PPO-style KL approximation (no abs)\n",
    "                approx_kl = float((batch.old_logprobs - new_logp).mean().item())\n",
    "                clipfrac = float((torch.abs(ratio - 1.0) > config[\"clip_eps\"]).float().mean().item())\n",
    "\n",
    "            policy_losses.append(float(loss_pi.item()))\n",
    "            value_losses.append(float(loss_v.item()))\n",
    "            entropies.append(float(entropy.item()))\n",
    "            approx_kls.append(approx_kl)\n",
    "            clipfracs.append(clipfrac)\n",
    "\n",
    "        # Early stop PPO epoch loop if KL too big\n",
    "        if config[\"target_kl\"] > 0 and approx_kls and (np.mean(approx_kls) > config[\"target_kl\"]):\n",
    "            break\n",
    "\n",
    "    # ===== Logging =====\n",
    "    fps = global_step / max(1e-9, (time.time() - t0))\n",
    "    writer.add_scalar(\"ppo/policy_loss\", float(np.mean(policy_losses) if policy_losses else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/value_loss\", float(np.mean(value_losses) if value_losses else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/entropy\", float(np.mean(entropies) if entropies else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/approx_kl\", float(np.mean(approx_kls) if approx_kls else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/clipfrac\", float(np.mean(clipfracs) if clipfracs else 0.0), global_step)\n",
    "    writer.add_scalar(\"train/fps\", float(fps), global_step)\n",
    "    writer.add_scalar(\"train/episodes\", float(episode_count), global_step)\n",
    "\n",
    "    # Value function explained variance\n",
    "    ev = explained_variance(np.asarray(buf.values), np.asarray(buf.returns))\n",
    "    writer.add_scalar(\"ppo/explained_variance\", ev, global_step)\n",
    "\n",
    "    # Console heartbeat\n",
    "    print(\n",
    "        f\"[rollout {rollout_idx}] step={global_step} \"\n",
    "        f\"roll_sum={roll_sum:.3f} roll_mean={roll_mean:.5f} \"\n",
    "        f\"pi_loss={np.mean(policy_losses) if policy_losses else 0.0:.4f} \"\n",
    "        f\"v_loss={np.mean(value_losses) if value_losses else 0.0:.4f} \"\n",
    "        f\"ent={np.mean(entropies) if entropies else 0.0:.3f} \"\n",
    "        f\"kl={np.mean(approx_kls) if approx_kls else 0.0:.4f} \"\n",
    "        f\"clip={np.mean(clipfracs) if clipfracs else 0.0:.3f} \"\n",
    "        f\"eps_done={episode_count} fps={fps:.1f}\"\n",
    "    )\n",
    "\n",
    "    # ===== Validation + Best Model Saving + Early Stop =====\n",
    "    if config[\"val_every_rollouts\"] > 0 and (rollout_idx % config[\"val_every_rollouts\"] == 0):\n",
    "        val = validation_run_ppo(env_val, model, episodes=20, device=device, greedy=True)\n",
    "\n",
    "        for k, v in val.items():\n",
    "            writer.add_scalar(\"val/\" + k, v, global_step)\n",
    "\n",
    "        print(f\"  [val] {val}\")\n",
    "\n",
    "        cur_val = float(val.get(\"episode_reward\", -1e9))\n",
    "        if cur_val > best_val_reward + config[\"min_delta\"]:\n",
    "            best_val_reward = cur_val\n",
    "            no_improve = 0\n",
    "            best_path = os.path.join(\"saves\", f\"ppo_{config['run_name']}_best.pt\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  [best] new best val episode_reward={best_val_reward:.4f} -> {best_path}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if config[\"early_stop\"] and (rollout_idx >= config[\"min_rollouts\"]) and (no_improve >= config[\"patience\"]):\n",
    "            print(f\"[PPO] early stopping: no val improvement for {no_improve} validations.\")\n",
    "            break\n",
    "\n",
    "    # ===== Save Periodic Checkpoint =====\n",
    "    if config[\"save_every_rollouts\"] > 0 and (rollout_idx % config[\"save_every_rollouts\"] == 0):\n",
    "        ckpt_path = os.path.join(\"saves\", f\"ppo_{config['run_name']}_rollout{rollout_idx}.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"  [save] {ckpt_path}\")\n",
    "\n",
    "    # Hard stop condition\n",
    "    if config[\"max_rollouts\"] and rollout_idx >= config[\"max_rollouts\"]:\n",
    "        print(\"[PPO] reached max_rollouts, stopping.\")\n",
    "        break\n",
    "\n",
    "if global_step >= config[\"total_steps\"]:\n",
    "    print(f\"[done] reached total_steps={config['total_steps']} at rollout={rollout_idx}\")\n",
    "elif rollout_idx >= config[\"max_rollouts\"]:\n",
    "    print(f\"[done] reached max_rollouts={config['max_rollouts']} at step={global_step}\")\n",
    "\n",
    "writer.close()\n",
    "print(\"\\n✓ Training completed!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO] device=cpu obs_shape=(45,) actions=3\n",
      "[PPO] reward_mode=close_pnl volumes=True extra_features=True\n",
      "[PPO] split=True max_rollouts=500 early_stop=True\n",
      "[PPO] logs: runs/  checkpoints: saves/\n",
      "============================================================\n",
      "[rollout 1] step=1024 roll_sum=51.012 roll_mean=0.04982 pi_loss=-0.0002 v_loss=59.3748 ent=1.099 kl=-0.0000 clip=0.000 eps_done=1 fps=845.6\n",
      "[rollout 2] step=2048 roll_sum=0.895 roll_mean=0.00087 pi_loss=-0.0004 v_loss=29.6163 ent=1.099 kl=-0.0001 clip=0.000 eps_done=2 fps=929.8\n",
      "[rollout 3] step=3072 roll_sum=39.754 roll_mean=0.03882 pi_loss=-0.0002 v_loss=13.4781 ent=1.098 kl=-0.0000 clip=0.000 eps_done=3 fps=970.3\n",
      "[rollout 4] step=4096 roll_sum=-49.380 roll_mean=-0.04822 pi_loss=-0.0000 v_loss=35.7440 ent=1.098 kl=0.0003 clip=0.000 eps_done=6 fps=985.3\n",
      "[rollout 5] step=5120 roll_sum=71.934 roll_mean=0.07025 pi_loss=-0.0004 v_loss=55.9057 ent=1.098 kl=0.0001 clip=0.000 eps_done=8 fps=993.8\n",
      "[rollout 6] step=6144 roll_sum=105.418 roll_mean=0.10295 pi_loss=0.0001 v_loss=78.8664 ent=1.098 kl=0.0002 clip=0.000 eps_done=11 fps=972.9\n",
      "[rollout 7] step=7168 roll_sum=79.792 roll_mean=0.07792 pi_loss=-0.0010 v_loss=48.0479 ent=1.098 kl=0.0013 clip=0.000 eps_done=13 fps=980.4\n",
      "[rollout 8] step=8192 roll_sum=148.774 roll_mean=0.14529 pi_loss=0.0011 v_loss=62.4325 ent=1.095 kl=-0.0007 clip=0.000 eps_done=14 fps=989.1\n",
      "[rollout 9] step=9216 roll_sum=79.677 roll_mean=0.07781 pi_loss=-0.0004 v_loss=61.4929 ent=1.092 kl=0.0010 clip=0.000 eps_done=15 fps=997.2\n",
      "[rollout 10] step=10240 roll_sum=64.724 roll_mean=0.06321 pi_loss=-0.0049 v_loss=14.6789 ent=1.065 kl=0.0059 clip=0.074 eps_done=17 fps=1001.8\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 186.4, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [best] new best val episode_reward=0.0000 -> saves\\ppo_ppo_aapl_final_best.pt\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout10.pt\n",
      "[rollout 11] step=11264 roll_sum=-18.113 roll_mean=-0.01769 pi_loss=0.0006 v_loss=15.1926 ent=1.070 kl=0.0037 clip=0.002 eps_done=19 fps=820.1\n",
      "[rollout 12] step=12288 roll_sum=-9.161 roll_mean=-0.00895 pi_loss=-0.0001 v_loss=11.7363 ent=1.079 kl=0.0005 clip=0.000 eps_done=20 fps=831.2\n",
      "[rollout 13] step=13312 roll_sum=76.078 roll_mean=0.07429 pi_loss=-0.0000 v_loss=9.4541 ent=1.080 kl=0.0001 clip=0.000 eps_done=21 fps=848.3\n",
      "[rollout 14] step=14336 roll_sum=28.640 roll_mean=0.02797 pi_loss=-0.0005 v_loss=9.2738 ent=1.079 kl=0.0011 clip=0.000 eps_done=22 fps=861.8\n",
      "[rollout 15] step=15360 roll_sum=-14.295 roll_mean=-0.01396 pi_loss=-0.0026 v_loss=30.4595 ent=1.047 kl=0.0017 clip=0.000 eps_done=25 fps=864.2\n",
      "[rollout 16] step=16384 roll_sum=0.283 roll_mean=0.00028 pi_loss=-0.0000 v_loss=48.3608 ent=1.002 kl=0.0016 clip=0.000 eps_done=28 fps=864.9\n",
      "[rollout 17] step=17408 roll_sum=87.916 roll_mean=0.08586 pi_loss=-0.0000 v_loss=49.5375 ent=0.998 kl=-0.0001 clip=0.000 eps_done=30 fps=864.4\n",
      "[rollout 18] step=18432 roll_sum=143.690 roll_mean=0.14032 pi_loss=0.0015 v_loss=54.5898 ent=0.949 kl=0.0006 clip=0.003 eps_done=31 fps=866.5\n",
      "[rollout 19] step=19456 roll_sum=-4.228 roll_mean=-0.00413 pi_loss=-0.0021 v_loss=61.5465 ent=0.925 kl=0.0013 clip=0.004 eps_done=33 fps=868.5\n",
      "[rollout 20] step=20480 roll_sum=95.542 roll_mean=0.09330 pi_loss=-0.0002 v_loss=79.9748 ent=0.965 kl=0.0002 clip=0.000 eps_done=35 fps=870.8\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 196.05, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout20.pt\n",
      "[rollout 21] step=21504 roll_sum=54.189 roll_mean=0.05292 pi_loss=-0.0021 v_loss=11.8027 ent=0.953 kl=0.0004 clip=0.022 eps_done=36 fps=779.1\n",
      "[rollout 22] step=22528 roll_sum=107.862 roll_mean=0.10533 pi_loss=0.0005 v_loss=39.2631 ent=0.900 kl=0.0003 clip=0.000 eps_done=39 fps=790.1\n",
      "[rollout 23] step=23552 roll_sum=-109.987 roll_mean=-0.10741 pi_loss=-0.0004 v_loss=70.8415 ent=0.868 kl=0.0016 clip=0.010 eps_done=40 fps=799.8\n",
      "[rollout 24] step=24576 roll_sum=80.189 roll_mean=0.07831 pi_loss=-0.0012 v_loss=19.8169 ent=0.895 kl=0.0005 clip=0.007 eps_done=42 fps=808.5\n",
      "[rollout 25] step=25600 roll_sum=84.692 roll_mean=0.08271 pi_loss=-0.0016 v_loss=75.0646 ent=0.882 kl=0.0019 clip=0.004 eps_done=43 fps=816.0\n",
      "[rollout 26] step=26624 roll_sum=-8.182 roll_mean=-0.00799 pi_loss=-0.0029 v_loss=59.8477 ent=0.879 kl=0.0016 clip=0.083 eps_done=44 fps=822.4\n",
      "[rollout 27] step=27648 roll_sum=34.857 roll_mean=0.03404 pi_loss=-0.0021 v_loss=12.0102 ent=0.911 kl=0.0007 clip=0.008 eps_done=46 fps=829.2\n",
      "[rollout 28] step=28672 roll_sum=-42.362 roll_mean=-0.04137 pi_loss=-0.0027 v_loss=33.2426 ent=0.849 kl=0.0034 clip=0.022 eps_done=48 fps=836.6\n",
      "[rollout 29] step=29696 roll_sum=42.255 roll_mean=0.04126 pi_loss=-0.0014 v_loss=10.9630 ent=0.815 kl=0.0011 clip=0.001 eps_done=50 fps=839.3\n",
      "[rollout 30] step=30720 roll_sum=53.843 roll_mean=0.05258 pi_loss=-0.0020 v_loss=11.8770 ent=0.781 kl=0.0031 clip=0.033 eps_done=51 fps=843.3\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 223.05, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout30.pt\n",
      "[rollout 31] step=31744 roll_sum=112.267 roll_mean=0.10964 pi_loss=-0.0023 v_loss=19.9358 ent=0.750 kl=0.0027 clip=0.024 eps_done=53 fps=780.2\n",
      "[rollout 32] step=32768 roll_sum=20.484 roll_mean=0.02000 pi_loss=-0.0008 v_loss=25.7113 ent=0.721 kl=0.0031 clip=0.019 eps_done=54 fps=787.7\n",
      "[rollout 33] step=33792 roll_sum=110.863 roll_mean=0.10826 pi_loss=-0.0025 v_loss=48.8528 ent=0.699 kl=0.0035 clip=0.023 eps_done=55 fps=794.0\n",
      "[rollout 34] step=34816 roll_sum=-32.465 roll_mean=-0.03170 pi_loss=-0.0014 v_loss=25.6998 ent=0.675 kl=0.0036 clip=0.013 eps_done=57 fps=799.7\n",
      "[rollout 35] step=35840 roll_sum=10.979 roll_mean=0.01072 pi_loss=-0.0039 v_loss=28.0536 ent=0.716 kl=0.0153 clip=0.152 eps_done=59 fps=805.9\n",
      "[rollout 36] step=36864 roll_sum=248.360 roll_mean=0.24254 pi_loss=0.0099 v_loss=123.0790 ent=0.767 kl=0.0066 clip=0.067 eps_done=60 fps=811.5\n",
      "[rollout 37] step=37888 roll_sum=1.584 roll_mean=0.00155 pi_loss=-0.0009 v_loss=51.6219 ent=0.764 kl=0.0000 clip=0.023 eps_done=61 fps=816.4\n",
      "[rollout 38] step=38912 roll_sum=179.145 roll_mean=0.17495 pi_loss=0.0053 v_loss=66.8887 ent=0.712 kl=0.0077 clip=0.050 eps_done=62 fps=821.7\n",
      "[rollout 39] step=39936 roll_sum=-63.980 roll_mean=-0.06248 pi_loss=-0.0004 v_loss=26.5860 ent=0.707 kl=0.0018 clip=0.022 eps_done=63 fps=826.4\n",
      "[rollout 40] step=40960 roll_sum=-32.876 roll_mean=-0.03211 pi_loss=-0.0032 v_loss=36.1880 ent=0.771 kl=0.0040 clip=0.078 eps_done=65 fps=831.2\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 196.75, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout40.pt\n",
      "[rollout 41] step=41984 roll_sum=129.673 roll_mean=0.12663 pi_loss=0.0002 v_loss=36.4919 ent=0.792 kl=0.0007 clip=0.021 eps_done=66 fps=790.5\n",
      "[rollout 42] step=43008 roll_sum=4.298 roll_mean=0.00420 pi_loss=-0.0028 v_loss=31.3607 ent=0.789 kl=0.0050 clip=0.036 eps_done=68 fps=795.6\n",
      "[rollout 43] step=44032 roll_sum=201.462 roll_mean=0.19674 pi_loss=0.0022 v_loss=52.4440 ent=0.713 kl=0.0038 clip=0.041 eps_done=69 fps=799.9\n",
      "[rollout 44] step=45056 roll_sum=-77.751 roll_mean=-0.07593 pi_loss=-0.0030 v_loss=31.0942 ent=0.713 kl=0.0055 clip=0.041 eps_done=72 fps=804.2\n",
      "[rollout 45] step=46080 roll_sum=21.978 roll_mean=0.02146 pi_loss=-0.0004 v_loss=12.7457 ent=0.767 kl=0.0007 clip=0.004 eps_done=74 fps=808.8\n",
      "[rollout 46] step=47104 roll_sum=29.294 roll_mean=0.02861 pi_loss=-0.0029 v_loss=7.5842 ent=0.821 kl=0.0008 clip=0.053 eps_done=75 fps=813.4\n",
      "[rollout 47] step=48128 roll_sum=13.831 roll_mean=0.01351 pi_loss=-0.0017 v_loss=23.8348 ent=0.778 kl=0.0018 clip=0.004 eps_done=79 fps=817.5\n",
      "[rollout 48] step=49152 roll_sum=63.843 roll_mean=0.06235 pi_loss=-0.0006 v_loss=23.1875 ent=0.828 kl=0.0002 clip=0.000 eps_done=81 fps=821.2\n",
      "[rollout 49] step=50176 roll_sum=14.243 roll_mean=0.01391 pi_loss=0.0003 v_loss=8.2058 ent=0.824 kl=0.0004 clip=0.004 eps_done=82 fps=825.5\n",
      "[rollout 50] step=51200 roll_sum=65.346 roll_mean=0.06381 pi_loss=0.0001 v_loss=9.0070 ent=0.843 kl=-0.0010 clip=0.003 eps_done=84 fps=829.3\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 183.4, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout50.pt\n",
      "[rollout 51] step=52224 roll_sum=99.233 roll_mean=0.09691 pi_loss=-0.0008 v_loss=18.3175 ent=0.857 kl=0.0018 clip=0.020 eps_done=87 fps=800.8\n",
      "[rollout 52] step=53248 roll_sum=93.685 roll_mean=0.09149 pi_loss=0.0009 v_loss=18.2118 ent=0.927 kl=0.0007 clip=0.000 eps_done=88 fps=804.7\n",
      "[rollout 53] step=54272 roll_sum=192.614 roll_mean=0.18810 pi_loss=0.0028 v_loss=55.2758 ent=0.907 kl=0.0050 clip=0.048 eps_done=92 fps=808.7\n",
      "[rollout 54] step=55296 roll_sum=16.961 roll_mean=0.01656 pi_loss=-0.0008 v_loss=28.8418 ent=0.874 kl=-0.0009 clip=0.033 eps_done=94 fps=812.5\n",
      "[rollout 55] step=56320 roll_sum=54.223 roll_mean=0.05295 pi_loss=-0.0006 v_loss=13.5344 ent=0.881 kl=0.0020 clip=0.000 eps_done=96 fps=816.1\n",
      "[rollout 56] step=57344 roll_sum=29.678 roll_mean=0.02898 pi_loss=-0.0001 v_loss=16.7717 ent=0.906 kl=0.0012 clip=0.000 eps_done=100 fps=819.9\n",
      "[rollout 57] step=58368 roll_sum=38.292 roll_mean=0.03739 pi_loss=-0.0015 v_loss=15.7489 ent=0.905 kl=0.0002 clip=0.000 eps_done=101 fps=823.4\n",
      "[rollout 58] step=59392 roll_sum=64.204 roll_mean=0.06270 pi_loss=-0.0026 v_loss=18.4141 ent=0.861 kl=0.0009 clip=0.015 eps_done=104 fps=826.9\n",
      "[rollout 59] step=60416 roll_sum=15.563 roll_mean=0.01520 pi_loss=-0.0004 v_loss=17.0326 ent=0.762 kl=-0.0027 clip=0.006 eps_done=105 fps=830.3\n",
      "[rollout 60] step=61440 roll_sum=-28.517 roll_mean=-0.02785 pi_loss=-0.0004 v_loss=13.8195 ent=0.749 kl=0.0004 clip=0.001 eps_done=107 fps=833.5\n",
      "  [val] {'episode_reward': 32.50636091734823, 'episode_steps': 187.45, 'num_trades': 1.0, 'win_rate': 0.9, 'avg_trade_return': 32.160086130958824, 'avg_hold_steps': 185.75, 'sum_trade_return': 32.160086130958824}\n",
      "  [best] new best val episode_reward=32.5064 -> saves\\ppo_ppo_aapl_final_best.pt\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout60.pt\n",
      "[rollout 61] step=62464 roll_sum=312.015 roll_mean=0.30470 pi_loss=0.0060 v_loss=80.1737 ent=0.787 kl=0.0007 clip=0.017 eps_done=108 fps=807.7\n",
      "[rollout 62] step=63488 roll_sum=1.844 roll_mean=0.00180 pi_loss=-0.0006 v_loss=22.0672 ent=0.860 kl=0.0007 clip=0.009 eps_done=110 fps=811.0\n",
      "[rollout 63] step=64512 roll_sum=259.676 roll_mean=0.25359 pi_loss=0.0029 v_loss=67.4938 ent=0.788 kl=0.0028 clip=0.013 eps_done=111 fps=814.3\n",
      "[rollout 64] step=65536 roll_sum=52.240 roll_mean=0.05102 pi_loss=-0.0007 v_loss=42.5502 ent=0.943 kl=0.0010 clip=0.019 eps_done=113 fps=816.5\n",
      "[rollout 65] step=66560 roll_sum=18.135 roll_mean=0.01771 pi_loss=-0.0020 v_loss=11.0584 ent=0.880 kl=0.0100 clip=0.046 eps_done=115 fps=818.5\n",
      "[rollout 66] step=67584 roll_sum=100.897 roll_mean=0.09853 pi_loss=-0.0040 v_loss=17.3447 ent=0.862 kl=0.0031 clip=0.033 eps_done=117 fps=821.3\n",
      "[rollout 67] step=68608 roll_sum=117.517 roll_mean=0.11476 pi_loss=0.0009 v_loss=50.8685 ent=0.898 kl=-0.0004 clip=0.012 eps_done=119 fps=824.5\n",
      "[rollout 68] step=69632 roll_sum=36.436 roll_mean=0.03558 pi_loss=-0.0005 v_loss=31.0305 ent=0.761 kl=0.0004 clip=0.000 eps_done=120 fps=827.5\n",
      "[rollout 69] step=70656 roll_sum=-33.569 roll_mean=-0.03278 pi_loss=-0.0002 v_loss=24.2313 ent=0.795 kl=-0.0003 clip=0.020 eps_done=121 fps=830.3\n",
      "[rollout 70] step=71680 roll_sum=67.178 roll_mean=0.06560 pi_loss=-0.0063 v_loss=20.8426 ent=0.852 kl=0.0054 clip=0.071 eps_done=124 fps=833.0\n",
      "  [val] {'episode_reward': 40.50451488252332, 'episode_steps': 197.25, 'num_trades': 1.0, 'win_rate': 1.0, 'avg_trade_return': 39.860574294966966, 'avg_hold_steps': 196.25, 'sum_trade_return': 39.860574294966966}\n",
      "  [best] new best val episode_reward=40.5045 -> saves\\ppo_ppo_aapl_final_best.pt\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout70.pt\n",
      "[rollout 71] step=72704 roll_sum=19.226 roll_mean=0.01877 pi_loss=-0.0022 v_loss=26.3876 ent=0.732 kl=0.0021 clip=0.016 eps_done=125 fps=809.4\n",
      "[rollout 72] step=73728 roll_sum=62.330 roll_mean=0.06087 pi_loss=0.0003 v_loss=10.4595 ent=0.771 kl=0.0009 clip=0.006 eps_done=127 fps=812.5\n",
      "[rollout 73] step=74752 roll_sum=73.956 roll_mean=0.07222 pi_loss=-0.0004 v_loss=12.1043 ent=0.807 kl=0.0022 clip=0.020 eps_done=128 fps=815.0\n",
      "[rollout 74] step=75776 roll_sum=60.347 roll_mean=0.05893 pi_loss=-0.0005 v_loss=22.2144 ent=0.734 kl=0.0014 clip=0.008 eps_done=129 fps=817.8\n",
      "[rollout 75] step=76800 roll_sum=-51.542 roll_mean=-0.05033 pi_loss=-0.0004 v_loss=17.2415 ent=0.674 kl=0.0007 clip=0.006 eps_done=130 fps=820.5\n",
      "[rollout 76] step=77824 roll_sum=45.483 roll_mean=0.04442 pi_loss=0.0035 v_loss=57.8236 ent=0.825 kl=0.0058 clip=0.035 eps_done=132 fps=823.3\n",
      "[rollout 77] step=78848 roll_sum=52.443 roll_mean=0.05121 pi_loss=-0.0006 v_loss=14.2561 ent=0.904 kl=0.0075 clip=0.081 eps_done=134 fps=825.8\n",
      "[rollout 78] step=79872 roll_sum=66.253 roll_mean=0.06470 pi_loss=-0.0027 v_loss=8.3799 ent=0.752 kl=0.0052 clip=0.048 eps_done=135 fps=828.3\n",
      "[rollout 79] step=80896 roll_sum=22.784 roll_mean=0.02225 pi_loss=-0.0005 v_loss=11.3971 ent=0.705 kl=0.0007 clip=0.001 eps_done=138 fps=830.8\n",
      "[rollout 80] step=81920 roll_sum=2.608 roll_mean=0.00255 pi_loss=-0.0025 v_loss=19.0200 ent=0.614 kl=0.0043 clip=0.016 eps_done=140 fps=833.3\n",
      "  [val] {'episode_reward': 34.27407161951119, 'episode_steps': 184.45, 'num_trades': 1.0, 'win_rate': 1.0, 'avg_trade_return': 33.69057907231674, 'avg_hold_steps': 183.45, 'sum_trade_return': 33.69057907231674}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout80.pt\n",
      "[rollout 81] step=82944 roll_sum=74.297 roll_mean=0.07256 pi_loss=-0.0007 v_loss=10.6037 ent=0.739 kl=-0.0002 clip=0.001 eps_done=142 fps=813.2\n",
      "[rollout 82] step=83968 roll_sum=147.357 roll_mean=0.14390 pi_loss=0.0038 v_loss=32.9653 ent=0.615 kl=0.0015 clip=0.019 eps_done=143 fps=815.6\n",
      "[rollout 83] step=84992 roll_sum=-8.278 roll_mean=-0.00808 pi_loss=-0.0004 v_loss=21.7512 ent=0.573 kl=0.0006 clip=0.013 eps_done=145 fps=817.9\n",
      "[rollout 84] step=86016 roll_sum=277.363 roll_mean=0.27086 pi_loss=0.0034 v_loss=76.4509 ent=0.629 kl=0.0049 clip=0.014 eps_done=146 fps=820.7\n",
      "[rollout 85] step=87040 roll_sum=-81.401 roll_mean=-0.07949 pi_loss=-0.0002 v_loss=49.0782 ent=0.768 kl=0.0012 clip=0.011 eps_done=148 fps=823.1\n",
      "[rollout 86] step=88064 roll_sum=42.267 roll_mean=0.04128 pi_loss=0.0017 v_loss=23.7163 ent=0.755 kl=0.0017 clip=0.022 eps_done=150 fps=825.4\n",
      "[rollout 87] step=89088 roll_sum=25.769 roll_mean=0.02516 pi_loss=-0.0027 v_loss=11.5790 ent=0.871 kl=0.0004 clip=0.009 eps_done=151 fps=827.9\n",
      "[rollout 88] step=90112 roll_sum=108.159 roll_mean=0.10562 pi_loss=-0.0002 v_loss=15.5865 ent=0.910 kl=0.0021 clip=0.000 eps_done=152 fps=830.3\n",
      "[rollout 89] step=91136 roll_sum=87.810 roll_mean=0.08575 pi_loss=0.0002 v_loss=21.7389 ent=0.776 kl=0.0011 clip=0.008 eps_done=154 fps=832.7\n",
      "[rollout 90] step=92160 roll_sum=80.874 roll_mean=0.07898 pi_loss=-0.0004 v_loss=42.0655 ent=0.630 kl=-0.0002 clip=0.001 eps_done=157 fps=834.8\n",
      "  [val] {'episode_reward': 27.591979019323748, 'episode_steps': 187.95, 'num_trades': 1.8, 'win_rate': 0.8833333333333332, 'avg_trade_return': 14.875520366821604, 'avg_hold_steps': 119.8, 'sum_trade_return': 27.35353431593677}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout90.pt\n",
      "[rollout 91] step=93184 roll_sum=73.416 roll_mean=0.07170 pi_loss=0.0016 v_loss=30.2011 ent=0.583 kl=0.0014 clip=0.021 eps_done=158 fps=817.8\n",
      "[rollout 92] step=94208 roll_sum=87.672 roll_mean=0.08562 pi_loss=0.0025 v_loss=19.2031 ent=0.749 kl=0.0069 clip=0.077 eps_done=159 fps=820.1\n",
      "[rollout 93] step=95232 roll_sum=9.321 roll_mean=0.00910 pi_loss=0.0007 v_loss=13.7902 ent=0.742 kl=0.0016 clip=0.026 eps_done=161 fps=822.1\n",
      "[rollout 94] step=96256 roll_sum=32.470 roll_mean=0.03171 pi_loss=-0.0005 v_loss=10.5718 ent=0.696 kl=0.0011 clip=0.000 eps_done=162 fps=824.3\n",
      "[rollout 95] step=97280 roll_sum=90.399 roll_mean=0.08828 pi_loss=-0.0018 v_loss=17.5095 ent=0.687 kl=0.0013 clip=0.005 eps_done=165 fps=826.4\n",
      "[rollout 96] step=98304 roll_sum=94.928 roll_mean=0.09270 pi_loss=0.0002 v_loss=20.8911 ent=0.644 kl=0.0011 clip=0.008 eps_done=166 fps=828.3\n",
      "[rollout 97] step=99328 roll_sum=-6.235 roll_mean=-0.00609 pi_loss=-0.0036 v_loss=39.5924 ent=0.531 kl=0.0050 clip=0.032 eps_done=169 fps=830.6\n",
      "[rollout 98] step=100352 roll_sum=64.110 roll_mean=0.06261 pi_loss=-0.0015 v_loss=17.6897 ent=0.536 kl=0.0007 clip=0.002 eps_done=170 fps=832.8\n",
      "[rollout 99] step=101376 roll_sum=97.318 roll_mean=0.09504 pi_loss=0.0027 v_loss=25.3350 ent=0.490 kl=0.0040 clip=0.005 eps_done=172 fps=834.9\n",
      "[rollout 100] step=102400 roll_sum=8.084 roll_mean=0.00789 pi_loss=0.0012 v_loss=20.7307 ent=0.551 kl=0.0057 clip=0.035 eps_done=173 fps=836.5\n",
      "  [val] {'episode_reward': 36.43926530306259, 'episode_steps': 191.95, 'num_trades': 1.0, 'win_rate': 0.95, 'avg_trade_return': 35.8228508235987, 'avg_hold_steps': 190.95, 'sum_trade_return': 35.8228508235987}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout100.pt\n",
      "[rollout 101] step=103424 roll_sum=70.050 roll_mean=0.06841 pi_loss=-0.0013 v_loss=17.2809 ent=0.525 kl=0.0039 clip=0.024 eps_done=176 fps=820.3\n",
      "[rollout 102] step=104448 roll_sum=48.707 roll_mean=0.04757 pi_loss=-0.0017 v_loss=28.9599 ent=0.572 kl=0.0048 clip=0.020 eps_done=180 fps=822.1\n",
      "[rollout 103] step=105472 roll_sum=56.455 roll_mean=0.05513 pi_loss=0.0022 v_loss=27.7935 ent=0.533 kl=0.0013 clip=0.009 eps_done=182 fps=824.2\n",
      "[rollout 104] step=106496 roll_sum=27.306 roll_mean=0.02667 pi_loss=-0.0011 v_loss=29.9226 ent=0.611 kl=0.0011 clip=0.023 eps_done=183 fps=826.3\n",
      "[rollout 105] step=107520 roll_sum=129.097 roll_mean=0.12607 pi_loss=-0.0002 v_loss=31.7672 ent=0.746 kl=-0.0004 clip=0.008 eps_done=185 fps=828.3\n",
      "[rollout 106] step=108544 roll_sum=25.278 roll_mean=0.02469 pi_loss=-0.0006 v_loss=21.1077 ent=0.569 kl=0.0004 clip=0.005 eps_done=186 fps=830.3\n",
      "[rollout 107] step=109568 roll_sum=1.151 roll_mean=0.00112 pi_loss=-0.0002 v_loss=12.7719 ent=0.610 kl=0.0016 clip=0.003 eps_done=188 fps=832.2\n",
      "[rollout 108] step=110592 roll_sum=72.978 roll_mean=0.07127 pi_loss=0.0014 v_loss=27.6293 ent=0.624 kl=0.0015 clip=0.012 eps_done=190 fps=834.1\n",
      "[rollout 109] step=111616 roll_sum=-1.893 roll_mean=-0.00185 pi_loss=-0.0002 v_loss=58.8871 ent=0.715 kl=0.0032 clip=0.034 eps_done=191 fps=836.1\n",
      "[rollout 110] step=112640 roll_sum=-51.464 roll_mean=-0.05026 pi_loss=-0.0001 v_loss=14.3386 ent=0.612 kl=0.0022 clip=0.008 eps_done=193 fps=838.0\n",
      "  [val] {'episode_reward': 32.35640989567801, 'episode_steps': 171.15, 'num_trades': 1.0, 'win_rate': 0.95, 'avg_trade_return': 31.777916171149393, 'avg_hold_steps': 170.15, 'sum_trade_return': 31.777916171149393}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout110.pt\n",
      "[rollout 111] step=113664 roll_sum=33.717 roll_mean=0.03293 pi_loss=0.0002 v_loss=9.9352 ent=0.724 kl=-0.0005 clip=0.000 eps_done=195 fps=824.9\n",
      "[rollout 112] step=114688 roll_sum=97.975 roll_mean=0.09568 pi_loss=0.0011 v_loss=19.4304 ent=0.809 kl=0.0009 clip=0.002 eps_done=196 fps=826.6\n",
      "[rollout 113] step=115712 roll_sum=44.615 roll_mean=0.04357 pi_loss=-0.0011 v_loss=26.3070 ent=0.778 kl=0.0070 clip=0.043 eps_done=199 fps=828.3\n",
      "[rollout 114] step=116736 roll_sum=38.607 roll_mean=0.03770 pi_loss=-0.0017 v_loss=43.6244 ent=0.678 kl=0.0018 clip=0.003 eps_done=200 fps=829.6\n",
      "[rollout 115] step=117760 roll_sum=104.988 roll_mean=0.10253 pi_loss=0.0003 v_loss=55.3003 ent=0.616 kl=0.0007 clip=0.003 eps_done=201 fps=831.2\n",
      "[rollout 116] step=118784 roll_sum=94.255 roll_mean=0.09205 pi_loss=-0.0011 v_loss=30.5858 ent=0.742 kl=0.0003 clip=0.012 eps_done=203 fps=832.8\n",
      "[rollout 117] step=119808 roll_sum=-5.776 roll_mean=-0.00564 pi_loss=-0.0016 v_loss=9.8870 ent=0.681 kl=0.0040 clip=0.059 eps_done=204 fps=834.6\n",
      "[rollout 118] step=120832 roll_sum=4.951 roll_mean=0.00484 pi_loss=-0.0008 v_loss=15.0119 ent=0.694 kl=0.0011 clip=0.007 eps_done=207 fps=836.2\n",
      "[rollout 119] step=121856 roll_sum=20.130 roll_mean=0.01966 pi_loss=0.0003 v_loss=9.4895 ent=0.697 kl=0.0015 clip=0.023 eps_done=208 fps=837.7\n",
      "[rollout 120] step=122880 roll_sum=-7.630 roll_mean=-0.00745 pi_loss=-0.0012 v_loss=30.9492 ent=0.726 kl=0.0010 clip=0.003 eps_done=210 fps=839.3\n",
      "  [val] {'episode_reward': 41.86705187007506, 'episode_steps': 209.55, 'num_trades': 1.0, 'win_rate': 0.95, 'avg_trade_return': 41.250469968260276, 'avg_hold_steps': 208.55, 'sum_trade_return': 41.250469968260276}\n",
      "  [best] new best val episode_reward=41.8671 -> saves\\ppo_ppo_aapl_final_best.pt\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout120.pt\n",
      "[rollout 121] step=123904 roll_sum=115.030 roll_mean=0.11233 pi_loss=-0.0004 v_loss=20.9164 ent=0.622 kl=0.0018 clip=0.022 eps_done=211 fps=824.4\n",
      "[rollout 122] step=124928 roll_sum=230.134 roll_mean=0.22474 pi_loss=0.0077 v_loss=78.0709 ent=0.703 kl=0.0097 clip=0.074 eps_done=212 fps=826.0\n",
      "[rollout 123] step=125952 roll_sum=-8.544 roll_mean=-0.00834 pi_loss=0.0047 v_loss=39.0521 ent=0.712 kl=0.0058 clip=0.054 eps_done=214 fps=827.2\n",
      "[rollout 124] step=126976 roll_sum=23.774 roll_mean=0.02322 pi_loss=-0.0011 v_loss=12.2588 ent=0.640 kl=0.0036 clip=0.064 eps_done=215 fps=828.8\n",
      "[rollout 125] step=128000 roll_sum=5.461 roll_mean=0.00533 pi_loss=0.0009 v_loss=13.5604 ent=0.751 kl=0.0017 clip=0.019 eps_done=217 fps=830.3\n",
      "[rollout 126] step=129024 roll_sum=63.469 roll_mean=0.06198 pi_loss=-0.0013 v_loss=9.4435 ent=0.744 kl=0.0011 clip=0.008 eps_done=220 fps=832.0\n",
      "[rollout 127] step=130048 roll_sum=49.872 roll_mean=0.04870 pi_loss=0.0008 v_loss=13.9858 ent=0.736 kl=0.0009 clip=0.000 eps_done=221 fps=833.5\n",
      "[rollout 128] step=131072 roll_sum=192.550 roll_mean=0.18804 pi_loss=0.0089 v_loss=52.5243 ent=0.649 kl=-0.0001 clip=0.037 eps_done=222 fps=835.1\n",
      "[rollout 129] step=132096 roll_sum=-48.273 roll_mean=-0.04714 pi_loss=0.0036 v_loss=25.3842 ent=0.806 kl=0.0126 clip=0.107 eps_done=223 fps=836.5\n",
      "[rollout 130] step=133120 roll_sum=27.310 roll_mean=0.02667 pi_loss=-0.0030 v_loss=22.1276 ent=0.706 kl=0.0006 clip=0.015 eps_done=226 fps=837.1\n",
      "  [val] {'episode_reward': 38.38930150613163, 'episode_steps': 201.05, 'num_trades': 1.8, 'win_rate': 0.85, 'avg_trade_return': 19.27562140242956, 'avg_hold_steps': 134.40833333333333, 'sum_trade_return': 38.156990368742996}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout130.pt\n",
      "[rollout 131] step=134144 roll_sum=-30.082 roll_mean=-0.02938 pi_loss=-0.0015 v_loss=22.6762 ent=0.692 kl=0.0028 clip=0.032 eps_done=227 fps=822.9\n",
      "[rollout 132] step=135168 roll_sum=93.685 roll_mean=0.09149 pi_loss=-0.0009 v_loss=11.1297 ent=0.765 kl=0.0031 clip=0.040 eps_done=229 fps=824.4\n",
      "[rollout 133] step=136192 roll_sum=65.286 roll_mean=0.06376 pi_loss=-0.0008 v_loss=10.9884 ent=0.680 kl=0.0024 clip=0.018 eps_done=230 fps=825.8\n",
      "[rollout 134] step=137216 roll_sum=37.090 roll_mean=0.03622 pi_loss=0.0013 v_loss=5.7345 ent=0.699 kl=0.0013 clip=0.003 eps_done=232 fps=827.3\n",
      "[rollout 135] step=138240 roll_sum=3.346 roll_mean=0.00327 pi_loss=-0.0020 v_loss=20.0403 ent=0.696 kl=-0.0005 clip=0.017 eps_done=234 fps=828.8\n",
      "[rollout 136] step=139264 roll_sum=31.605 roll_mean=0.03086 pi_loss=-0.0011 v_loss=7.9017 ent=0.698 kl=0.0010 clip=0.001 eps_done=235 fps=830.0\n",
      "[rollout 137] step=140288 roll_sum=38.689 roll_mean=0.03778 pi_loss=0.0015 v_loss=8.9357 ent=0.551 kl=0.0038 clip=0.015 eps_done=236 fps=830.7\n",
      "[rollout 138] step=141312 roll_sum=28.251 roll_mean=0.02759 pi_loss=0.0029 v_loss=18.1905 ent=0.609 kl=0.0018 clip=0.008 eps_done=238 fps=832.1\n",
      "[rollout 139] step=142336 roll_sum=104.773 roll_mean=0.10232 pi_loss=0.0022 v_loss=44.2884 ent=0.639 kl=0.0054 clip=0.042 eps_done=240 fps=833.4\n",
      "[rollout 140] step=143360 roll_sum=28.178 roll_mean=0.02752 pi_loss=-0.0015 v_loss=12.4064 ent=0.683 kl=0.0016 clip=0.038 eps_done=243 fps=834.8\n",
      "  [val] {'episode_reward': 25.955486042706713, 'episode_steps': 204.45, 'num_trades': 2.1, 'win_rate': 0.8333333333333333, 'avg_trade_return': 10.145098171726104, 'avg_hold_steps': 122.01666666666668, 'sum_trade_return': 25.906728826318044}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout140.pt\n",
      "[rollout 141] step=144384 roll_sum=16.249 roll_mean=0.01587 pi_loss=0.0032 v_loss=14.8246 ent=0.670 kl=0.0033 clip=0.021 eps_done=245 fps=822.0\n",
      "[rollout 142] step=145408 roll_sum=182.703 roll_mean=0.17842 pi_loss=0.0071 v_loss=67.8631 ent=0.555 kl=0.0009 clip=0.044 eps_done=248 fps=823.4\n",
      "[rollout 143] step=146432 roll_sum=53.748 roll_mean=0.05249 pi_loss=0.0025 v_loss=20.0740 ent=0.659 kl=0.0006 clip=0.006 eps_done=250 fps=824.9\n",
      "[rollout 144] step=147456 roll_sum=72.107 roll_mean=0.07042 pi_loss=-0.0028 v_loss=15.3338 ent=0.593 kl=0.0025 clip=0.016 eps_done=251 fps=826.1\n",
      "[rollout 145] step=148480 roll_sum=2.754 roll_mean=0.00269 pi_loss=-0.0003 v_loss=15.9107 ent=0.651 kl=0.0116 clip=0.087 eps_done=254 fps=827.4\n",
      "[rollout 146] step=149504 roll_sum=41.769 roll_mean=0.04079 pi_loss=0.0007 v_loss=6.2857 ent=0.584 kl=0.0009 clip=0.004 eps_done=255 fps=828.8\n",
      "[rollout 147] step=150528 roll_sum=227.431 roll_mean=0.22210 pi_loss=0.0061 v_loss=48.8743 ent=0.547 kl=0.0042 clip=0.049 eps_done=257 fps=830.3\n",
      "[rollout 148] step=151552 roll_sum=8.413 roll_mean=0.00822 pi_loss=-0.0020 v_loss=29.7894 ent=0.620 kl=0.0022 clip=0.048 eps_done=259 fps=831.5\n",
      "[rollout 149] step=152576 roll_sum=74.385 roll_mean=0.07264 pi_loss=-0.0018 v_loss=35.6944 ent=0.581 kl=0.0029 clip=0.015 eps_done=261 fps=832.9\n",
      "[rollout 150] step=153600 roll_sum=-26.437 roll_mean=-0.02582 pi_loss=0.0010 v_loss=12.0779 ent=0.585 kl=0.0039 clip=0.020 eps_done=263 fps=834.3\n",
      "  [val] {'episode_reward': 31.04825972203115, 'episode_steps': 183.75, 'num_trades': 1.0, 'win_rate': 0.95, 'avg_trade_return': 30.556186151785617, 'avg_hold_steps': 182.75, 'sum_trade_return': 30.556186151785617}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout150.pt\n",
      "[rollout 151] step=154624 roll_sum=24.201 roll_mean=0.02363 pi_loss=-0.0049 v_loss=27.9779 ent=0.640 kl=0.0222 clip=0.204 eps_done=264 fps=824.1\n",
      "[rollout 152] step=155648 roll_sum=-25.589 roll_mean=-0.02499 pi_loss=-0.0005 v_loss=19.1860 ent=0.600 kl=0.0016 clip=0.017 eps_done=265 fps=825.5\n",
      "[rollout 153] step=156672 roll_sum=179.376 roll_mean=0.17517 pi_loss=0.0048 v_loss=48.9986 ent=0.553 kl=0.0005 clip=0.080 eps_done=267 fps=826.8\n",
      "[rollout 154] step=157696 roll_sum=69.463 roll_mean=0.06784 pi_loss=-0.0026 v_loss=24.2362 ent=0.618 kl=0.0028 clip=0.013 eps_done=268 fps=828.1\n",
      "[rollout 155] step=158720 roll_sum=18.423 roll_mean=0.01799 pi_loss=-0.0021 v_loss=22.8183 ent=0.603 kl=0.0026 clip=0.047 eps_done=269 fps=829.4\n",
      "[rollout 156] step=159744 roll_sum=59.018 roll_mean=0.05764 pi_loss=0.0034 v_loss=10.5338 ent=0.613 kl=0.0035 clip=0.040 eps_done=272 fps=830.4\n",
      "[rollout 157] step=160768 roll_sum=55.806 roll_mean=0.05450 pi_loss=-0.0018 v_loss=9.6646 ent=0.643 kl=0.0024 clip=0.015 eps_done=273 fps=831.5\n",
      "[rollout 158] step=161792 roll_sum=78.806 roll_mean=0.07696 pi_loss=-0.0020 v_loss=11.2635 ent=0.655 kl=0.0011 clip=0.025 eps_done=277 fps=832.9\n",
      "[rollout 159] step=162816 roll_sum=81.033 roll_mean=0.07913 pi_loss=-0.0003 v_loss=12.3524 ent=0.566 kl=0.0011 clip=0.006 eps_done=280 fps=834.1\n",
      "[rollout 160] step=163840 roll_sum=79.023 roll_mean=0.07717 pi_loss=-0.0014 v_loss=19.3750 ent=0.634 kl=-0.0009 clip=0.008 eps_done=282 fps=835.3\n",
      "  [val] {'episode_reward': 31.34412094114699, 'episode_steps': 188.55, 'num_trades': 1.9, 'win_rate': 0.7083333333333333, 'avg_trade_return': 14.909263356982247, 'avg_hold_steps': 119.575, 'sum_trade_return': 31.149084260619713}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout160.pt\n",
      "[rollout 161] step=164864 roll_sum=39.058 roll_mean=0.03814 pi_loss=-0.0005 v_loss=9.5696 ent=0.609 kl=0.0052 clip=0.031 eps_done=285 fps=825.1\n",
      "[rollout 162] step=165888 roll_sum=42.198 roll_mean=0.04121 pi_loss=0.0016 v_loss=18.3405 ent=0.634 kl=0.0007 clip=0.001 eps_done=286 fps=826.3\n",
      "[rollout 163] step=166912 roll_sum=20.500 roll_mean=0.02002 pi_loss=-0.0007 v_loss=9.9485 ent=0.642 kl=0.0025 clip=0.030 eps_done=288 fps=827.5\n",
      "[rollout 164] step=167936 roll_sum=173.197 roll_mean=0.16914 pi_loss=0.0072 v_loss=67.8366 ent=0.584 kl=0.0147 clip=0.083 eps_done=289 fps=828.6\n",
      "[rollout 165] step=168960 roll_sum=188.078 roll_mean=0.18367 pi_loss=0.0023 v_loss=65.1298 ent=0.581 kl=0.0042 clip=0.051 eps_done=291 fps=829.8\n",
      "[rollout 166] step=169984 roll_sum=21.719 roll_mean=0.02121 pi_loss=0.0013 v_loss=25.9679 ent=0.561 kl=0.0072 clip=0.055 eps_done=292 fps=830.9\n",
      "[rollout 167] step=171008 roll_sum=23.335 roll_mean=0.02279 pi_loss=0.0025 v_loss=7.0418 ent=0.590 kl=0.0007 clip=0.019 eps_done=293 fps=832.2\n",
      "[rollout 168] step=172032 roll_sum=-7.403 roll_mean=-0.00723 pi_loss=-0.0006 v_loss=22.3431 ent=0.585 kl=0.0019 clip=0.004 eps_done=295 fps=833.4\n",
      "[rollout 169] step=173056 roll_sum=143.611 roll_mean=0.14025 pi_loss=-0.0004 v_loss=38.0124 ent=0.454 kl=0.0010 clip=0.004 eps_done=296 fps=834.5\n",
      "[rollout 170] step=174080 roll_sum=46.628 roll_mean=0.04554 pi_loss=-0.0003 v_loss=14.4098 ent=0.583 kl=0.0010 clip=0.004 eps_done=298 fps=835.7\n",
      "  [val] {'episode_reward': 38.82088720837949, 'episode_steps': 190.35, 'num_trades': 1.0, 'win_rate': 1.0, 'avg_trade_return': 38.21199716533105, 'avg_hold_steps': 189.35, 'sum_trade_return': 38.21199716533105}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout170.pt\n",
      "[rollout 171] step=175104 roll_sum=23.270 roll_mean=0.02272 pi_loss=0.0006 v_loss=12.5993 ent=0.563 kl=0.0030 clip=0.028 eps_done=301 fps=826.2\n",
      "[rollout 172] step=176128 roll_sum=98.853 roll_mean=0.09654 pi_loss=-0.0011 v_loss=11.2075 ent=0.535 kl=0.0077 clip=0.031 eps_done=302 fps=827.3\n",
      "[rollout 173] step=177152 roll_sum=39.097 roll_mean=0.03818 pi_loss=-0.0006 v_loss=9.6107 ent=0.631 kl=0.0019 clip=0.013 eps_done=305 fps=828.5\n",
      "[rollout 174] step=178176 roll_sum=-17.649 roll_mean=-0.01724 pi_loss=0.0019 v_loss=5.5738 ent=0.665 kl=0.0058 clip=0.041 eps_done=307 fps=829.6\n",
      "[rollout 175] step=179200 roll_sum=48.690 roll_mean=0.04755 pi_loss=-0.0030 v_loss=9.0625 ent=0.532 kl=0.0047 clip=0.042 eps_done=309 fps=830.7\n",
      "[rollout 176] step=180224 roll_sum=99.341 roll_mean=0.09701 pi_loss=0.0111 v_loss=34.5470 ent=0.607 kl=0.0055 clip=0.031 eps_done=311 fps=831.7\n",
      "[rollout 177] step=181248 roll_sum=35.645 roll_mean=0.03481 pi_loss=0.0011 v_loss=30.0591 ent=0.551 kl=0.0108 clip=0.118 eps_done=312 fps=832.7\n",
      "[rollout 178] step=182272 roll_sum=-5.822 roll_mean=-0.00569 pi_loss=-0.0015 v_loss=23.6268 ent=0.643 kl=0.0029 clip=0.005 eps_done=314 fps=833.4\n",
      "[rollout 179] step=183296 roll_sum=83.483 roll_mean=0.08153 pi_loss=0.0038 v_loss=26.3993 ent=0.606 kl=0.0009 clip=0.011 eps_done=315 fps=834.3\n",
      "[rollout 180] step=184320 roll_sum=47.278 roll_mean=0.04617 pi_loss=0.0054 v_loss=22.2350 ent=0.655 kl=0.0018 clip=0.079 eps_done=317 fps=835.4\n",
      "  [val] {'episode_reward': 29.271783173579536, 'episode_steps': 199.95, 'num_trades': 2.65, 'win_rate': 0.7499999999999999, 'avg_trade_return': 10.01476144780015, 'avg_hold_steps': 83.37083333333334, 'sum_trade_return': 29.210963269099512}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout180.pt\n",
      "[rollout 181] step=185344 roll_sum=134.627 roll_mean=0.13147 pi_loss=0.0135 v_loss=32.6788 ent=0.556 kl=0.0140 clip=0.097 eps_done=318 fps=825.5\n",
      "[rollout 182] step=186368 roll_sum=-64.311 roll_mean=-0.06280 pi_loss=0.0031 v_loss=23.4271 ent=0.651 kl=0.0248 clip=0.145 eps_done=320 fps=826.7\n",
      "[rollout 183] step=187392 roll_sum=44.710 roll_mean=0.04366 pi_loss=-0.0009 v_loss=11.0227 ent=0.480 kl=0.0027 clip=0.043 eps_done=322 fps=827.8\n",
      "[rollout 184] step=188416 roll_sum=228.073 roll_mean=0.22273 pi_loss=0.0158 v_loss=59.5992 ent=0.564 kl=0.0134 clip=0.041 eps_done=323 fps=828.9\n",
      "[rollout 185] step=189440 roll_sum=5.796 roll_mean=0.00566 pi_loss=0.0025 v_loss=13.8512 ent=0.608 kl=0.0021 clip=0.037 eps_done=325 fps=830.0\n",
      "[rollout 186] step=190464 roll_sum=80.553 roll_mean=0.07867 pi_loss=0.0003 v_loss=23.6411 ent=0.517 kl=0.0017 clip=0.016 eps_done=326 fps=831.1\n",
      "[rollout 187] step=191488 roll_sum=-2.014 roll_mean=-0.00197 pi_loss=-0.0004 v_loss=19.6679 ent=0.636 kl=0.0008 clip=0.004 eps_done=328 fps=832.1\n",
      "[rollout 188] step=192512 roll_sum=53.367 roll_mean=0.05212 pi_loss=-0.0001 v_loss=10.6430 ent=0.648 kl=-0.0003 clip=0.008 eps_done=329 fps=833.2\n",
      "[rollout 189] step=193536 roll_sum=14.575 roll_mean=0.01423 pi_loss=-0.0031 v_loss=20.9541 ent=0.678 kl=0.0027 clip=0.016 eps_done=331 fps=834.1\n",
      "[rollout 190] step=194560 roll_sum=25.975 roll_mean=0.02537 pi_loss=-0.0005 v_loss=9.2208 ent=0.642 kl=0.0092 clip=0.067 eps_done=332 fps=835.3\n",
      "  [val] {'episode_reward': 26.013965170879867, 'episode_steps': 185.45, 'num_trades': 3.1, 'win_rate': 0.74, 'avg_trade_return': 7.759372143416476, 'avg_hold_steps': 70.87333333333333, 'sum_trade_return': 26.09006685649233}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout190.pt\n",
      "[rollout 191] step=195584 roll_sum=169.390 roll_mean=0.16542 pi_loss=0.0298 v_loss=41.2231 ent=0.548 kl=0.0186 clip=0.085 eps_done=333 fps=826.8\n",
      "[rollout 192] step=196608 roll_sum=79.044 roll_mean=0.07719 pi_loss=0.0021 v_loss=28.0079 ent=0.575 kl=0.0038 clip=0.029 eps_done=335 fps=827.8\n",
      "[rollout 193] step=197632 roll_sum=1.790 roll_mean=0.00175 pi_loss=0.0151 v_loss=13.3310 ent=0.583 kl=0.0304 clip=0.246 eps_done=337 fps=828.8\n",
      "[rollout 194] step=198656 roll_sum=34.947 roll_mean=0.03413 pi_loss=-0.0004 v_loss=22.6190 ent=0.543 kl=0.0011 clip=0.002 eps_done=339 fps=829.8\n",
      "[rollout 195] step=199680 roll_sum=31.721 roll_mean=0.03098 pi_loss=0.0009 v_loss=13.0291 ent=0.528 kl=-0.0003 clip=0.002 eps_done=340 fps=830.8\n",
      "[rollout 196] step=200704 roll_sum=23.712 roll_mean=0.02316 pi_loss=0.0021 v_loss=5.1039 ent=0.508 kl=0.0025 clip=0.042 eps_done=342 fps=831.5\n",
      "[rollout 197] step=201728 roll_sum=67.175 roll_mean=0.06560 pi_loss=0.0027 v_loss=46.9644 ent=0.587 kl=0.0025 clip=0.027 eps_done=344 fps=831.1\n",
      "[rollout 198] step=202752 roll_sum=64.303 roll_mean=0.06280 pi_loss=-0.0011 v_loss=19.9077 ent=0.562 kl=0.0002 clip=0.019 eps_done=345 fps=832.0\n",
      "[rollout 199] step=203776 roll_sum=23.639 roll_mean=0.02309 pi_loss=0.0024 v_loss=8.3708 ent=0.473 kl=0.0016 clip=0.015 eps_done=346 fps=832.9\n",
      "[rollout 200] step=204800 roll_sum=35.983 roll_mean=0.03514 pi_loss=0.0000 v_loss=15.3815 ent=0.557 kl=0.0004 clip=0.018 eps_done=348 fps=833.9\n",
      "  [val] {'episode_reward': 33.80850711896361, 'episode_steps': 198.4, 'num_trades': 2.4, 'win_rate': 0.8916666666666666, 'avg_trade_return': 13.204190728195474, 'avg_hold_steps': 92.03333333333333, 'sum_trade_return': 33.692055522275105}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout200.pt\n",
      "[rollout 201] step=205824 roll_sum=31.481 roll_mean=0.03074 pi_loss=0.0012 v_loss=51.4243 ent=0.557 kl=0.0036 clip=0.033 eps_done=351 fps=825.1\n",
      "[rollout 202] step=206848 roll_sum=26.296 roll_mean=0.02568 pi_loss=0.0032 v_loss=10.2038 ent=0.522 kl=0.0153 clip=0.081 eps_done=352 fps=826.1\n",
      "[rollout 203] step=207872 roll_sum=196.178 roll_mean=0.19158 pi_loss=0.0165 v_loss=39.5603 ent=0.531 kl=0.0127 clip=0.074 eps_done=354 fps=827.0\n",
      "[rollout 204] step=208896 roll_sum=18.088 roll_mean=0.01766 pi_loss=0.0017 v_loss=5.4298 ent=0.552 kl=0.0022 clip=0.032 eps_done=355 fps=828.0\n",
      "[rollout 205] step=209920 roll_sum=75.939 roll_mean=0.07416 pi_loss=0.0029 v_loss=11.8872 ent=0.496 kl=0.0079 clip=0.027 eps_done=357 fps=829.0\n",
      "[rollout 206] step=210944 roll_sum=50.144 roll_mean=0.04897 pi_loss=-0.0000 v_loss=12.2807 ent=0.631 kl=0.0020 clip=0.048 eps_done=359 fps=829.9\n",
      "[rollout 207] step=211968 roll_sum=17.959 roll_mean=0.01754 pi_loss=0.0030 v_loss=8.9682 ent=0.648 kl=0.0035 clip=0.069 eps_done=360 fps=830.8\n",
      "[rollout 208] step=212992 roll_sum=44.378 roll_mean=0.04334 pi_loss=-0.0004 v_loss=11.7366 ent=0.630 kl=0.0001 clip=0.016 eps_done=362 fps=831.7\n",
      "[rollout 209] step=214016 roll_sum=-22.030 roll_mean=-0.02151 pi_loss=-0.0026 v_loss=27.7836 ent=0.602 kl=0.0012 clip=0.010 eps_done=363 fps=832.9\n",
      "[rollout 210] step=215040 roll_sum=39.409 roll_mean=0.03848 pi_loss=0.0007 v_loss=21.4071 ent=0.553 kl=0.0066 clip=0.056 eps_done=364 fps=833.6\n",
      "  [val] {'episode_reward': 30.611000958609985, 'episode_steps': 174.35, 'num_trades': 3.25, 'win_rate': 0.7591666666666665, 'avg_trade_return': 9.010687701934273, 'avg_hold_steps': 56.802499999999995, 'sum_trade_return': 30.686124169756237}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout210.pt\n",
      "[rollout 211] step=216064 roll_sum=220.823 roll_mean=0.21565 pi_loss=0.0319 v_loss=49.4125 ent=0.564 kl=0.0313 clip=0.090 eps_done=365 fps=825.1\n",
      "[rollout 212] step=217088 roll_sum=-35.103 roll_mean=-0.03428 pi_loss=0.0090 v_loss=19.9384 ent=0.625 kl=0.0160 clip=0.213 eps_done=367 fps=825.9\n",
      "[rollout 213] step=218112 roll_sum=111.849 roll_mean=0.10923 pi_loss=0.0027 v_loss=35.9176 ent=0.538 kl=0.0031 clip=0.026 eps_done=369 fps=826.6\n",
      "[rollout 214] step=219136 roll_sum=29.667 roll_mean=0.02897 pi_loss=0.0019 v_loss=13.6564 ent=0.586 kl=0.0058 clip=0.042 eps_done=370 fps=827.3\n",
      "[rollout 215] step=220160 roll_sum=-12.161 roll_mean=-0.01188 pi_loss=0.0010 v_loss=5.3739 ent=0.470 kl=-0.0007 clip=0.009 eps_done=371 fps=828.0\n",
      "[rollout 216] step=221184 roll_sum=25.013 roll_mean=0.02443 pi_loss=0.0011 v_loss=25.7519 ent=0.526 kl=0.0038 clip=0.031 eps_done=373 fps=828.8\n",
      "[rollout 217] step=222208 roll_sum=43.918 roll_mean=0.04289 pi_loss=0.0031 v_loss=19.4931 ent=0.530 kl=0.0065 clip=0.074 eps_done=375 fps=829.4\n",
      "[rollout 218] step=223232 roll_sum=61.146 roll_mean=0.05971 pi_loss=-0.0007 v_loss=14.7479 ent=0.501 kl=-0.0001 clip=0.013 eps_done=376 fps=830.0\n",
      "[rollout 219] step=224256 roll_sum=12.041 roll_mean=0.01176 pi_loss=0.0037 v_loss=12.9337 ent=0.521 kl=0.0055 clip=0.063 eps_done=378 fps=830.3\n",
      "[rollout 220] step=225280 roll_sum=-36.555 roll_mean=-0.03570 pi_loss=0.0038 v_loss=4.4312 ent=0.537 kl=-0.0006 clip=0.017 eps_done=379 fps=831.2\n",
      "  [val] {'episode_reward': 24.010070563926405, 'episode_steps': 210.45, 'num_trades': 2.45, 'win_rate': 0.8333333333333333, 'avg_trade_return': 8.521054938387415, 'avg_hold_steps': 101.59166666666667, 'sum_trade_return': 23.915461999204513}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout220.pt\n",
      "[rollout 221] step=226304 roll_sum=211.983 roll_mean=0.20701 pi_loss=0.0093 v_loss=70.6991 ent=0.514 kl=0.0059 clip=0.042 eps_done=380 fps=822.8\n",
      "[rollout 222] step=227328 roll_sum=2.934 roll_mean=0.00287 pi_loss=0.0021 v_loss=10.8619 ent=0.635 kl=0.0043 clip=0.049 eps_done=381 fps=823.6\n",
      "[rollout 223] step=228352 roll_sum=32.648 roll_mean=0.03188 pi_loss=-0.0018 v_loss=12.4858 ent=0.626 kl=0.0020 clip=0.015 eps_done=383 fps=824.5\n",
      "[rollout 224] step=229376 roll_sum=67.605 roll_mean=0.06602 pi_loss=-0.0006 v_loss=10.7515 ent=0.608 kl=0.0056 clip=0.044 eps_done=386 fps=825.5\n",
      "[rollout 225] step=230400 roll_sum=-44.672 roll_mean=-0.04362 pi_loss=0.0020 v_loss=15.3193 ent=0.589 kl=0.0022 clip=0.043 eps_done=387 fps=826.4\n",
      "[rollout 226] step=231424 roll_sum=39.822 roll_mean=0.03889 pi_loss=0.0002 v_loss=9.0010 ent=0.661 kl=0.0021 clip=0.004 eps_done=390 fps=827.3\n",
      "[rollout 227] step=232448 roll_sum=4.157 roll_mean=0.00406 pi_loss=0.0009 v_loss=55.1696 ent=0.634 kl=0.0083 clip=0.069 eps_done=392 fps=827.9\n",
      "[rollout 228] step=233472 roll_sum=-9.925 roll_mean=-0.00969 pi_loss=0.0050 v_loss=12.1496 ent=0.492 kl=0.0083 clip=0.081 eps_done=395 fps=828.6\n",
      "[rollout 229] step=234496 roll_sum=163.953 roll_mean=0.16011 pi_loss=0.0139 v_loss=44.1631 ent=0.403 kl=0.0140 clip=0.109 eps_done=397 fps=829.6\n",
      "[rollout 230] step=235520 roll_sum=4.297 roll_mean=0.00420 pi_loss=0.0033 v_loss=6.1205 ent=0.540 kl=0.0001 clip=0.015 eps_done=398 fps=830.5\n",
      "  [val] {'episode_reward': 24.9857928731323, 'episode_steps': 209.65, 'num_trades': 3.25, 'win_rate': 0.7633333333333334, 'avg_trade_return': 6.908896306001407, 'avg_hold_steps': 75.00416666666668, 'sum_trade_return': 25.061115895228248}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout230.pt\n",
      "[rollout 231] step=236544 roll_sum=-5.386 roll_mean=-0.00526 pi_loss=0.0014 v_loss=10.6843 ent=0.510 kl=0.0001 clip=0.012 eps_done=399 fps=821.9\n",
      "[rollout 232] step=237568 roll_sum=30.402 roll_mean=0.02969 pi_loss=0.0010 v_loss=28.7808 ent=0.443 kl=0.0001 clip=0.004 eps_done=400 fps=822.9\n",
      "[rollout 233] step=238592 roll_sum=106.131 roll_mean=0.10364 pi_loss=0.0041 v_loss=41.1424 ent=0.440 kl=0.0023 clip=0.017 eps_done=402 fps=823.8\n",
      "[rollout 234] step=239616 roll_sum=65.136 roll_mean=0.06361 pi_loss=-0.0017 v_loss=13.9635 ent=0.454 kl=0.0043 clip=0.021 eps_done=403 fps=824.8\n",
      "[rollout 235] step=240640 roll_sum=36.501 roll_mean=0.03565 pi_loss=0.0067 v_loss=24.2605 ent=0.472 kl=0.0039 clip=0.048 eps_done=404 fps=825.6\n",
      "[rollout 236] step=241664 roll_sum=53.867 roll_mean=0.05260 pi_loss=0.0025 v_loss=15.1196 ent=0.471 kl=0.0018 clip=0.025 eps_done=406 fps=826.5\n",
      "[rollout 237] step=242688 roll_sum=3.374 roll_mean=0.00329 pi_loss=-0.0007 v_loss=22.9255 ent=0.293 kl=0.0240 clip=0.105 eps_done=408 fps=827.4\n",
      "[rollout 238] step=243712 roll_sum=58.759 roll_mean=0.05738 pi_loss=0.0004 v_loss=26.1452 ent=0.294 kl=0.0002 clip=0.012 eps_done=411 fps=828.3\n",
      "[rollout 239] step=244736 roll_sum=91.398 roll_mean=0.08926 pi_loss=0.0029 v_loss=20.6315 ent=0.357 kl=0.0007 clip=0.024 eps_done=413 fps=829.2\n",
      "[rollout 240] step=245760 roll_sum=74.431 roll_mean=0.07269 pi_loss=0.0040 v_loss=16.9437 ent=0.156 kl=0.0022 clip=0.024 eps_done=418 fps=830.1\n",
      "  [val] {'episode_reward': 36.35454032866997, 'episode_steps': 204.1, 'num_trades': 1.0, 'win_rate': 1.0, 'avg_trade_return': 35.73927935679693, 'avg_hold_steps': 203.1, 'sum_trade_return': 35.73927935679693}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout240.pt\n",
      "[rollout 241] step=246784 roll_sum=74.377 roll_mean=0.07263 pi_loss=-0.0002 v_loss=9.5702 ent=0.172 kl=0.0005 clip=0.011 eps_done=419 fps=822.8\n",
      "[rollout 242] step=247808 roll_sum=84.972 roll_mean=0.08298 pi_loss=0.0002 v_loss=10.9529 ent=0.162 kl=0.0004 clip=0.002 eps_done=421 fps=823.6\n",
      "[rollout 243] step=248832 roll_sum=-5.448 roll_mean=-0.00532 pi_loss=-0.0012 v_loss=12.0714 ent=0.276 kl=0.0013 clip=0.012 eps_done=422 fps=824.4\n",
      "[rollout 244] step=249856 roll_sum=51.851 roll_mean=0.05064 pi_loss=0.0003 v_loss=12.6370 ent=0.175 kl=-0.0001 clip=0.006 eps_done=423 fps=825.2\n",
      "[rollout 245] step=250880 roll_sum=46.079 roll_mean=0.04500 pi_loss=0.0006 v_loss=8.2035 ent=0.225 kl=0.0005 clip=0.012 eps_done=425 fps=826.0\n",
      "[rollout 246] step=251904 roll_sum=40.661 roll_mean=0.03971 pi_loss=-0.0006 v_loss=7.7598 ent=0.219 kl=0.0001 clip=0.002 eps_done=428 fps=826.9\n",
      "[rollout 247] step=252928 roll_sum=137.236 roll_mean=0.13402 pi_loss=0.0056 v_loss=32.2803 ent=0.244 kl=0.0046 clip=0.011 eps_done=429 fps=827.7\n",
      "[rollout 248] step=253952 roll_sum=70.487 roll_mean=0.06884 pi_loss=0.0035 v_loss=10.7745 ent=0.158 kl=0.0014 clip=0.020 eps_done=430 fps=828.6\n",
      "[rollout 249] step=254976 roll_sum=160.080 roll_mean=0.15633 pi_loss=0.0033 v_loss=22.1763 ent=0.220 kl=0.0036 clip=0.019 eps_done=432 fps=829.4\n",
      "[rollout 250] step=256000 roll_sum=66.625 roll_mean=0.06506 pi_loss=0.0004 v_loss=16.2485 ent=0.237 kl=0.0010 clip=0.001 eps_done=434 fps=830.2\n",
      "  [val] {'episode_reward': 34.38043131121607, 'episode_steps': 184.25, 'num_trades': 1.0, 'win_rate': 1.0, 'avg_trade_return': 33.78817623905352, 'avg_hold_steps': 183.25, 'sum_trade_return': 33.78817623905352}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout250.pt\n",
      "[rollout 251] step=257024 roll_sum=-46.404 roll_mean=-0.04532 pi_loss=-0.0045 v_loss=44.3543 ent=0.298 kl=0.0002 clip=0.024 eps_done=436 fps=824.0\n",
      "[rollout 252] step=258048 roll_sum=31.287 roll_mean=0.03055 pi_loss=0.0009 v_loss=13.2547 ent=0.259 kl=0.0008 clip=0.005 eps_done=437 fps=824.8\n",
      "[rollout 253] step=259072 roll_sum=18.921 roll_mean=0.01848 pi_loss=-0.0015 v_loss=16.5022 ent=0.165 kl=0.0003 clip=0.018 eps_done=438 fps=825.6\n",
      "[rollout 254] step=260096 roll_sum=237.761 roll_mean=0.23219 pi_loss=0.0008 v_loss=51.0187 ent=0.172 kl=0.0069 clip=0.014 eps_done=441 fps=826.5\n",
      "[rollout 255] step=261120 roll_sum=104.755 roll_mean=0.10230 pi_loss=0.0016 v_loss=11.4598 ent=0.094 kl=-0.0008 clip=0.004 eps_done=443 fps=827.3\n",
      "[rollout 256] step=262144 roll_sum=75.444 roll_mean=0.07368 pi_loss=0.0005 v_loss=9.2378 ent=0.129 kl=-0.0012 clip=0.014 eps_done=444 fps=828.2\n",
      "[rollout 257] step=263168 roll_sum=16.607 roll_mean=0.01622 pi_loss=0.0023 v_loss=24.0582 ent=0.156 kl=-0.0001 clip=0.010 eps_done=447 fps=829.0\n",
      "[rollout 258] step=264192 roll_sum=-11.993 roll_mean=-0.01171 pi_loss=0.0019 v_loss=15.1740 ent=0.179 kl=0.0002 clip=0.002 eps_done=448 fps=829.8\n",
      "[rollout 259] step=265216 roll_sum=20.275 roll_mean=0.01980 pi_loss=0.0003 v_loss=6.5293 ent=0.183 kl=0.0001 clip=0.007 eps_done=449 fps=830.6\n",
      "[rollout 260] step=266240 roll_sum=58.654 roll_mean=0.05728 pi_loss=-0.0015 v_loss=7.6721 ent=0.188 kl=0.0011 clip=0.010 eps_done=451 fps=831.5\n",
      "  [val] {'episode_reward': 33.23878960630435, 'episode_steps': 194.8, 'num_trades': 1.4, 'win_rate': 0.875, 'avg_trade_return': 22.78183763384156, 'avg_hold_steps': 157.5, 'sum_trade_return': 32.85593402541467}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout260.pt\n",
      "[rollout 261] step=267264 roll_sum=48.437 roll_mean=0.04730 pi_loss=-0.0013 v_loss=6.0335 ent=0.191 kl=-0.0007 clip=0.013 eps_done=453 fps=825.0\n",
      "[rollout 262] step=268288 roll_sum=97.428 roll_mean=0.09514 pi_loss=-0.0020 v_loss=10.5691 ent=0.201 kl=-0.0006 clip=0.010 eps_done=455 fps=825.7\n",
      "[rollout 263] step=269312 roll_sum=31.303 roll_mean=0.03057 pi_loss=0.0008 v_loss=34.0522 ent=0.201 kl=0.0023 clip=0.020 eps_done=457 fps=826.5\n",
      "[rollout 264] step=270336 roll_sum=75.864 roll_mean=0.07409 pi_loss=0.0005 v_loss=40.3315 ent=0.144 kl=0.0001 clip=0.007 eps_done=458 fps=827.4\n",
      "[rollout 265] step=271360 roll_sum=61.958 roll_mean=0.06051 pi_loss=0.0011 v_loss=14.6261 ent=0.182 kl=0.0009 clip=0.006 eps_done=461 fps=828.1\n",
      "[rollout 266] step=272384 roll_sum=128.728 roll_mean=0.12571 pi_loss=-0.0001 v_loss=18.3236 ent=0.113 kl=0.0010 clip=0.002 eps_done=462 fps=828.9\n",
      "[rollout 267] step=273408 roll_sum=-10.930 roll_mean=-0.01067 pi_loss=-0.0010 v_loss=9.2521 ent=0.222 kl=0.0004 clip=0.013 eps_done=463 fps=829.7\n",
      "[rollout 268] step=274432 roll_sum=156.050 roll_mean=0.15239 pi_loss=0.0007 v_loss=23.9889 ent=0.116 kl=-0.0000 clip=0.002 eps_done=466 fps=830.4\n",
      "[rollout 269] step=275456 roll_sum=29.641 roll_mean=0.02895 pi_loss=0.0017 v_loss=21.2796 ent=0.168 kl=-0.0010 clip=0.002 eps_done=467 fps=831.2\n",
      "[rollout 270] step=276480 roll_sum=76.034 roll_mean=0.07425 pi_loss=0.0037 v_loss=15.2816 ent=0.138 kl=-0.0001 clip=0.006 eps_done=468 fps=831.8\n",
      "  [val] {'episode_reward': 26.76015833386204, 'episode_steps': 203.6, 'num_trades': 1.4, 'win_rate': 0.775, 'avg_trade_return': 16.60438476805811, 'avg_hold_steps': 168.025, 'sum_trade_return': 26.406863382888503}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout270.pt\n",
      "[rollout 271] step=277504 roll_sum=56.069 roll_mean=0.05475 pi_loss=0.0027 v_loss=17.7647 ent=0.205 kl=0.0013 clip=0.010 eps_done=470 fps=825.1\n",
      "[rollout 272] step=278528 roll_sum=28.520 roll_mean=0.02785 pi_loss=-0.0002 v_loss=12.4682 ent=0.203 kl=-0.0005 clip=0.001 eps_done=471 fps=825.8\n",
      "[rollout 273] step=279552 roll_sum=35.132 roll_mean=0.03431 pi_loss=0.0012 v_loss=13.2283 ent=0.200 kl=0.0019 clip=0.012 eps_done=472 fps=826.6\n",
      "[rollout 274] step=280576 roll_sum=25.914 roll_mean=0.02531 pi_loss=-0.0005 v_loss=8.1703 ent=0.215 kl=-0.0005 clip=0.003 eps_done=473 fps=827.3\n",
      "[rollout 275] step=281600 roll_sum=118.055 roll_mean=0.11529 pi_loss=0.0014 v_loss=12.8424 ent=0.185 kl=-0.0000 clip=0.011 eps_done=474 fps=828.0\n",
      "[rollout 276] step=282624 roll_sum=50.745 roll_mean=0.04956 pi_loss=0.0013 v_loss=15.2445 ent=0.193 kl=-0.0001 clip=0.002 eps_done=477 fps=828.6\n",
      "[rollout 277] step=283648 roll_sum=-14.406 roll_mean=-0.01407 pi_loss=-0.0007 v_loss=7.1111 ent=0.229 kl=0.0003 clip=0.005 eps_done=478 fps=829.3\n",
      "[rollout 278] step=284672 roll_sum=63.773 roll_mean=0.06228 pi_loss=0.0007 v_loss=11.3996 ent=0.159 kl=0.0006 clip=0.003 eps_done=479 fps=830.0\n",
      "[rollout 279] step=285696 roll_sum=70.368 roll_mean=0.06872 pi_loss=0.0060 v_loss=19.7494 ent=0.204 kl=0.0034 clip=0.009 eps_done=481 fps=830.8\n",
      "[rollout 280] step=286720 roll_sum=-33.635 roll_mean=-0.03285 pi_loss=0.0005 v_loss=4.5581 ent=0.213 kl=0.0019 clip=0.009 eps_done=482 fps=831.5\n",
      "  [val] {'episode_reward': 31.01791780153883, 'episode_steps': 199.25, 'num_trades': 1.45, 'win_rate': 1.0, 'avg_trade_return': 19.733445870701505, 'avg_hold_steps': 156.75, 'sum_trade_return': 30.670730805750754}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout280.pt\n",
      "[rollout 281] step=287744 roll_sum=-0.508 roll_mean=-0.00050 pi_loss=0.0000 v_loss=21.5738 ent=0.217 kl=-0.0006 clip=0.001 eps_done=484 fps=825.5\n",
      "[rollout 282] step=288768 roll_sum=28.000 roll_mean=0.02734 pi_loss=0.0003 v_loss=16.2056 ent=0.148 kl=0.0003 clip=0.000 eps_done=486 fps=825.7\n",
      "[rollout 283] step=289792 roll_sum=-1.942 roll_mean=-0.00190 pi_loss=-0.0018 v_loss=6.9818 ent=0.175 kl=0.0034 clip=0.017 eps_done=487 fps=826.5\n",
      "[rollout 284] step=290816 roll_sum=89.950 roll_mean=0.08784 pi_loss=-0.0009 v_loss=7.6654 ent=0.155 kl=0.0004 clip=0.005 eps_done=489 fps=827.2\n",
      "[rollout 285] step=291840 roll_sum=62.600 roll_mean=0.06113 pi_loss=0.0000 v_loss=6.6559 ent=0.144 kl=0.0004 clip=0.006 eps_done=491 fps=828.0\n",
      "[rollout 286] step=292864 roll_sum=18.021 roll_mean=0.01760 pi_loss=0.0012 v_loss=10.2131 ent=0.194 kl=0.0003 clip=0.005 eps_done=492 fps=828.7\n",
      "[rollout 287] step=293888 roll_sum=-3.427 roll_mean=-0.00335 pi_loss=0.0001 v_loss=4.6886 ent=0.198 kl=0.0015 clip=0.015 eps_done=494 fps=829.4\n",
      "[rollout 288] step=294912 roll_sum=5.002 roll_mean=0.00488 pi_loss=0.0009 v_loss=4.8985 ent=0.206 kl=-0.0001 clip=0.004 eps_done=495 fps=830.1\n",
      "[rollout 289] step=295936 roll_sum=47.827 roll_mean=0.04671 pi_loss=0.0009 v_loss=16.0809 ent=0.141 kl=0.0018 clip=0.026 eps_done=498 fps=830.8\n",
      "[rollout 290] step=296960 roll_sum=248.421 roll_mean=0.24260 pi_loss=0.0117 v_loss=59.1468 ent=0.198 kl=0.0211 clip=0.052 eps_done=500 fps=831.6\n",
      "  [val] {'episode_reward': 27.720065597541776, 'episode_steps': 187.65, 'num_trades': 2.15, 'win_rate': 0.8333333333333333, 'avg_trade_return': 11.507668158630471, 'avg_hold_steps': 100.48333333333333, 'sum_trade_return': 27.555129159522874}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout290.pt\n",
      "[rollout 291] step=297984 roll_sum=39.908 roll_mean=0.03897 pi_loss=-0.0004 v_loss=9.8613 ent=0.200 kl=0.0042 clip=0.019 eps_done=502 fps=826.1\n",
      "[rollout 292] step=299008 roll_sum=98.362 roll_mean=0.09606 pi_loss=-0.0006 v_loss=11.5467 ent=0.258 kl=0.0023 clip=0.023 eps_done=504 fps=826.8\n",
      "[rollout 293] step=300032 roll_sum=179.420 roll_mean=0.17521 pi_loss=0.0130 v_loss=39.6222 ent=0.242 kl=0.0166 clip=0.052 eps_done=505 fps=827.5\n",
      "[rollout 294] step=301056 roll_sum=-18.633 roll_mean=-0.01820 pi_loss=0.0077 v_loss=15.1429 ent=0.380 kl=0.0276 clip=0.183 eps_done=506 fps=828.2\n",
      "[rollout 295] step=302080 roll_sum=67.620 roll_mean=0.06604 pi_loss=0.0002 v_loss=20.2167 ent=0.222 kl=0.0045 clip=0.035 eps_done=509 fps=828.8\n",
      "[rollout 296] step=303104 roll_sum=30.347 roll_mean=0.02964 pi_loss=-0.0005 v_loss=8.4612 ent=0.218 kl=0.0029 clip=0.039 eps_done=511 fps=829.4\n",
      "[rollout 297] step=304128 roll_sum=21.442 roll_mean=0.02094 pi_loss=0.0030 v_loss=7.2776 ent=0.219 kl=0.0016 clip=0.020 eps_done=514 fps=830.1\n",
      "[rollout 298] step=305152 roll_sum=27.350 roll_mean=0.02671 pi_loss=0.0010 v_loss=8.1137 ent=0.271 kl=0.0020 clip=0.030 eps_done=516 fps=830.8\n",
      "[rollout 299] step=306176 roll_sum=-24.151 roll_mean=-0.02358 pi_loss=-0.0004 v_loss=5.6249 ent=0.207 kl=0.0015 clip=0.011 eps_done=517 fps=831.2\n",
      "[rollout 300] step=307200 roll_sum=86.987 roll_mean=0.08495 pi_loss=0.0042 v_loss=30.6935 ent=0.197 kl=0.0015 clip=0.009 eps_done=519 fps=831.9\n",
      "  [val] {'episode_reward': 24.297919195388523, 'episode_steps': 186.25, 'num_trades': 3.5, 'win_rate': 0.736547619047619, 'avg_trade_return': 6.559756257835642, 'avg_hold_steps': 67.01464285714285, 'sum_trade_return': 24.426713834809437}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout300.pt\n",
      "[rollout 301] step=308224 roll_sum=-21.253 roll_mean=-0.02076 pi_loss=-0.0009 v_loss=20.0810 ent=0.211 kl=0.0007 clip=0.009 eps_done=520 fps=826.6\n",
      "[rollout 302] step=309248 roll_sum=-24.267 roll_mean=-0.02370 pi_loss=0.0084 v_loss=11.4827 ent=0.271 kl=0.0130 clip=0.100 eps_done=522 fps=827.2\n",
      "[rollout 303] step=310272 roll_sum=23.431 roll_mean=0.02288 pi_loss=-0.0025 v_loss=13.4555 ent=0.342 kl=0.0023 clip=0.042 eps_done=523 fps=827.9\n",
      "[rollout 304] step=311296 roll_sum=90.634 roll_mean=0.08851 pi_loss=0.0008 v_loss=13.5181 ent=0.348 kl=0.0017 clip=0.003 eps_done=524 fps=828.5\n",
      "[rollout 305] step=312320 roll_sum=-22.017 roll_mean=-0.02150 pi_loss=-0.0008 v_loss=9.6460 ent=0.256 kl=0.0012 clip=0.027 eps_done=526 fps=829.2\n",
      "[rollout 306] step=313344 roll_sum=37.386 roll_mean=0.03651 pi_loss=-0.0015 v_loss=13.0697 ent=0.314 kl=0.0030 clip=0.025 eps_done=527 fps=829.9\n",
      "[rollout 307] step=314368 roll_sum=40.241 roll_mean=0.03930 pi_loss=0.0003 v_loss=16.1673 ent=0.243 kl=0.0002 clip=0.003 eps_done=529 fps=830.7\n",
      "[rollout 308] step=315392 roll_sum=19.132 roll_mean=0.01868 pi_loss=-0.0022 v_loss=21.1223 ent=0.238 kl=-0.0000 clip=0.012 eps_done=532 fps=831.4\n",
      "[rollout 309] step=316416 roll_sum=70.202 roll_mean=0.06856 pi_loss=0.0068 v_loss=23.2206 ent=0.211 kl=0.0079 clip=0.029 eps_done=534 fps=832.1\n",
      "[rollout 310] step=317440 roll_sum=56.571 roll_mean=0.05525 pi_loss=0.0009 v_loss=14.8581 ent=0.162 kl=-0.0007 clip=0.003 eps_done=535 fps=832.5\n",
      "  [val] {'episode_reward': 24.611522431512476, 'episode_steps': 200.9, 'num_trades': 3.3, 'win_rate': 0.6266666666666667, 'avg_trade_return': 5.603221499658277, 'avg_hold_steps': 78.03, 'sum_trade_return': 24.712769850026426}\n",
      "  [save] saves\\ppo_ppo_aapl_final_rollout310.pt\n",
      "[rollout 311] step=318464 roll_sum=-5.976 roll_mean=-0.00584 pi_loss=0.0005 v_loss=12.4052 ent=0.193 kl=0.0008 clip=0.010 eps_done=536 fps=826.7\n",
      "[rollout 312] step=319488 roll_sum=7.021 roll_mean=0.00686 pi_loss=0.0060 v_loss=9.4971 ent=0.278 kl=0.0016 clip=0.059 eps_done=537 fps=827.3\n",
      "[rollout 313] step=320512 roll_sum=29.012 roll_mean=0.02833 pi_loss=0.0016 v_loss=9.1226 ent=0.396 kl=0.0020 clip=0.022 eps_done=538 fps=828.0\n",
      "[rollout 314] step=321536 roll_sum=37.778 roll_mean=0.03689 pi_loss=0.0015 v_loss=6.9958 ent=0.403 kl=0.0146 clip=0.133 eps_done=541 fps=828.6\n",
      "[rollout 315] step=322560 roll_sum=-10.773 roll_mean=-0.01052 pi_loss=0.0007 v_loss=7.4924 ent=0.428 kl=0.0063 clip=0.114 eps_done=542 fps=829.2\n",
      "[rollout 316] step=323584 roll_sum=14.759 roll_mean=0.01441 pi_loss=-0.0016 v_loss=8.2054 ent=0.399 kl=0.0027 clip=0.067 eps_done=544 fps=829.9\n",
      "[rollout 317] step=324608 roll_sum=40.051 roll_mean=0.03911 pi_loss=0.0149 v_loss=17.1569 ent=0.235 kl=0.0100 clip=0.043 eps_done=546 fps=830.6\n",
      "[rollout 318] step=325632 roll_sum=-8.281 roll_mean=-0.00809 pi_loss=0.0101 v_loss=8.1006 ent=0.494 kl=0.0205 clip=0.250 eps_done=547 fps=831.4\n",
      "[rollout 319] step=326656 roll_sum=-3.862 roll_mean=-0.00377 pi_loss=0.0021 v_loss=4.9213 ent=0.241 kl=-0.0000 clip=0.022 eps_done=550 fps=832.1\n",
      "[rollout 320] step=327680 roll_sum=-19.179 roll_mean=-0.01873 pi_loss=0.0008 v_loss=17.0190 ent=0.222 kl=0.0017 clip=0.010 eps_done=551 fps=832.7\n",
      "  [val] {'episode_reward': 30.396168957225427, 'episode_steps': 187.2, 'num_trades': 6.0, 'win_rate': 0.7711904761904763, 'avg_trade_return': 5.401886202062953, 'avg_hold_steps': 34.921309523809526, 'sum_trade_return': 31.03403984221078}\n",
      "[PPO] early stopping: no val improvement for 20 validations.\n",
      "\n",
      "✓ Training completed!\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cell 10: Load best model + final validation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T18:17:58.051175Z",
     "start_time": "2025-12-28T18:17:43.306447Z"
    }
   },
   "source": [
    "best_path = os.path.join(\"saves\", f\"ppo_{config['run_name']}_best.pt\")\n",
    "if os.path.exists(best_path):\n",
    "    state = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    print(f\"Loaded best model from: {best_path}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    final_val = validation_run_ppo(env_val, model, episodes=100, device=device, greedy=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Final Validation Results (100 episodes):\")\n",
    "    print(\"=\" * 60)\n",
    "    for k, v in final_val.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(f\"Best model checkpoint not found: {best_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from: saves\\ppo_ppo_aapl_final_best.pt\n",
      "\n",
      "============================================================\n",
      "Final Validation Results (100 episodes):\n",
      "============================================================\n",
      "  episode_reward: 33.0629\n",
      "  episode_steps: 203.9000\n",
      "  num_trades: 1.0000\n",
      "  win_rate: 0.9400\n",
      "  avg_trade_return: 32.5245\n",
      "  avg_hold_steps: 202.9000\n",
      "  sum_trade_return: 32.5245\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **TensorBoard**: View training metrics with `tensorboard --logdir runs/`\n",
    "- **Checkpoints**: Best model saved to `saves/ppo_{run_name}_best.pt`\n",
    "- **Configuration**: Adjust hyperparameters in the Configuration cell (Section 5)\n",
    "- **Early Stopping**: Enabled by default based on validation performance\n",
    "- **Data Split**: Chronological split prevents data leakage (recommended)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
