{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training for Stock Trading\n",
    "\n",
    "This notebook implements Proximal Policy Optimization (PPO) for training a reinforcement learning agent to trade stocks.\n",
    "\n",
    "## Overview\n",
    "- **Environment**: Stock trading environment using historical price data\n",
    "- **Algorithm**: PPO (Proximal Policy Optimization)\n",
    "- **Model**: Actor-Critic network (MLP or CNN)\n",
    "- **Features**: Volume, extra features (volatility, ATR-like), chronological train/val split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T17:16:32.484866Z",
     "start_time": "2025-12-28T17:16:32.474364Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Import our stock trading environment package\n",
    "from stock_trading_env import (\n",
    "    StocksEnv,\n",
    "    load_many_from_dir,\n",
    "    split_many_by_ratio,\n",
    "    Actions,\n",
    ")\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed: int):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "os.makedirs(\"saves\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available: False\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PPO Model Definitions\n",
    "\n",
    "Define the Actor-Critic networks for PPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T17:16:45.452081Z",
     "start_time": "2025-12-28T17:16:45.436623Z"
    }
   },
   "source": [
    "class ActorCriticMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    PPO Actor-Critic for DISCRETE actions (MLP version).\n",
    "    \n",
    "    Input:  (B, obs_dim)\n",
    "    Output: logits (B, n_actions), value (B,)\n",
    "      - logits are unnormalized scores for a Categorical distribution\n",
    "      - value is V(s) baseline for advantage estimation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_dim: int, n_actions: int, hidden: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.policy_head = nn.Linear(hidden, n_actions)  # actor logits\n",
    "        self.value_head = nn.Linear(hidden, 1)           # critic value\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        \"\"\"PPO is usually more stable with small initial weights.\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "        # Slightly smaller init for final policy head can help early training\n",
    "        nn.init.orthogonal_(self.policy_head.weight, gain=0.01)\n",
    "        nn.init.constant_(self.policy_head.bias, 0.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"x: float tensor (B, obs_dim), returns: logits (B, n_actions), value (B,)\"\"\"\n",
    "        z = self.shared(x)\n",
    "        logits = self.policy_head(z)\n",
    "        value = self.value_head(z).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class ActorCriticConv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    PPO Actor-Critic for State1D observations (CNN version).\n",
    "    \n",
    "    Input:  (B, C, T)\n",
    "    Output: logits (B, n_actions), value (B,)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, in_channels: int, n_actions: int, bars_count: int, hidden: int = 256):\n",
    "        super().__init__()\n",
    "        \n",
    "        # A small 1D CNN feature extractor\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "        )\n",
    "        \n",
    "        # Compute conv output size = 64 * bars_count\n",
    "        conv_out = 64 * bars_count\n",
    "        \n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(conv_out, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        self.policy_head = nn.Linear(hidden, n_actions)\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "        \n",
    "        nn.init.orthogonal_(self.policy_head.weight, gain=0.01)\n",
    "        nn.init.constant_(self.policy_head.bias, 0.0)\n",
    "    \n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"x: float tensor (B, C, T), returns: logits (B, n_actions), value (B,)\"\"\"\n",
    "        feat = self.conv(x)\n",
    "        z = self.shared(feat)\n",
    "        logits = self.policy_head(z)\n",
    "        value = self.value_head(z).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "print(\"✓ PPO models defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PPO models defined\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO Buffer and Utilities\n",
    "\n",
    "Define the rollout buffer for collecting experience and computing GAE advantages.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T17:17:07.295459Z",
     "start_time": "2025-12-28T17:17:07.269617Z"
    }
   },
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Iterator\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PPOBatch:\n",
    "    \"\"\"One minibatch used during PPO updates.\"\"\"\n",
    "    obs: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "    old_logprobs: torch.Tensor\n",
    "    advantages: torch.Tensor\n",
    "    returns: torch.Tensor\n",
    "    old_values: torch.Tensor\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Stores a fixed-length rollout of experience and computes:\n",
    "      - GAE(lambda) advantages\n",
    "      - returns (targets for value function)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, obs_shape, size: int, device=\"cpu\", dtype=np.float32):\n",
    "        if isinstance(obs_shape, int):\n",
    "            obs_shape = (obs_shape,)\n",
    "        self.obs_shape = tuple(obs_shape)\n",
    "        self.size = int(size)\n",
    "        self.device = device\n",
    "        \n",
    "        # Storage\n",
    "        self.obs = np.zeros((self.size, *self.obs_shape), dtype=dtype)\n",
    "        self.actions = np.zeros((self.size,), dtype=np.int64)\n",
    "        self.rewards = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.values = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.logprobs = np.zeros((self.size,), dtype=np.float32)\n",
    "        \n",
    "        # Computed after rollout\n",
    "        self.advantages = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.returns = np.zeros((self.size,), dtype=np.float32)\n",
    "        \n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "    \n",
    "    def reset(self):\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "    \n",
    "    def add(self, obs: np.ndarray, action: int, reward: float, done: bool, \n",
    "            value: float, logprob: float):\n",
    "        \"\"\"Add one transition.\"\"\"\n",
    "        if self.ptr >= self.size:\n",
    "            raise RuntimeError(\"RolloutBuffer is full. Call reset() before adding more.\")\n",
    "        \n",
    "        self.obs[self.ptr] = np.asarray(obs, dtype=self.obs.dtype)\n",
    "        self.actions[self.ptr] = int(action)\n",
    "        self.rewards[self.ptr] = float(reward)\n",
    "        self.dones[self.ptr] = 1.0 if bool(done) else 0.0\n",
    "        self.values[self.ptr] = float(value)\n",
    "        self.logprobs[self.ptr] = float(logprob)\n",
    "        \n",
    "        self.ptr += 1\n",
    "        if self.ptr == self.size:\n",
    "            self.full = True\n",
    "    \n",
    "    def compute_gae(self, last_value: float, gamma: float = 0.99, \n",
    "                    lam: float = 0.95, normalize_adv: bool = True):\n",
    "        \"\"\"Compute advantages and returns using GAE(lambda).\"\"\"\n",
    "        if not self.full:\n",
    "            raise RuntimeError(\"RolloutBuffer not full. Collect 'size' steps before compute_gae().\")\n",
    "        \n",
    "        adv = 0.0\n",
    "        for t in reversed(range(self.size)):\n",
    "            mask = 1.0 - self.dones[t]  # 0 if terminal else 1\n",
    "            next_value = last_value if t == self.size - 1 else self.values[t + 1]\n",
    "            delta = self.rewards[t] + gamma * next_value * mask - self.values[t]\n",
    "            adv = delta + gamma * lam * mask * adv\n",
    "            self.advantages[t] = adv\n",
    "        \n",
    "        self.returns = self.advantages + self.values\n",
    "        \n",
    "        if normalize_adv:\n",
    "            m = float(self.advantages.mean())\n",
    "            s = float(self.advantages.std()) + 1e-8\n",
    "            self.advantages = (self.advantages - m) / s\n",
    "    \n",
    "    def get_batches(self, batch_size: int, shuffle: bool = True) -> Iterator[PPOBatch]:\n",
    "        \"\"\"Yield mini-batches as torch tensors.\"\"\"\n",
    "        if not self.full:\n",
    "            raise RuntimeError(\"RolloutBuffer not full. Collect rollout before batching.\")\n",
    "        \n",
    "        idxs = np.arange(self.size)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "        \n",
    "        for start in range(0, self.size, batch_size):\n",
    "            b_idx = idxs[start:start + batch_size]\n",
    "            \n",
    "            obs_t = torch.as_tensor(self.obs[b_idx], device=self.device, dtype=torch.float32)\n",
    "            actions_t = torch.as_tensor(self.actions[b_idx], device=self.device, dtype=torch.long)\n",
    "            old_logp_t = torch.as_tensor(self.logprobs[b_idx], device=self.device, dtype=torch.float32)\n",
    "            adv_t = torch.as_tensor(self.advantages[b_idx], device=self.device, dtype=torch.float32)\n",
    "            ret_t = torch.as_tensor(self.returns[b_idx], device=self.device, dtype=torch.float32)\n",
    "            old_v_t = torch.as_tensor(self.values[b_idx], device=self.device, dtype=torch.float32)\n",
    "            \n",
    "            yield PPOBatch(\n",
    "                obs=obs_t,\n",
    "                actions=actions_t,\n",
    "                old_logprobs=old_logp_t,\n",
    "                advantages=adv_t,\n",
    "                returns=ret_t,\n",
    "                old_values=old_v_t,\n",
    "            )\n",
    "\n",
    "print(\"✓ PPO buffer defined\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PPO buffer defined\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def policy_act(model, obs, device: torch.device, greedy: bool = False):\n",
    "    \"\"\"\n",
    "    Sample (or greedy-select) action from current policy.\n",
    "    Returns: action(int), logprob(float), value(float)\n",
    "    \"\"\"\n",
    "    obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    logits, value = model(obs_t)\n",
    "    dist = Categorical(logits=logits)\n",
    "    \n",
    "    if greedy:\n",
    "        action_t = torch.argmax(logits, dim=1)\n",
    "    else:\n",
    "        action_t = dist.sample()\n",
    "    \n",
    "    logprob_t = dist.log_prob(action_t)\n",
    "    return int(action_t.item()), float(logprob_t.item()), float(value.item())\n",
    "\n",
    "\n",
    "def explained_variance(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"1 - Var[y_true - y_pred] / Var[y_true]. Diagnostic: is the critic predicting returns well?\"\"\"\n",
    "    var_y = np.var(y_true)\n",
    "    if var_y < 1e-12:\n",
    "        return 0.0\n",
    "    return float(1.0 - np.var(y_true - y_pred) / (var_y + 1e-12))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_run_ppo(env, model, episodes: int = 50, device=\"cpu\", greedy: bool = True):\n",
    "    \"\"\"\n",
    "    Evaluate a PPO policy on the given environment.\n",
    "    Returns mean metrics across episodes.\n",
    "    \"\"\"\n",
    "    device = torch.device(device) if not isinstance(device, torch.device) else device\n",
    "    base_env = env.unwrapped if hasattr(env, \"unwrapped\") else env\n",
    "    \n",
    "    stats = {\n",
    "        \"episode_reward\": [],\n",
    "        \"episode_steps\": [],\n",
    "        \"num_trades\": [],\n",
    "        \"win_rate\": [],\n",
    "        \"avg_trade_return\": [],\n",
    "        \"avg_hold_steps\": [],\n",
    "        \"sum_trade_return\": [],\n",
    "    }\n",
    "    \n",
    "    for _ in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "        \n",
    "        # Manual trade tracking\n",
    "        in_pos = False\n",
    "        entry_price = None\n",
    "        hold_steps = 0\n",
    "        trade_returns = []\n",
    "        trade_hold_steps = []\n",
    "        \n",
    "        while not done:\n",
    "            obs_v = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "            logits, _value = model(obs_v)\n",
    "            \n",
    "            if greedy:\n",
    "                action = int(torch.argmax(logits, dim=1).item())\n",
    "            else:\n",
    "                dist = Categorical(logits=logits)\n",
    "                action = int(dist.sample().item())\n",
    "            \n",
    "            act_enum = Actions(action)\n",
    "            st = base_env._state\n",
    "            next_idx = st._offset + 1\n",
    "            \n",
    "            exec_open = None\n",
    "            if 0 <= next_idx < st._prices.open.shape[0]:\n",
    "                exec_open = float(st._prices.open[next_idx])\n",
    "            \n",
    "            prev_have_pos_env = bool(st.have_position)\n",
    "            \n",
    "            # Bookkeeping BEFORE step\n",
    "            if exec_open is not None:\n",
    "                if act_enum == Actions.Buy and not in_pos:\n",
    "                    in_pos = True\n",
    "                    entry_price = exec_open\n",
    "                    hold_steps = 0\n",
    "                elif act_enum == Actions.Close and in_pos:\n",
    "                    if entry_price and entry_price > 0:\n",
    "                        tr = 100.0 * (exec_open - entry_price) / entry_price\n",
    "                    else:\n",
    "                        tr = 0.0\n",
    "                    trade_returns.append(tr)\n",
    "                    trade_hold_steps.append(hold_steps)\n",
    "                    in_pos = False\n",
    "                    entry_price = None\n",
    "                    hold_steps = 0\n",
    "            \n",
    "            # Step environment\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = bool(terminated or truncated)\n",
    "            total_reward += float(reward)\n",
    "            steps += 1\n",
    "            \n",
    "            # Detect forced close\n",
    "            now_have_pos_env = bool(base_env._state.have_position)\n",
    "            if in_pos and prev_have_pos_env and (not now_have_pos_env) and act_enum != Actions.Close:\n",
    "                exit_price = exec_open if exec_open is not None else float(base_env._state._cur_close())\n",
    "                if entry_price and entry_price > 0:\n",
    "                    tr = 100.0 * (exit_price - entry_price) / entry_price\n",
    "                else:\n",
    "                    tr = 0.0\n",
    "                trade_returns.append(tr)\n",
    "                trade_hold_steps.append(hold_steps)\n",
    "                in_pos = False\n",
    "                entry_price = None\n",
    "                hold_steps = 0\n",
    "            \n",
    "            if in_pos:\n",
    "                hold_steps += 1\n",
    "        \n",
    "        # If episode ends while holding\n",
    "        if in_pos and entry_price and entry_price > 0:\n",
    "            last_close = float(base_env._state._cur_close())\n",
    "            tr = 100.0 * (last_close - entry_price) / entry_price\n",
    "            trade_returns.append(tr)\n",
    "            trade_hold_steps.append(hold_steps)\n",
    "        \n",
    "        # Episode metrics\n",
    "        stats[\"episode_reward\"].append(total_reward)\n",
    "        stats[\"episode_steps\"].append(steps)\n",
    "        n_trades = len(trade_returns)\n",
    "        stats[\"num_trades\"].append(float(n_trades))\n",
    "        \n",
    "        if n_trades > 0:\n",
    "            wins = sum(1 for x in trade_returns if x > 0.0)\n",
    "            stats[\"win_rate\"].append(float(wins / n_trades))\n",
    "            stats[\"avg_trade_return\"].append(float(np.mean(trade_returns)))\n",
    "            stats[\"avg_hold_steps\"].append(float(np.mean(trade_hold_steps)))\n",
    "            stats[\"sum_trade_return\"].append(float(np.sum(trade_returns)))\n",
    "        else:\n",
    "            stats[\"win_rate\"].append(0.0)\n",
    "            stats[\"avg_trade_return\"].append(0.0)\n",
    "            stats[\"avg_hold_steps\"].append(0.0)\n",
    "            stats[\"sum_trade_return\"].append(0.0)\n",
    "    \n",
    "    return {k: float(np.mean(v)) for k, v in stats.items()}\n",
    "\n",
    "print(\"✓ Helper functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Training Configuration =====\n",
    "config = {\n",
    "    # Data\n",
    "    \"data_dir\": \"yf_data\",\n",
    "    \"run_name\": \"ppo_training\",\n",
    "    \"seed\": 42,\n",
    "    \n",
    "    # Device\n",
    "    \"use_cuda\": True,  # Set to False to force CPU\n",
    "    \n",
    "    # PPO Hyperparameters\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_eps\": 0.2,\n",
    "    \"lr\": 3e-4,\n",
    "    \"rollout_steps\": 1024,\n",
    "    \"minibatch\": 256,\n",
    "    \"epochs\": 5,\n",
    "    \"value_coef\": 0.5,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"target_kl\": 0.02,\n",
    "    \n",
    "    # Environment\n",
    "    \"bars\": 10,\n",
    "    \"volumes\": True,\n",
    "    \"extra_features\": True,\n",
    "    \"reward_mode\": \"close_pnl\",  # or \"step_logret\"\n",
    "    \"state_1d\": False,  # True for CNN, False for MLP\n",
    "    \"time_limit\": 1000,\n",
    "    \n",
    "    # Data Split\n",
    "    \"split\": True,  # Chronological train/val split (recommended)\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"min_train\": 200,\n",
    "    \"min_val\": 200,\n",
    "    \n",
    "    # Training Control\n",
    "    \"max_rollouts\": 500,\n",
    "    \"total_steps\": 10_000_000,\n",
    "    \n",
    "    # Validation & Checkpointing\n",
    "    \"val_every_rollouts\": 10,\n",
    "    \"save_every_rollouts\": 10,\n",
    "    \"early_stop\": True,\n",
    "    \"patience\": 20,\n",
    "    \"min_rollouts\": 50,\n",
    "    \"min_delta\": 1e-3,\n",
    "}\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if (config[\"use_cuda\"] and torch.cuda.is_available()) else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "print(\"✓ Configuration set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Prepare Data\n",
    "\n",
    "Load stock price data and split into train/validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all price data\n",
    "prices_all = load_many_from_dir(config[\"data_dir\"])\n",
    "print(f\"Loaded {len(prices_all)} instruments: {list(prices_all.keys())}\")\n",
    "\n",
    "# Split into train/validation (chronological, no leakage)\n",
    "if config[\"split\"]:\n",
    "    prices_train, prices_val = split_many_by_ratio(\n",
    "        prices_all,\n",
    "        train_ratio=config[\"train_ratio\"],\n",
    "        min_train=config[\"min_train\"],\n",
    "        min_val=config[\"min_val\"],\n",
    "    )\n",
    "    print(f\"Train instruments: {len(prices_train)}\")\n",
    "    print(f\"Validation instruments: {len(prices_val)}\")\n",
    "else:\n",
    "    prices_train = prices_all\n",
    "    prices_val = prices_all\n",
    "    print(\"No split: using same data for train and validation (in-sample)\")\n",
    "\n",
    "print(\"✓ Data loaded and split\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Environments\n",
    "\n",
    "Set up training and validation environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training environment\n",
    "env_train_base = StocksEnv(\n",
    "    prices_train,\n",
    "    bars_count=config[\"bars\"],\n",
    "    volumes=config[\"volumes\"],\n",
    "    extra_features=config[\"extra_features\"],\n",
    "    reset_on_close=False,\n",
    "    reward_on_close=False,\n",
    "    reward_mode=config[\"reward_mode\"],\n",
    "    state_1d=config[\"state_1d\"],\n",
    ")\n",
    "env_train = gym.wrappers.TimeLimit(env_train_base, max_episode_steps=config[\"time_limit\"])\n",
    "\n",
    "# Create validation environment\n",
    "env_val = StocksEnv(\n",
    "    prices_val,\n",
    "    bars_count=config[\"bars\"],\n",
    "    volumes=config[\"volumes\"],\n",
    "    extra_features=config[\"extra_features\"],\n",
    "    reset_on_close=False,\n",
    "    reward_on_close=False,\n",
    "    reward_mode=config[\"reward_mode\"],\n",
    "    state_1d=config[\"state_1d\"],\n",
    ")\n",
    "\n",
    "obs_shape = env_train.observation_space.shape\n",
    "n_actions = env_train.action_space.n\n",
    "\n",
    "print(f\"Observation space: {obs_shape}\")\n",
    "print(f\"Action space: {n_actions} actions\")\n",
    "print(\"✓ Environments created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Model and Optimizer\n",
    "\n",
    "Create the PPO model and optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build model\n",
    "if config[\"state_1d\"]:\n",
    "    C, T = obs_shape  # (channels, time)\n",
    "    model = ActorCriticConv1D(in_channels=C, n_actions=n_actions, bars_count=T).to(device)\n",
    "    print(f\"Created Conv1D model: channels={C}, bars={T}\")\n",
    "else:\n",
    "    obs_dim = obs_shape[0]\n",
    "    model = ActorCriticMLP(obs_dim=obs_dim, n_actions=n_actions).to(device)\n",
    "    print(f\"Created MLP model: obs_dim={obs_dim}\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "# Create TensorBoard writer\n",
    "writer = SummaryWriter(comment=f\"-ppo-{config['run_name']}\")\n",
    "\n",
    "print(\"✓ Model and optimizer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Main PPO training loop: collect rollouts, compute advantages, update policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training bookkeeping\n",
    "obs, info = env_train.reset(seed=config[\"seed\"])\n",
    "episode_reward = 0.0\n",
    "episode_steps = 0\n",
    "episode_count = 0\n",
    "\n",
    "global_step = 0\n",
    "rollout_idx = 0\n",
    "t0 = time.time()\n",
    "\n",
    "best_val_reward = -1e9\n",
    "no_improve = 0\n",
    "\n",
    "print(f\"[PPO] device={device} obs_shape={obs_shape} actions={n_actions}\")\n",
    "print(f\"[PPO] reward_mode={config['reward_mode']} volumes={config['volumes']} extra_features={config['extra_features']}\")\n",
    "print(f\"[PPO] split={config['split']} max_rollouts={config['max_rollouts']} early_stop={config['early_stop']}\")\n",
    "print(f\"[PPO] logs: runs/  checkpoints: saves/\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Main training loop\n",
    "while (global_step < config[\"total_steps\"]) and (rollout_idx < config[\"max_rollouts\"]):\n",
    "    rollout_idx += 1\n",
    "    buf = RolloutBuffer(obs_shape=obs_shape, size=config[\"rollout_steps\"], device=device)\n",
    "    \n",
    "    # ===== Collect Rollout =====\n",
    "    for _ in range(config[\"rollout_steps\"]):\n",
    "        global_step += 1\n",
    "        \n",
    "        action, logprob, value = policy_act(model, obs, device=device, greedy=False)\n",
    "        next_obs, reward, terminated, truncated, info = env_train.step(action)\n",
    "        \n",
    "        episode_done = bool(terminated or truncated)\n",
    "        buf_done = bool(terminated)  # Only terminated (not truncated) for GAE\n",
    "        \n",
    "        buf.add(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            reward=float(reward),\n",
    "            done=buf_done,\n",
    "            value=value,\n",
    "            logprob=logprob,\n",
    "        )\n",
    "        \n",
    "        episode_reward += float(reward)\n",
    "        episode_steps += 1\n",
    "        obs = next_obs\n",
    "        \n",
    "        if episode_done:\n",
    "            episode_count += 1\n",
    "            writer.add_scalar(\"train/episode_reward\", episode_reward, global_step)\n",
    "            writer.add_scalar(\"train/episode_steps\", episode_steps, global_step)\n",
    "            \n",
    "            obs, info = env_train.reset()\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = 0\n",
    "        \n",
    "        if global_step >= config[\"total_steps\"]:\n",
    "            break\n",
    "    \n",
    "    # Rollout reward heartbeat\n",
    "    roll_sum = float(buf.rewards.sum())\n",
    "    roll_mean = float(buf.rewards.mean())\n",
    "    writer.add_scalar(\"train/rollout_reward_sum\", roll_sum, global_step)\n",
    "    writer.add_scalar(\"train/rollout_reward_mean\", roll_mean, global_step)\n",
    "    \n",
    "    # Bootstrap last value for GAE\n",
    "    with torch.no_grad():\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        _, last_v = model(obs_t)\n",
    "        last_value = float(last_v.item())\n",
    "    \n",
    "    buf.compute_gae(\n",
    "        last_value=last_value,\n",
    "        gamma=config[\"gamma\"],\n",
    "        lam=config[\"gae_lambda\"],\n",
    "        normalize_adv=True\n",
    "    )\n",
    "    \n",
    "    # ===== PPO Update =====\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    approx_kls = []\n",
    "    clipfracs = []\n",
    "    \n",
    "    for _epoch in range(config[\"epochs\"]):\n",
    "        for batch in buf.get_batches(batch_size=config[\"minibatch\"], shuffle=True):\n",
    "            logits, values = model(batch.obs)\n",
    "            dist = Categorical(logits=logits)\n",
    "            \n",
    "            new_logp = dist.log_prob(batch.actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "            \n",
    "            # PPO ratio\n",
    "            ratio = torch.exp(new_logp - batch.old_logprobs)\n",
    "            \n",
    "            # Policy loss (clipped surrogate)\n",
    "            unclipped = ratio * batch.advantages\n",
    "            clipped = torch.clamp(ratio, 1.0 - config[\"clip_eps\"], 1.0 + config[\"clip_eps\"]) * batch.advantages\n",
    "            loss_pi = -torch.min(unclipped, clipped).mean()\n",
    "            \n",
    "            # Value loss\n",
    "            loss_v = (batch.returns - values).pow(2).mean()\n",
    "            \n",
    "            # Total loss\n",
    "            loss = loss_pi + config[\"value_coef\"] * loss_v - config[\"entropy_coef\"] * entropy\n",
    "            \n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                approx_kl = float((batch.old_logprobs - new_logp).mean().abs().item())\n",
    "                clipfrac = float((torch.abs(ratio - 1.0) > config[\"clip_eps\"]).float().mean().item())\n",
    "            \n",
    "            policy_losses.append(float(loss_pi.item()))\n",
    "            value_losses.append(float(loss_v.item()))\n",
    "            entropies.append(float(entropy.item()))\n",
    "            approx_kls.append(approx_kl)\n",
    "            clipfracs.append(clipfrac)\n",
    "        \n",
    "        # Early stop PPO epoch loop if KL too big\n",
    "        if config[\"target_kl\"] > 0 and approx_kls and (np.mean(approx_kls) > config[\"target_kl\"]):\n",
    "            break\n",
    "    \n",
    "    # ===== Logging =====\n",
    "    fps = global_step / max(1e-9, (time.time() - t0))\n",
    "    writer.add_scalar(\"ppo/policy_loss\", float(np.mean(policy_losses) if policy_losses else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/value_loss\", float(np.mean(value_losses) if value_losses else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/entropy\", float(np.mean(entropies) if entropies else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/approx_kl\", float(np.mean(approx_kls) if approx_kls else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/clipfrac\", float(np.mean(clipfracs) if clipfracs else 0.0), global_step)\n",
    "    writer.add_scalar(\"train/fps\", float(fps), global_step)\n",
    "    writer.add_scalar(\"train/episodes\", float(episode_count), global_step)\n",
    "    \n",
    "    # Value function explained variance\n",
    "    ev = explained_variance(np.asarray(buf.values), np.asarray(buf.returns))\n",
    "    writer.add_scalar(\"ppo/explained_variance\", ev, global_step)\n",
    "    \n",
    "    # Console heartbeat\n",
    "    print(\n",
    "        f\"[rollout {rollout_idx}] step={global_step} \"\n",
    "        f\"roll_sum={roll_sum:.3f} roll_mean={roll_mean:.5f} \"\n",
    "        f\"pi_loss={np.mean(policy_losses) if policy_losses else 0.0:.4f} \"\n",
    "        f\"v_loss={np.mean(value_losses) if value_losses else 0.0:.4f} \"\n",
    "        f\"ent={np.mean(entropies) if entropies else 0.0:.3f} \"\n",
    "        f\"kl={np.mean(approx_kls) if approx_kls else 0.0:.4f} \"\n",
    "        f\"clip={np.mean(clipfracs) if clipfracs else 0.0:.3f} \"\n",
    "        f\"eps_done={episode_count} fps={fps:.1f}\"\n",
    "    )\n",
    "    \n",
    "    # ===== Validation + Best Model Saving + Early Stop =====\n",
    "    if config[\"val_every_rollouts\"] > 0 and (rollout_idx % config[\"val_every_rollouts\"] == 0):\n",
    "        model.eval()\n",
    "        val = validation_run_ppo(env_val, model, episodes=20, device=device, greedy=True)\n",
    "        model.train()\n",
    "        \n",
    "        for k, v in val.items():\n",
    "            writer.add_scalar(\"val/\" + k, v, global_step)\n",
    "        \n",
    "        print(f\"  [val] {val}\")\n",
    "        \n",
    "        cur_val = float(val.get(\"episode_reward\", -1e9))\n",
    "        if cur_val > best_val_reward + config[\"min_delta\"]:\n",
    "            best_val_reward = cur_val\n",
    "            no_improve = 0\n",
    "            best_path = os.path.join(\"saves\", f\"ppo_{config['run_name']}_best.pt\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  [best] new best val episode_reward={best_val_reward:.4f} -> {best_path}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "        \n",
    "        if config[\"early_stop\"] and (rollout_idx >= config[\"min_rollouts\"]) and (no_improve >= config[\"patience\"]):\n",
    "            print(f\"[PPO] early stopping: no val improvement for {no_improve} validations.\")\n",
    "            break\n",
    "    \n",
    "    # ===== Save Periodic Checkpoint =====\n",
    "    if config[\"save_every_rollouts\"] > 0 and (rollout_idx % config[\"save_every_rollouts\"] == 0):\n",
    "        ckpt_path = os.path.join(\"saves\", f\"ppo_{config['run_name']}_rollout{rollout_idx}.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"  [save] {ckpt_path}\")\n",
    "    \n",
    "    # Hard stop condition\n",
    "    if config[\"max_rollouts\"] and rollout_idx >= config[\"max_rollouts\"]:\n",
    "        print(\"[PPO] reached max_rollouts, stopping.\")\n",
    "        break\n",
    "\n",
    "if global_step >= config[\"total_steps\"]:\n",
    "    print(f\"[done] reached total_steps={config['total_steps']} at rollout={rollout_idx}\")\n",
    "elif rollout_idx >= config[\"max_rollouts\"]:\n",
    "    print(f\"[done] reached max_rollouts={config['max_rollouts']} at step={global_step}\")\n",
    "\n",
    "writer.close()\n",
    "print(\"\\n✓ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "best_path = os.path.join(\"saves\", f\"ppo_{config['run_name']}_best.pt\")\n",
    "if os.path.exists(best_path):\n",
    "    model.load_state_dict(torch.load(best_path, map_location=device))\n",
    "    print(f\"Loaded best model from: {best_path}\")\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    final_val = validation_run_ppo(env_val, model, episodes=100, device=device, greedy=True)\n",
    "    model.train()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Final Validation Results (100 episodes):\")\n",
    "    print(\"=\" * 60)\n",
    "    for k, v in final_val.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(f\"Best model checkpoint not found: {best_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **TensorBoard**: View training metrics with `tensorboard --logdir runs/`\n",
    "- **Checkpoints**: Best model saved to `saves/ppo_{run_name}_best.pt`\n",
    "- **Configuration**: Adjust hyperparameters in the Configuration cell (Section 5)\n",
    "- **Early Stopping**: Enabled by default based on validation performance\n",
    "- **Data Split**: Chronological split prevents data leakage (recommended)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
