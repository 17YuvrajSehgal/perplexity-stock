{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Training for Stock Trading\n",
    "\n",
    "This notebook implements Proximal Policy Optimization (PPO) for training a reinforcement learning agent to trade stocks.\n",
    "\n",
    "## Overview\n",
    "- **Environment**: Stock trading environment using historical price data\n",
    "- **Algorithm**: PPO (Proximal Policy Optimization)\n",
    "- **Model**: Actor-Critic network (MLP or CNN)\n",
    "- **Features**: Volume, extra features (volatility, ATR-like), chronological train/val split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:10:34.851063Z",
     "start_time": "2025-12-28T19:10:34.838642Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "# Import our stock trading environment package\n",
    "from stock_trading_env import (\n",
    "    StocksEnv,\n",
    "    load_many_from_dir,\n",
    "    split_many_by_ratio,\n",
    "    Actions,\n",
    ")\n",
    "\n",
    "# ---------- Reproducibility ----------\n",
    "def set_seed(seed: int, deterministic: bool = True):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    if deterministic:\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Pick a default seed (match your CLI default if you had one)\n",
    "SEED = 0\n",
    "set_seed(SEED, deterministic=True)\n",
    "\n",
    "# ---------- Device ----------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ---------- Output directories ----------\n",
    "os.makedirs(\"runs\", exist_ok=True)\n",
    "os.makedirs(\"saves\", exist_ok=True)\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"Seed: {SEED} (deterministic=True)\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "print(f\"Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Imports successful\n",
      "Seed: 0 (deterministic=True)\n",
      "PyTorch version: 2.9.0+cpu\n",
      "CUDA available: False\n",
      "Device: cpu\n"
     ]
    }
   ],
   "execution_count": 39
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PPO Model Definitions\n",
    "\n",
    "Define the Actor-Critic networks for PPO.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:10:34.888782Z",
     "start_time": "2025-12-28T19:10:34.876200Z"
    }
   },
   "source": [
    "# Cell 2: PPO Actor-Critic models (DISCRETE actions) — matches earlier repo logic\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class ActorCriticMLP(nn.Module):\n",
    "    \"\"\"\n",
    "    PPO Actor-Critic for DISCRETE actions (vector observation).\n",
    "\n",
    "    Input:  x (B, obs_dim)\n",
    "    Output: logits (B, n_actions), value (B,)\n",
    "      - logits are unnormalized scores for torch.distributions.Categorical(logits=logits)\n",
    "      - value is V(s) for GAE/advantage estimation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, n_actions: int, hidden: int = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(obs_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Linear(hidden, n_actions)\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Orthogonal init is common for PPO\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        # Smaller gain for the final policy head helps early training stability\n",
    "        nn.init.orthogonal_(self.policy_head.weight, gain=0.01)\n",
    "        nn.init.constant_(self.policy_head.bias, 0.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        z = self.shared(x)\n",
    "        logits = self.policy_head(z)\n",
    "        value = self.value_head(z).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "class ActorCriticConv1D(nn.Module):\n",
    "    \"\"\"\n",
    "    PPO Actor-Critic for State1D observations (CNN over time bars).\n",
    "\n",
    "    Expected env State1D shape (C, T) where:\n",
    "      - C = (3 + (1 if volumes else 0)) + 2  -> typically 6 if volumes=True\n",
    "      - T = bars_count (e.g., 10)\n",
    "\n",
    "    Input:  x (B, C, T)\n",
    "    Output: logits (B, n_actions), value (B,)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_actions: int, bars_count: int, volumes: bool = True, hidden: int = 256):\n",
    "        super().__init__()\n",
    "\n",
    "        # Match env.State1D channel logic:\n",
    "        # base channels: 3 + (1 if volumes else 0)\n",
    "        # +2 for (have_position, unrealized_return)\n",
    "        in_channels = (3 + (1 if volumes else 0)) + 2\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv1d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),  # -> (B, 64 * bars_count)\n",
    "        )\n",
    "\n",
    "        conv_out = 64 * bars_count\n",
    "\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(conv_out, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden, hidden),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.policy_head = nn.Linear(hidden, n_actions)\n",
    "        self.value_head = nn.Linear(hidden, 1)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv1d)):\n",
    "                nn.init.orthogonal_(m.weight, gain=1.0)\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0.0)\n",
    "\n",
    "        nn.init.orthogonal_(self.policy_head.weight, gain=0.01)\n",
    "        nn.init.constant_(self.policy_head.bias, 0.0)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        feat = self.conv(x)\n",
    "        z = self.shared(feat)\n",
    "        logits = self.policy_head(z)\n",
    "        value = self.value_head(z).squeeze(-1)\n",
    "        return logits, value\n",
    "\n",
    "\n",
    "print(\"✓ PPO models defined (MLP + Conv1D)\")\n",
    "print(\"  - Use ActorCriticMLP for vector obs\")\n",
    "print(\"  - Use ActorCriticConv1D only if env returns State1D (C,T), e.g. (6, bars)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PPO models defined (MLP + Conv1D)\n",
      "  - Use ActorCriticMLP for vector obs\n",
      "  - Use ActorCriticConv1D only if env returns State1D (C,T), e.g. (6, bars)\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PPO Buffer and Utilities\n",
    "\n",
    "Define the rollout buffer for collecting experience and computing GAE advantages.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:10:34.920923Z",
     "start_time": "2025-12-28T19:10:34.902987Z"
    }
   },
   "source": [
    "# Cell 3 (updated): RolloutBuffer + PPOBatch\n",
    "# Fixes/robustness:\n",
    "# - Safer dtype/device handling\n",
    "# - Enforces add() obs shape\n",
    "# - Handles truncated/terminated properly via done float\n",
    "# - Supports 1D (vector) and 2D (C,T) observations cleanly\n",
    "# - Compute GAE in float32, normalize advantages safely\n",
    "# - Optional pin_memory for faster CPU->GPU transfer\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Iterator, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PPOBatch:\n",
    "    \"\"\"One minibatch used during PPO updates.\"\"\"\n",
    "    obs: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "    old_logprobs: torch.Tensor\n",
    "    advantages: torch.Tensor\n",
    "    returns: torch.Tensor\n",
    "    old_values: torch.Tensor\n",
    "\n",
    "\n",
    "class RolloutBuffer:\n",
    "    \"\"\"\n",
    "    Fixed-length rollout buffer for PPO.\n",
    "\n",
    "    Stores:\n",
    "      obs[t], action[t], reward[t], done[t], value[t], logprob[t]\n",
    "\n",
    "    After calling compute_gae():\n",
    "      advantages[t], returns[t] are filled.\n",
    "\n",
    "    Notes:\n",
    "      - done should be True when episode ended for ANY reason:\n",
    "        done = terminated OR truncated (Gymnasium).\n",
    "      - obs can be vector (obs_dim,) or State1D (C,T).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        obs_shape: Union[int, Tuple[int, ...]],\n",
    "        size: int,\n",
    "        device: Union[str, torch.device] = \"cpu\",\n",
    "        obs_dtype: np.dtype = np.float32,\n",
    "        pin_memory: bool = False,\n",
    "    ):\n",
    "        if isinstance(obs_shape, int):\n",
    "            obs_shape = (obs_shape,)\n",
    "        self.obs_shape = tuple(obs_shape)\n",
    "        self.size = int(size)\n",
    "        self.device = torch.device(device) if not isinstance(device, torch.device) else device\n",
    "        self.pin_memory = bool(pin_memory) and (self.device.type == \"cuda\")\n",
    "\n",
    "        # Storage (numpy)\n",
    "        self.obs = np.zeros((self.size, *self.obs_shape), dtype=obs_dtype)\n",
    "        self.actions = np.zeros((self.size,), dtype=np.int64)\n",
    "        self.rewards = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.dones = np.zeros((self.size,), dtype=np.float32)   # 1.0 if done else 0.0\n",
    "        self.values = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.logprobs = np.zeros((self.size,), dtype=np.float32)\n",
    "\n",
    "        # Computed after rollout\n",
    "        self.advantages = np.zeros((self.size,), dtype=np.float32)\n",
    "        self.returns = np.zeros((self.size,), dtype=np.float32)\n",
    "\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "    def reset(self):\n",
    "        self.ptr = 0\n",
    "        self.full = False\n",
    "\n",
    "    def add(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        action: int,\n",
    "        reward: float,\n",
    "        done: bool,\n",
    "        value: float,\n",
    "        logprob: float,\n",
    "    ):\n",
    "        \"\"\"Add one transition.\"\"\"\n",
    "        if self.ptr >= self.size:\n",
    "            raise RuntimeError(\"RolloutBuffer is full. Call reset() before adding more.\")\n",
    "\n",
    "        obs_arr = np.asarray(obs, dtype=self.obs.dtype)\n",
    "\n",
    "        # Enforce shape to catch silent bugs early (common in notebooks)\n",
    "        if obs_arr.shape != self.obs_shape:\n",
    "            raise ValueError(\n",
    "                f\"Obs shape mismatch: got {obs_arr.shape}, expected {self.obs_shape}. \"\n",
    "                \"Check env output (vector vs State1D) and model input.\"\n",
    "            )\n",
    "\n",
    "        self.obs[self.ptr] = obs_arr\n",
    "        self.actions[self.ptr] = int(action)\n",
    "        self.rewards[self.ptr] = float(reward)\n",
    "        self.dones[self.ptr] = 1.0 if bool(done) else 0.0\n",
    "        self.values[self.ptr] = float(value)\n",
    "        self.logprobs[self.ptr] = float(logprob)\n",
    "\n",
    "        self.ptr += 1\n",
    "        self.full = (self.ptr == self.size)\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def compute_gae(\n",
    "        self,\n",
    "        last_value: float,\n",
    "        gamma: float = 0.99,\n",
    "        lam: float = 0.95,\n",
    "        normalize_adv: bool = True,\n",
    "        eps: float = 1e-8,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute GAE(lambda) advantages and returns.\n",
    "\n",
    "        last_value should be V(s_T) for the *last obs after the rollout*,\n",
    "        used to bootstrap if the last transition was not terminal.\n",
    "        \"\"\"\n",
    "        if not self.full:\n",
    "            raise RuntimeError(\"RolloutBuffer not full. Collect 'size' steps before compute_gae().\")\n",
    "\n",
    "        last_value = float(last_value)\n",
    "\n",
    "        adv = 0.0\n",
    "        for t in reversed(range(self.size)):\n",
    "            done = self.dones[t]\n",
    "            mask = 1.0 - done  # 0 if terminal else 1\n",
    "\n",
    "            next_value = last_value if (t == self.size - 1) else float(self.values[t + 1])\n",
    "            delta = float(self.rewards[t]) + gamma * next_value * mask - float(self.values[t])\n",
    "\n",
    "            adv = delta + gamma * lam * mask * adv\n",
    "            self.advantages[t] = adv\n",
    "\n",
    "        # returns target for critic\n",
    "        self.returns = self.advantages + self.values\n",
    "\n",
    "        if normalize_adv:\n",
    "            m = float(self.advantages.mean())\n",
    "            s = float(self.advantages.std())\n",
    "            self.advantages = (self.advantages - m) / (s + eps)\n",
    "\n",
    "    def get_batches(self, batch_size: int, shuffle: bool = True) -> Iterator[PPOBatch]:\n",
    "        \"\"\"Yield mini-batches as torch tensors.\"\"\"\n",
    "        if not self.full:\n",
    "            raise RuntimeError(\"RolloutBuffer not full. Collect rollout before batching.\")\n",
    "\n",
    "        if batch_size <= 0:\n",
    "            raise ValueError(\"batch_size must be > 0\")\n",
    "\n",
    "        idxs = np.arange(self.size)\n",
    "        if shuffle:\n",
    "            np.random.shuffle(idxs)\n",
    "\n",
    "        for start in range(0, self.size, batch_size):\n",
    "            b_idx = idxs[start : start + batch_size]\n",
    "\n",
    "            # CPU tensors first (optionally pinned), then move to device\n",
    "            obs_cpu = torch.from_numpy(self.obs[b_idx]).float()\n",
    "            if self.pin_memory:\n",
    "                obs_cpu = obs_cpu.pin_memory()\n",
    "            obs_t = obs_cpu.to(self.device, non_blocking=self.pin_memory)\n",
    "\n",
    "            actions_t = torch.from_numpy(self.actions[b_idx]).long().to(self.device)\n",
    "            old_logp_t = torch.from_numpy(self.logprobs[b_idx]).float().to(self.device)\n",
    "            adv_t = torch.from_numpy(self.advantages[b_idx]).float().to(self.device)\n",
    "            ret_t = torch.from_numpy(self.returns[b_idx]).float().to(self.device)\n",
    "            old_v_t = torch.from_numpy(self.values[b_idx]).float().to(self.device)\n",
    "\n",
    "            yield PPOBatch(\n",
    "                obs=obs_t,\n",
    "                actions=actions_t,\n",
    "                old_logprobs=old_logp_t,\n",
    "                advantages=adv_t,\n",
    "                returns=ret_t,\n",
    "                old_values=old_v_t,\n",
    "            )\n",
    "\n",
    "\n",
    "print(\"✓ PPO buffer defined (updated/robust)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PPO buffer defined (updated/robust)\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cell 4"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:10:34.957071Z",
     "start_time": "2025-12-28T19:10:34.939684Z"
    }
   },
   "source": [
    "from typing import Dict, Any\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "@torch.no_grad()\n",
    "def policy_act(model, obs, device: torch.device, greedy: bool = False):\n",
    "    \"\"\"\n",
    "    Sample (or greedy-select) action from current policy.\n",
    "    Returns: action(int), logprob(float), value(float)\n",
    "    Works for both:\n",
    "      - vector obs: (obs_dim,)\n",
    "      - State1D obs: (C, T)\n",
    "    \"\"\"\n",
    "    obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "    logits, value = model(obs_t)\n",
    "    dist = Categorical(logits=logits)\n",
    "\n",
    "    if greedy:\n",
    "        action_t = torch.argmax(logits, dim=1)\n",
    "    else:\n",
    "        action_t = dist.sample()\n",
    "\n",
    "    logprob_t = dist.log_prob(action_t)\n",
    "    return int(action_t.item()), float(logprob_t.item()), float(value.item())\n",
    "\n",
    "\n",
    "def explained_variance(y_pred: np.ndarray, y_true: np.ndarray) -> float:\n",
    "    \"\"\"1 - Var[y_true - y_pred] / Var[y_true]. Diagnostic for critic fit.\"\"\"\n",
    "    var_y = float(np.var(y_true))\n",
    "    if var_y < 1e-12:\n",
    "        return 0.0\n",
    "    return float(1.0 - np.var(y_true - y_pred) / (var_y + 1e-12))\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def validation_run_ppo(env, model, episodes: int = 50, device=\"cpu\", greedy: bool = True) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Evaluate a PPO policy on the given environment.\n",
    "    Returns mean metrics across episodes.\n",
    "\n",
    "    IMPORTANT: This assumes the env executes actions at OPEN(t+1), consistent with your repo env.\n",
    "    \"\"\"\n",
    "    device = torch.device(device) if not isinstance(device, torch.device) else device\n",
    "\n",
    "    # Ensure deterministic eval behavior (dropout/bn etc.)\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "\n",
    "    base_env = env.unwrapped if hasattr(env, \"unwrapped\") else env\n",
    "    st = getattr(base_env, \"_state\", None)\n",
    "    if st is None:\n",
    "        # restore mode before raising\n",
    "        if was_training:\n",
    "            model.train()\n",
    "        raise AttributeError(\"validation_run_ppo expected env.unwrapped to have attribute '_state'.\")\n",
    "\n",
    "    stats = {\n",
    "        \"episode_reward\": [],\n",
    "        \"episode_steps\": [],\n",
    "        \"num_trades\": [],\n",
    "        \"win_rate\": [],\n",
    "        \"avg_trade_return\": [],\n",
    "        \"avg_hold_steps\": [],\n",
    "        \"sum_trade_return\": [],\n",
    "    }\n",
    "\n",
    "    for _ in range(episodes):\n",
    "        obs, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0.0\n",
    "        steps = 0\n",
    "\n",
    "        # Manual trade tracking (OPEN(t+1) execution)\n",
    "        in_pos = False\n",
    "        entry_price = None\n",
    "        hold_steps = 0\n",
    "        trade_returns = []\n",
    "        trade_hold_steps = []\n",
    "\n",
    "        while not done:\n",
    "            action, _lp, _v = policy_act(model, obs, device=device, greedy=greedy)\n",
    "\n",
    "            # Safer enum conversion\n",
    "            try:\n",
    "                act_enum = Actions(action)\n",
    "            except Exception:\n",
    "                # If action is invalid, treat as \"Skip\" (safe fallback)\n",
    "                act_enum = Actions.Skip\n",
    "\n",
    "            # Access state after reset/step (base_env._state can change reference)\n",
    "            st = base_env._state\n",
    "            next_idx = st._offset + 1\n",
    "\n",
    "            exec_open = None\n",
    "            if 0 <= next_idx < st._prices.open.shape[0]:\n",
    "                exec_open = float(st._prices.open[next_idx])\n",
    "\n",
    "            prev_have_pos_env = bool(st.have_position)\n",
    "\n",
    "            # Bookkeeping BEFORE env.step(), using OPEN(t+1)\n",
    "            if exec_open is not None:\n",
    "                if act_enum == Actions.Buy and not in_pos:\n",
    "                    in_pos = True\n",
    "                    entry_price = exec_open\n",
    "                    hold_steps = 0\n",
    "                elif act_enum == Actions.Close and in_pos:\n",
    "                    if entry_price is not None and entry_price > 0:\n",
    "                        tr = 100.0 * (exec_open - entry_price) / entry_price\n",
    "                    else:\n",
    "                        tr = 0.0\n",
    "                    trade_returns.append(tr)\n",
    "                    trade_hold_steps.append(hold_steps)\n",
    "                    in_pos = False\n",
    "                    entry_price = None\n",
    "                    hold_steps = 0\n",
    "\n",
    "            # Step environment\n",
    "            obs, reward, terminated, truncated, info = env.step(action)\n",
    "            done = bool(terminated or truncated)\n",
    "            total_reward += float(reward)\n",
    "            steps += 1\n",
    "\n",
    "            # Detect forced close by env (stop-loss / done close, etc.)\n",
    "            now_have_pos_env = bool(base_env._state.have_position)\n",
    "            if in_pos and prev_have_pos_env and (not now_have_pos_env) and act_enum != Actions.Close:\n",
    "                # Use exec_open if available (OPEN(t+1)), else fallback to current close\n",
    "                exit_price = exec_open if exec_open is not None else float(base_env._state._cur_close())\n",
    "                if entry_price is not None and entry_price > 0:\n",
    "                    tr = 100.0 * (exit_price - entry_price) / entry_price\n",
    "                else:\n",
    "                    tr = 0.0\n",
    "                trade_returns.append(tr)\n",
    "                trade_hold_steps.append(hold_steps)\n",
    "                in_pos = False\n",
    "                entry_price = None\n",
    "                hold_steps = 0\n",
    "\n",
    "            if in_pos:\n",
    "                hold_steps += 1\n",
    "\n",
    "        # If episode ends while holding, close at last close for reporting\n",
    "        if in_pos and entry_price is not None and entry_price > 0:\n",
    "            last_close = float(base_env._state._cur_close())\n",
    "            tr = 100.0 * (last_close - entry_price) / entry_price\n",
    "            trade_returns.append(tr)\n",
    "            trade_hold_steps.append(hold_steps)\n",
    "\n",
    "        # Episode metrics\n",
    "        stats[\"episode_reward\"].append(total_reward)\n",
    "        stats[\"episode_steps\"].append(steps)\n",
    "\n",
    "        n_trades = len(trade_returns)\n",
    "        stats[\"num_trades\"].append(float(n_trades))\n",
    "\n",
    "        if n_trades > 0:\n",
    "            wins = sum(1 for x in trade_returns if x > 0.0)\n",
    "            stats[\"win_rate\"].append(float(wins / n_trades))\n",
    "            stats[\"avg_trade_return\"].append(float(np.mean(trade_returns)))\n",
    "            stats[\"avg_hold_steps\"].append(float(np.mean(trade_hold_steps)))\n",
    "            stats[\"sum_trade_return\"].append(float(np.sum(trade_returns)))\n",
    "        else:\n",
    "            stats[\"win_rate\"].append(0.0)\n",
    "            stats[\"avg_trade_return\"].append(0.0)\n",
    "            stats[\"avg_hold_steps\"].append(0.0)\n",
    "            stats[\"sum_trade_return\"].append(0.0)\n",
    "\n",
    "    # Restore model mode\n",
    "    if was_training:\n",
    "        model.train()\n",
    "\n",
    "    return {k: float(np.mean(v)) for k, v in stats.items()}\n",
    "\n",
    "\n",
    "print(\"✓ Helper functions defined (updated/robust)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Helper functions defined (updated/robust)\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cell 5"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:10:34.990216Z",
     "start_time": "2025-12-28T19:10:34.979922Z"
    }
   },
   "source": [
    "# ===== Training Configuration (matched to CLI + holding cost experiment) =====\n",
    "config = {\n",
    "    # Data\n",
    "    \"data_dir\": \"yf_data\",\n",
    "    \"run_name\": \"ppo_aapl_final_holdpen_2e-4\",  # NEW: identify holding-cost run\n",
    "    \"seed\": 0,                                  # same as before\n",
    "\n",
    "    # Device\n",
    "    \"use_cuda\": True,                           # CLI: --cuda (CPU fallback is fine)\n",
    "\n",
    "    # PPO Hyperparameters (UNCHANGED)\n",
    "    \"gamma\": 0.99,\n",
    "    \"gae_lambda\": 0.95,\n",
    "    \"clip_eps\": 0.2,\n",
    "    \"lr\": 3e-4,\n",
    "    \"rollout_steps\": 1024,\n",
    "    \"minibatch\": 256,\n",
    "    \"epochs\": 5,\n",
    "    \"value_coef\": 0.5,\n",
    "    \"entropy_coef\": 0.01,\n",
    "    \"max_grad_norm\": 0.5,\n",
    "    \"target_kl\": 0.02,\n",
    "\n",
    "    # Environment\n",
    "    \"bars\": 10,\n",
    "    \"volumes\": True,\n",
    "    \"extra_features\": True,\n",
    "    \"reward_mode\": \"close_pnl\",\n",
    "    \"state_1d\": False,\n",
    "    \"time_limit\": 1000,\n",
    "\n",
    "    # >>> NEW: Holding-cost controls <<<\n",
    "    \"hold_penalty_per_step\": 2e-4,   # 0.02 reward units per step (after *100 scaling)\n",
    "    \"max_hold_steps\": None,          # set to e.g. 50–100 ONLY if needed later\n",
    "\n",
    "    # Data Split\n",
    "    \"split\": True,\n",
    "    \"train_ratio\": 0.8,\n",
    "    \"min_train\": 300,\n",
    "    \"min_val\": 300,\n",
    "\n",
    "    # Training Control\n",
    "    \"max_rollouts\": 500,\n",
    "    \"total_steps\": 10_000_000,\n",
    "\n",
    "    # Validation & Checkpointing\n",
    "    \"val_every_rollouts\": 10,\n",
    "    \"save_every_rollouts\": 10,\n",
    "    \"early_stop\": True,\n",
    "    \"patience\": 20,\n",
    "    \"min_rollouts\": 50,\n",
    "    \"min_delta\": 0.01,\n",
    "}\n",
    "\n",
    "# Set device (matches --cuda behavior)\n",
    "device = torch.device(\"cuda\" if (config[\"use_cuda\"] and torch.cuda.is_available()) else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Set seed\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "print(\"✓ Configuration set (with holding cost)\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "✓ Configuration set (with holding cost)\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load and Prepare Data\n",
    "\n",
    "Load stock price data and split into train/validation sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:10:35.036395Z",
     "start_time": "2025-12-28T19:10:35.009510Z"
    }
   },
   "source": [
    "# Load all price data\n",
    "prices_all = load_many_from_dir(config[\"data_dir\"])\n",
    "print(f\"Loaded {len(prices_all)} instruments: {list(prices_all.keys())}\")\n",
    "\n",
    "# Optional: if you want to exactly match an AAPL-only run\n",
    "# TARGET = \"AAPL\"\n",
    "# if TARGET in prices_all:\n",
    "#     prices_all = {TARGET: prices_all[TARGET]}\n",
    "#     print(f\"Filtered to single instrument: {TARGET}\")\n",
    "\n",
    "# Split into train/validation (chronological, no leakage)\n",
    "if config[\"split\"]:\n",
    "    prices_train, prices_val = split_many_by_ratio(\n",
    "        prices_all,\n",
    "        train_ratio=config[\"train_ratio\"],\n",
    "        min_train=config[\"min_train\"],\n",
    "        min_val=config[\"min_val\"],\n",
    "    )\n",
    "    print(f\"Train instruments: {len(prices_train)}\")\n",
    "    print(f\"Validation instruments: {len(prices_val)}\")\n",
    "else:\n",
    "    prices_train = prices_all\n",
    "    prices_val = prices_all\n",
    "    print(\"No split: using same data for train and validation (in-sample)\")\n",
    "\n",
    "# Sanity checks (strongly recommended)\n",
    "for k in prices_train.keys():\n",
    "    tr_len = len(prices_train[k].close)\n",
    "    va_len = len(prices_val[k].close)\n",
    "    if config[\"split\"]:\n",
    "        assert tr_len >= config[\"min_train\"], f\"{k}: train too short ({tr_len})\"\n",
    "        assert va_len >= config[\"min_val\"], f\"{k}: val too short ({va_len})\"\n",
    "    print(f\"  {k}: train_len={tr_len}, val_len={va_len}\")\n",
    "\n",
    "print(\"✓ Data loaded and split\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 instruments: ['AAPL_1d_2020-01-01_to_2025-12-23', 'TSLA_1d_2020-01-01_to_2025-11-30', 'TSLA_1d_2020-01-01_to_2025-12-31']\n",
      "Train instruments: 2\n",
      "Validation instruments: 2\n",
      "  AAPL_1d_2020-01-01_to_2025-12-23: train_len=1201, val_len=301\n",
      "  TSLA_1d_2020-01-01_to_2025-11-30: train_len=1186, val_len=300\n",
      "✓ Data loaded and split\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Create Environments\n",
    "\n",
    "Set up training and validation environments.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:10:35.053472Z",
     "start_time": "2025-12-28T19:10:35.045705Z"
    }
   },
   "source": [
    "# Create training environment (match CLI behavior)\n",
    "env_train_base = StocksEnv(\n",
    "    prices_train,\n",
    "    bars_count=config[\"bars\"],\n",
    "    volumes=config[\"volumes\"],\n",
    "    extra_features=config[\"extra_features\"],\n",
    "    reward_mode=config[\"reward_mode\"],\n",
    "    state_1d=config[\"state_1d\"],\n",
    "    hold_penalty_per_step=float(config[\"hold_penalty_per_step\"]),\n",
    "    max_hold_steps=config.get(\"max_hold_steps\", None),\n",
    ")\n",
    "env_train = gym.wrappers.TimeLimit(env_train_base, max_episode_steps=config[\"time_limit\"])\n",
    "\n",
    "# Create validation environment (use same settings + same TimeLimit)\n",
    "env_val_base = StocksEnv(\n",
    "    prices_val,\n",
    "    bars_count=config[\"bars\"],\n",
    "    volumes=config[\"volumes\"],\n",
    "    extra_features=config[\"extra_features\"],\n",
    "    reward_mode=config[\"reward_mode\"],\n",
    "    state_1d=config[\"state_1d\"],\n",
    "    hold_penalty_per_step=float(config[\"hold_penalty_per_step\"]),\n",
    "    max_hold_steps=config.get(\"max_hold_steps\", None),\n",
    ")\n",
    "env_val = gym.wrappers.TimeLimit(env_val_base, max_episode_steps=config[\"time_limit\"])\n",
    "\n",
    "obs_shape = env_train.observation_space.shape\n",
    "n_actions = env_train.action_space.n\n",
    "\n",
    "print(f\"Observation space: {obs_shape}\")\n",
    "print(f\"Action space: {n_actions} actions\")\n",
    "print(\"✓ Environments created\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observation space: (45,)\n",
      "Action space: 3 actions\n",
      "✓ Environments created\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Initialize Model and Optimizer\n",
    "\n",
    "Create the PPO model and optimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:10:35.085765Z",
     "start_time": "2025-12-28T19:10:35.067158Z"
    }
   },
   "source": [
    "# Cell 8: Build model + optimizer + TensorBoard writer (compatible with updated Cell 2)\n",
    "\n",
    "# Infer observation shape from env\n",
    "# - Vector state: obs_shape = (obs_dim,)\n",
    "# - State1D:      obs_shape = (C, T)\n",
    "if config[\"state_1d\"]:\n",
    "    C, T = obs_shape  # (channels, time bars)\n",
    "\n",
    "    # Our updated ActorCriticConv1D computes in_channels internally from `volumes`\n",
    "    # (C should equal (3 + (1 if volumes else 0)) + 2, usually 6 if volumes=True)\n",
    "    expected_C = (3 + (1 if config[\"volumes\"] else 0)) + 2\n",
    "    if C != expected_C:\n",
    "        raise ValueError(\n",
    "            f\"State1D channel mismatch: env returned C={C}, expected C={expected_C}. \"\n",
    "            \"Check env volumes/state_1d settings.\"\n",
    "        )\n",
    "\n",
    "    model = ActorCriticConv1D(\n",
    "        n_actions=n_actions,\n",
    "        bars_count=T,\n",
    "        volumes=config[\"volumes\"],\n",
    "        hidden=256,\n",
    "    ).to(device)\n",
    "\n",
    "    print(f\"Created Conv1D model: C={C}, T={T}\")\n",
    "else:\n",
    "    obs_dim = obs_shape[0]\n",
    "    model = ActorCriticMLP(obs_dim=obs_dim, n_actions=n_actions, hidden=256).to(device)\n",
    "    print(f\"Created MLP model: obs_dim={obs_dim}\")\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=config[\"lr\"])\n",
    "\n",
    "# TensorBoard writer (clean run directory like scripts usually do)\n",
    "run_dir = os.path.join(\"runs\", config[\"run_name\"])\n",
    "writer = SummaryWriter(log_dir=run_dir)\n",
    "print(f\"TensorBoard log dir: {run_dir}\")\n",
    "print(\"Model params:\", sum(p.numel() for p in model.parameters()))\n",
    "\n",
    "print(\"✓ Model and optimizer initialized\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created MLP model: obs_dim=45\n",
      "TensorBoard log dir: runs\\ppo_aapl_final_holdpen_2e-4\n",
      "Model params: 78596\n",
      "✓ Model and optimizer initialized\n"
     ]
    }
   ],
   "execution_count": 46
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Training Loop\n",
    "\n",
    "Main PPO training loop: collect rollouts, compute advantages, update policy.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:16:59.597192Z",
     "start_time": "2025-12-28T19:10:35.098394Z"
    }
   },
   "source": [
    "# ===== Training bookkeeping =====\n",
    "obs, info = env_train.reset(seed=config[\"seed\"])\n",
    "episode_reward = 0.0\n",
    "episode_steps = 0\n",
    "episode_count = 0\n",
    "\n",
    "global_step = 0\n",
    "rollout_idx = 0\n",
    "t0 = time.time()\n",
    "\n",
    "best_val_reward = -1e9\n",
    "no_improve = 0\n",
    "\n",
    "print(f\"[PPO] device={device} obs_shape={obs_shape} actions={n_actions}\")\n",
    "print(f\"[PPO] reward_mode={config['reward_mode']} volumes={config['volumes']} extra_features={config['extra_features']}\")\n",
    "print(f\"[PPO] split={config['split']} max_rollouts={config['max_rollouts']} early_stop={config['early_stop']}\")\n",
    "print(f\"[PPO] logs: runs/  checkpoints: saves/\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Main training loop\n",
    "while (global_step < config[\"total_steps\"]) and (rollout_idx < config[\"max_rollouts\"]):\n",
    "    rollout_idx += 1\n",
    "    buf = RolloutBuffer(obs_shape=obs_shape, size=config[\"rollout_steps\"], device=device)\n",
    "\n",
    "    # ===== Collect Rollout =====\n",
    "    for _ in range(config[\"rollout_steps\"]):\n",
    "        global_step += 1\n",
    "\n",
    "        action, logprob, value = policy_act(model, obs, device=device, greedy=False)\n",
    "        next_obs, reward, terminated, truncated, info = env_train.step(action)\n",
    "\n",
    "        episode_done = bool(terminated or truncated)\n",
    "\n",
    "        # IMPORTANT FIX: mask GAE on ANY episode end (terminated OR truncated)\n",
    "        buf_done = episode_done\n",
    "\n",
    "        buf.add(\n",
    "            obs=obs,\n",
    "            action=action,\n",
    "            reward=float(reward),\n",
    "            done=buf_done,\n",
    "            value=value,\n",
    "            logprob=logprob,\n",
    "        )\n",
    "\n",
    "        episode_reward += float(reward)\n",
    "        episode_steps += 1\n",
    "        obs = next_obs\n",
    "\n",
    "        if episode_done:\n",
    "            episode_count += 1\n",
    "            writer.add_scalar(\"train/episode_reward\", episode_reward, global_step)\n",
    "            writer.add_scalar(\"train/episode_steps\", episode_steps, global_step)\n",
    "\n",
    "            obs, info = env_train.reset()\n",
    "            episode_reward = 0.0\n",
    "            episode_steps = 0\n",
    "\n",
    "        if global_step >= config[\"total_steps\"]:\n",
    "            break\n",
    "\n",
    "    # If we didn't fill the buffer (e.g., hit total_steps), stop cleanly\n",
    "    if not buf.full:\n",
    "        print(f\"[PPO] stopping: reached total_steps mid-rollout (filled {buf.ptr}/{buf.size}).\")\n",
    "        break\n",
    "\n",
    "    # Rollout reward heartbeat\n",
    "    roll_sum = float(buf.rewards.sum())\n",
    "    roll_mean = float(buf.rewards.mean())\n",
    "    writer.add_scalar(\"train/rollout_reward_sum\", roll_sum, global_step)\n",
    "    writer.add_scalar(\"train/rollout_reward_mean\", roll_mean, global_step)\n",
    "\n",
    "    # Bootstrap last value for GAE (only used if last transition not terminal)\n",
    "    with torch.no_grad():\n",
    "        obs_t = torch.as_tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
    "        _, last_v = model(obs_t)\n",
    "        last_value = float(last_v.item())\n",
    "\n",
    "    buf.compute_gae(\n",
    "        last_value=last_value,\n",
    "        gamma=config[\"gamma\"],\n",
    "        lam=config[\"gae_lambda\"],\n",
    "        normalize_adv=True,\n",
    "    )\n",
    "\n",
    "    # ===== PPO Update =====\n",
    "    policy_losses = []\n",
    "    value_losses = []\n",
    "    entropies = []\n",
    "    approx_kls = []\n",
    "    clipfracs = []\n",
    "\n",
    "    for _epoch in range(config[\"epochs\"]):\n",
    "        for batch in buf.get_batches(batch_size=config[\"minibatch\"], shuffle=True):\n",
    "            logits, values = model(batch.obs)\n",
    "            dist = Categorical(logits=logits)\n",
    "\n",
    "            new_logp = dist.log_prob(batch.actions)\n",
    "            entropy = dist.entropy().mean()\n",
    "\n",
    "            # PPO ratio\n",
    "            ratio = torch.exp(new_logp - batch.old_logprobs)\n",
    "\n",
    "            # Policy loss (clipped surrogate)\n",
    "            unclipped = ratio * batch.advantages\n",
    "            clipped = torch.clamp(ratio, 1.0 - config[\"clip_eps\"], 1.0 + config[\"clip_eps\"]) * batch.advantages\n",
    "            loss_pi = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "            # Value loss\n",
    "            loss_v = (batch.returns - values).pow(2).mean()\n",
    "\n",
    "            # Total loss\n",
    "            loss = loss_pi + config[\"value_coef\"] * loss_v - config[\"entropy_coef\"] * entropy\n",
    "\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), config[\"max_grad_norm\"])\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Better PPO-style KL approximation (no abs)\n",
    "                approx_kl = float((batch.old_logprobs - new_logp).mean().item())\n",
    "                clipfrac = float((torch.abs(ratio - 1.0) > config[\"clip_eps\"]).float().mean().item())\n",
    "\n",
    "            policy_losses.append(float(loss_pi.item()))\n",
    "            value_losses.append(float(loss_v.item()))\n",
    "            entropies.append(float(entropy.item()))\n",
    "            approx_kls.append(approx_kl)\n",
    "            clipfracs.append(clipfrac)\n",
    "\n",
    "        # Early stop PPO epoch loop if KL too big\n",
    "        if config[\"target_kl\"] > 0 and approx_kls and (np.mean(approx_kls) > config[\"target_kl\"]):\n",
    "            break\n",
    "\n",
    "    # ===== Logging =====\n",
    "    fps = global_step / max(1e-9, (time.time() - t0))\n",
    "    writer.add_scalar(\"ppo/policy_loss\", float(np.mean(policy_losses) if policy_losses else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/value_loss\", float(np.mean(value_losses) if value_losses else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/entropy\", float(np.mean(entropies) if entropies else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/approx_kl\", float(np.mean(approx_kls) if approx_kls else 0.0), global_step)\n",
    "    writer.add_scalar(\"ppo/clipfrac\", float(np.mean(clipfracs) if clipfracs else 0.0), global_step)\n",
    "    writer.add_scalar(\"train/fps\", float(fps), global_step)\n",
    "    writer.add_scalar(\"train/episodes\", float(episode_count), global_step)\n",
    "\n",
    "    # Value function explained variance\n",
    "    ev = explained_variance(np.asarray(buf.values), np.asarray(buf.returns))\n",
    "    writer.add_scalar(\"ppo/explained_variance\", ev, global_step)\n",
    "\n",
    "    # Console heartbeat\n",
    "    print(\n",
    "        f\"[rollout {rollout_idx}] step={global_step} \"\n",
    "        f\"roll_sum={roll_sum:.3f} roll_mean={roll_mean:.5f} \"\n",
    "        f\"pi_loss={np.mean(policy_losses) if policy_losses else 0.0:.4f} \"\n",
    "        f\"v_loss={np.mean(value_losses) if value_losses else 0.0:.4f} \"\n",
    "        f\"ent={np.mean(entropies) if entropies else 0.0:.3f} \"\n",
    "        f\"kl={np.mean(approx_kls) if approx_kls else 0.0:.4f} \"\n",
    "        f\"clip={np.mean(clipfracs) if clipfracs else 0.0:.3f} \"\n",
    "        f\"eps_done={episode_count} fps={fps:.1f}\"\n",
    "    )\n",
    "\n",
    "    # ===== Validation + Best Model Saving + Early Stop =====\n",
    "    if config[\"val_every_rollouts\"] > 0 and (rollout_idx % config[\"val_every_rollouts\"] == 0):\n",
    "        val = validation_run_ppo(env_val, model, episodes=20, device=device, greedy=True)\n",
    "\n",
    "        for k, v in val.items():\n",
    "            writer.add_scalar(\"val/\" + k, v, global_step)\n",
    "\n",
    "        print(f\"  [val] {val}\")\n",
    "\n",
    "        cur_val = float(val.get(\"episode_reward\", -1e9))\n",
    "        if cur_val > best_val_reward + config[\"min_delta\"]:\n",
    "            best_val_reward = cur_val\n",
    "            no_improve = 0\n",
    "            best_path = os.path.join(\"saves\", f\"ppo_{config['run_name']}_best.pt\")\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            print(f\"  [best] new best val episode_reward={best_val_reward:.4f} -> {best_path}\")\n",
    "        else:\n",
    "            no_improve += 1\n",
    "\n",
    "        if config[\"early_stop\"] and (rollout_idx >= config[\"min_rollouts\"]) and (no_improve >= config[\"patience\"]):\n",
    "            print(f\"[PPO] early stopping: no val improvement for {no_improve} validations.\")\n",
    "            break\n",
    "\n",
    "    # ===== Save Periodic Checkpoint =====\n",
    "    if config[\"save_every_rollouts\"] > 0 and (rollout_idx % config[\"save_every_rollouts\"] == 0):\n",
    "        ckpt_path = os.path.join(\"saves\", f\"ppo_{config['run_name']}_rollout{rollout_idx}.pt\")\n",
    "        torch.save(model.state_dict(), ckpt_path)\n",
    "        print(f\"  [save] {ckpt_path}\")\n",
    "\n",
    "    # Hard stop condition\n",
    "    if config[\"max_rollouts\"] and rollout_idx >= config[\"max_rollouts\"]:\n",
    "        print(\"[PPO] reached max_rollouts, stopping.\")\n",
    "        break\n",
    "\n",
    "if global_step >= config[\"total_steps\"]:\n",
    "    print(f\"[done] reached total_steps={config['total_steps']} at rollout={rollout_idx}\")\n",
    "elif rollout_idx >= config[\"max_rollouts\"]:\n",
    "    print(f\"[done] reached max_rollouts={config['max_rollouts']} at step={global_step}\")\n",
    "\n",
    "writer.close()\n",
    "print(\"\\n✓ Training completed!\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PPO] device=cpu obs_shape=(45,) actions=3\n",
      "[PPO] reward_mode=close_pnl volumes=True extra_features=True\n",
      "[PPO] split=True max_rollouts=500 early_stop=True\n",
      "[PPO] logs: runs/  checkpoints: saves/\n",
      "============================================================\n",
      "[rollout 1] step=1024 roll_sum=48.222 roll_mean=0.04709 pi_loss=-0.0002 v_loss=59.3379 ent=1.099 kl=0.0000 clip=0.000 eps_done=1 fps=934.6\n",
      "[rollout 2] step=2048 roll_sum=-2.165 roll_mean=-0.00211 pi_loss=-0.0004 v_loss=29.6044 ent=1.099 kl=-0.0001 clip=0.000 eps_done=2 fps=899.3\n",
      "[rollout 3] step=3072 roll_sum=34.857 roll_mean=0.03404 pi_loss=-0.0003 v_loss=13.3122 ent=1.098 kl=-0.0001 clip=0.000 eps_done=3 fps=882.7\n",
      "[rollout 4] step=4096 roll_sum=-52.316 roll_mean=-0.05109 pi_loss=-0.0000 v_loss=35.8102 ent=1.098 kl=0.0003 clip=0.000 eps_done=6 fps=876.7\n",
      "[rollout 5] step=5120 roll_sum=68.676 roll_mean=0.06707 pi_loss=-0.0004 v_loss=55.8822 ent=1.098 kl=0.0001 clip=0.000 eps_done=8 fps=884.7\n",
      "[rollout 6] step=6144 roll_sum=97.514 roll_mean=0.09523 pi_loss=0.0001 v_loss=78.2339 ent=1.098 kl=0.0002 clip=0.000 eps_done=11 fps=881.1\n",
      "[rollout 7] step=7168 roll_sum=76.211 roll_mean=0.07442 pi_loss=-0.0010 v_loss=47.9907 ent=1.098 kl=0.0013 clip=0.000 eps_done=13 fps=878.6\n",
      "[rollout 8] step=8192 roll_sum=145.570 roll_mean=0.14216 pi_loss=0.0009 v_loss=62.4688 ent=1.095 kl=-0.0007 clip=0.000 eps_done=14 fps=880.4\n",
      "[rollout 9] step=9216 roll_sum=76.743 roll_mean=0.07494 pi_loss=-0.0006 v_loss=61.5456 ent=1.092 kl=0.0011 clip=0.000 eps_done=15 fps=875.3\n",
      "[rollout 10] step=10240 roll_sum=61.097 roll_mean=0.05967 pi_loss=-0.0050 v_loss=14.5155 ent=1.066 kl=0.0057 clip=0.074 eps_done=17 fps=879.5\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 188.05, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [best] new best val episode_reward=0.0000 -> saves\\ppo_ppo_aapl_final_holdpen_2e-4_best.pt\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout10.pt\n",
      "[rollout 11] step=11264 roll_sum=-19.004 roll_mean=-0.01856 pi_loss=0.0007 v_loss=14.8777 ent=1.070 kl=0.0037 clip=0.002 eps_done=19 fps=695.9\n",
      "[rollout 12] step=12288 roll_sum=-12.509 roll_mean=-0.01222 pi_loss=-0.0000 v_loss=11.7011 ent=1.078 kl=0.0004 clip=0.000 eps_done=20 fps=712.9\n",
      "[rollout 13] step=13312 roll_sum=76.112 roll_mean=0.07433 pi_loss=0.0001 v_loss=9.1794 ent=1.079 kl=-0.0000 clip=0.000 eps_done=21 fps=731.4\n",
      "[rollout 14] step=14336 roll_sum=23.426 roll_mean=0.02288 pi_loss=-0.0005 v_loss=9.3507 ent=1.077 kl=0.0015 clip=0.000 eps_done=22 fps=744.9\n",
      "[rollout 15] step=15360 roll_sum=-1.403 roll_mean=-0.00137 pi_loss=-0.0016 v_loss=29.2694 ent=1.056 kl=0.0012 clip=0.000 eps_done=25 fps=758.4\n",
      "[rollout 16] step=16384 roll_sum=-3.943 roll_mean=-0.00385 pi_loss=-0.0007 v_loss=51.2713 ent=1.013 kl=0.0021 clip=0.000 eps_done=28 fps=773.0\n",
      "[rollout 17] step=17408 roll_sum=82.948 roll_mean=0.08100 pi_loss=0.0005 v_loss=49.3732 ent=1.005 kl=-0.0004 clip=0.000 eps_done=30 fps=785.8\n",
      "[rollout 18] step=18432 roll_sum=166.011 roll_mean=0.16212 pi_loss=0.0015 v_loss=63.6888 ent=0.962 kl=0.0007 clip=0.001 eps_done=31 fps=798.3\n",
      "[rollout 19] step=19456 roll_sum=9.656 roll_mean=0.00943 pi_loss=-0.0004 v_loss=66.7449 ent=0.923 kl=0.0003 clip=0.000 eps_done=33 fps=810.5\n",
      "[rollout 20] step=20480 roll_sum=77.635 roll_mean=0.07581 pi_loss=0.0001 v_loss=91.8192 ent=0.960 kl=0.0006 clip=0.000 eps_done=35 fps=820.5\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 192.25, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout20.pt\n",
      "[rollout 21] step=21504 roll_sum=40.892 roll_mean=0.03993 pi_loss=-0.0016 v_loss=10.7567 ent=0.962 kl=0.0004 clip=0.002 eps_done=36 fps=747.8\n",
      "[rollout 22] step=22528 roll_sum=107.538 roll_mean=0.10502 pi_loss=0.0003 v_loss=38.2993 ent=0.866 kl=0.0025 clip=0.003 eps_done=39 fps=750.2\n",
      "[rollout 23] step=23552 roll_sum=-105.311 roll_mean=-0.10284 pi_loss=-0.0001 v_loss=63.2488 ent=0.815 kl=0.0019 clip=0.025 eps_done=40 fps=760.0\n",
      "[rollout 24] step=24576 roll_sum=59.841 roll_mean=0.05844 pi_loss=-0.0017 v_loss=19.7804 ent=0.829 kl=0.0010 clip=0.023 eps_done=42 fps=767.1\n",
      "[rollout 25] step=25600 roll_sum=79.678 roll_mean=0.07781 pi_loss=-0.0011 v_loss=75.7677 ent=0.793 kl=0.0005 clip=0.011 eps_done=43 fps=774.2\n",
      "[rollout 26] step=26624 roll_sum=-21.502 roll_mean=-0.02100 pi_loss=-0.0025 v_loss=47.9506 ent=0.752 kl=-0.0006 clip=0.013 eps_done=44 fps=782.5\n",
      "[rollout 27] step=27648 roll_sum=50.055 roll_mean=0.04888 pi_loss=-0.0015 v_loss=10.6201 ent=0.769 kl=0.0005 clip=0.007 eps_done=46 fps=789.9\n",
      "[rollout 28] step=28672 roll_sum=-45.171 roll_mean=-0.04411 pi_loss=-0.0022 v_loss=32.9710 ent=0.685 kl=0.0034 clip=0.015 eps_done=48 fps=798.2\n",
      "[rollout 29] step=29696 roll_sum=11.017 roll_mean=0.01076 pi_loss=-0.0016 v_loss=8.7800 ent=0.692 kl=0.0031 clip=0.009 eps_done=50 fps=805.7\n",
      "[rollout 30] step=30720 roll_sum=68.504 roll_mean=0.06690 pi_loss=-0.0015 v_loss=11.7131 ent=0.693 kl=0.0013 clip=0.003 eps_done=51 fps=813.1\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 188.75, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout30.pt\n",
      "[rollout 31] step=31744 roll_sum=105.355 roll_mean=0.10289 pi_loss=0.0005 v_loss=16.8032 ent=0.692 kl=0.0039 clip=0.010 eps_done=53 fps=765.1\n",
      "[rollout 32] step=32768 roll_sum=52.653 roll_mean=0.05142 pi_loss=-0.0003 v_loss=16.7290 ent=0.605 kl=0.0025 clip=0.014 eps_done=54 fps=772.0\n",
      "[rollout 33] step=33792 roll_sum=41.897 roll_mean=0.04092 pi_loss=-0.0020 v_loss=30.2193 ent=0.542 kl=0.0010 clip=0.033 eps_done=55 fps=778.7\n",
      "[rollout 34] step=34816 roll_sum=-23.016 roll_mean=-0.02248 pi_loss=-0.0011 v_loss=33.4292 ent=0.632 kl=0.0010 clip=0.001 eps_done=57 fps=784.8\n",
      "[rollout 35] step=35840 roll_sum=-7.833 roll_mean=-0.00765 pi_loss=-0.0013 v_loss=26.1275 ent=0.575 kl=0.0011 clip=0.018 eps_done=59 fps=790.5\n",
      "[rollout 36] step=36864 roll_sum=295.828 roll_mean=0.28889 pi_loss=0.0050 v_loss=150.3846 ent=0.767 kl=0.0009 clip=0.030 eps_done=60 fps=796.6\n",
      "[rollout 37] step=37888 roll_sum=-9.908 roll_mean=-0.00968 pi_loss=-0.0000 v_loss=49.1235 ent=0.681 kl=0.0010 clip=0.018 eps_done=61 fps=802.7\n",
      "[rollout 38] step=38912 roll_sum=172.108 roll_mean=0.16807 pi_loss=0.0027 v_loss=75.3564 ent=0.641 kl=0.0018 clip=0.021 eps_done=62 fps=807.6\n",
      "[rollout 39] step=39936 roll_sum=-64.855 roll_mean=-0.06334 pi_loss=-0.0016 v_loss=18.7464 ent=0.560 kl=0.0009 clip=0.023 eps_done=63 fps=813.0\n",
      "[rollout 40] step=40960 roll_sum=-0.938 roll_mean=-0.00092 pi_loss=-0.0004 v_loss=28.0925 ent=0.654 kl=0.0003 clip=0.042 eps_done=65 fps=818.6\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 198.9, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout40.pt\n",
      "[rollout 41] step=41984 roll_sum=85.886 roll_mean=0.08387 pi_loss=-0.0007 v_loss=32.4198 ent=0.697 kl=-0.0002 clip=0.000 eps_done=66 fps=779.7\n",
      "[rollout 42] step=43008 roll_sum=2.830 roll_mean=0.00276 pi_loss=-0.0030 v_loss=32.3616 ent=0.753 kl=0.0035 clip=0.016 eps_done=68 fps=784.3\n",
      "[rollout 43] step=44032 roll_sum=211.022 roll_mean=0.20608 pi_loss=0.0031 v_loss=59.0904 ent=0.680 kl=0.0014 clip=0.024 eps_done=69 fps=789.3\n",
      "[rollout 44] step=45056 roll_sum=-47.896 roll_mean=-0.04677 pi_loss=-0.0022 v_loss=31.1462 ent=0.649 kl=0.0020 clip=0.024 eps_done=72 fps=793.7\n",
      "[rollout 45] step=46080 roll_sum=14.805 roll_mean=0.01446 pi_loss=-0.0011 v_loss=12.6721 ent=0.786 kl=0.0004 clip=0.018 eps_done=74 fps=799.1\n",
      "[rollout 46] step=47104 roll_sum=21.017 roll_mean=0.02052 pi_loss=-0.0036 v_loss=9.2151 ent=0.854 kl=-0.0000 clip=0.021 eps_done=75 fps=803.5\n",
      "[rollout 47] step=48128 roll_sum=-0.240 roll_mean=-0.00023 pi_loss=-0.0010 v_loss=16.7198 ent=0.762 kl=0.0011 clip=0.000 eps_done=79 fps=807.0\n",
      "[rollout 48] step=49152 roll_sum=82.483 roll_mean=0.08055 pi_loss=-0.0003 v_loss=23.2612 ent=0.857 kl=0.0007 clip=0.000 eps_done=81 fps=805.3\n",
      "[rollout 49] step=50176 roll_sum=4.285 roll_mean=0.00418 pi_loss=0.0003 v_loss=9.0918 ent=0.856 kl=0.0009 clip=0.007 eps_done=82 fps=778.9\n",
      "[rollout 50] step=51200 roll_sum=58.885 roll_mean=0.05751 pi_loss=-0.0010 v_loss=8.1788 ent=0.868 kl=-0.0009 clip=0.009 eps_done=84 fps=763.3\n",
      "  [val] {'episode_reward': 3.194984117754183, 'episode_steps': 183.25, 'num_trades': 0.55, 'win_rate': 0.55, 'avg_trade_return': 3.6484193743214943, 'avg_hold_steps': 33.55, 'sum_trade_return': 3.6484193743214943}\n",
      "  [best] new best val episode_reward=3.1950 -> saves\\ppo_ppo_aapl_final_holdpen_2e-4_best.pt\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout50.pt\n",
      "[rollout 51] step=52224 roll_sum=78.189 roll_mean=0.07636 pi_loss=-0.0035 v_loss=17.8485 ent=0.867 kl=0.0030 clip=0.026 eps_done=87 fps=737.5\n",
      "[rollout 52] step=53248 roll_sum=103.432 roll_mean=0.10101 pi_loss=-0.0008 v_loss=21.4948 ent=0.920 kl=0.0011 clip=0.000 eps_done=88 fps=742.0\n",
      "[rollout 53] step=54272 roll_sum=193.942 roll_mean=0.18940 pi_loss=0.0032 v_loss=53.2915 ent=0.890 kl=0.0026 clip=0.028 eps_done=92 fps=747.0\n",
      "[rollout 54] step=55296 roll_sum=6.321 roll_mean=0.00617 pi_loss=-0.0022 v_loss=32.6484 ent=0.865 kl=0.0001 clip=0.032 eps_done=94 fps=749.6\n",
      "[rollout 55] step=56320 roll_sum=27.540 roll_mean=0.02689 pi_loss=-0.0004 v_loss=12.7206 ent=0.846 kl=0.0012 clip=0.009 eps_done=96 fps=753.0\n",
      "[rollout 56] step=57344 roll_sum=25.497 roll_mean=0.02490 pi_loss=-0.0010 v_loss=18.3510 ent=0.851 kl=0.0006 clip=0.001 eps_done=100 fps=757.3\n",
      "[rollout 57] step=58368 roll_sum=48.108 roll_mean=0.04698 pi_loss=-0.0010 v_loss=14.1697 ent=0.858 kl=0.0000 clip=0.001 eps_done=101 fps=761.5\n",
      "[rollout 58] step=59392 roll_sum=77.061 roll_mean=0.07525 pi_loss=0.0002 v_loss=22.3656 ent=0.854 kl=-0.0010 clip=0.000 eps_done=104 fps=765.7\n",
      "[rollout 59] step=60416 roll_sum=-20.263 roll_mean=-0.01979 pi_loss=-0.0017 v_loss=14.4107 ent=0.757 kl=0.0110 clip=0.075 eps_done=105 fps=770.3\n",
      "[rollout 60] step=61440 roll_sum=-40.604 roll_mean=-0.03965 pi_loss=-0.0069 v_loss=13.5133 ent=0.727 kl=0.0093 clip=0.099 eps_done=107 fps=773.8\n",
      "  [val] {'episode_reward': 25.07755111112032, 'episode_steps': 202.4, 'num_trades': 1.0, 'win_rate': 1.0, 'avg_trade_return': 26.63061169449129, 'avg_hold_steps': 191.25, 'sum_trade_return': 26.63061169449129}\n",
      "  [best] new best val episode_reward=25.0776 -> saves\\ppo_ppo_aapl_final_holdpen_2e-4_best.pt\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout60.pt\n",
      "[rollout 61] step=62464 roll_sum=329.598 roll_mean=0.32187 pi_loss=0.0054 v_loss=96.0844 ent=0.759 kl=0.0069 clip=0.016 eps_done=108 fps=746.2\n",
      "[rollout 62] step=63488 roll_sum=27.507 roll_mean=0.02686 pi_loss=-0.0000 v_loss=15.1488 ent=0.841 kl=0.0004 clip=0.001 eps_done=110 fps=750.4\n",
      "[rollout 63] step=64512 roll_sum=241.795 roll_mean=0.23613 pi_loss=0.0036 v_loss=70.4169 ent=0.792 kl=0.0031 clip=0.025 eps_done=111 fps=748.3\n",
      "[rollout 64] step=65536 roll_sum=56.683 roll_mean=0.05535 pi_loss=-0.0011 v_loss=42.8869 ent=0.981 kl=0.0007 clip=0.025 eps_done=113 fps=750.9\n",
      "[rollout 65] step=66560 roll_sum=22.865 roll_mean=0.02233 pi_loss=0.0027 v_loss=11.5832 ent=0.929 kl=0.0072 clip=0.061 eps_done=115 fps=753.3\n",
      "[rollout 66] step=67584 roll_sum=94.190 roll_mean=0.09198 pi_loss=-0.0027 v_loss=17.9827 ent=0.903 kl=0.0009 clip=0.016 eps_done=117 fps=756.4\n",
      "[rollout 67] step=68608 roll_sum=100.936 roll_mean=0.09857 pi_loss=0.0013 v_loss=53.5704 ent=0.946 kl=0.0008 clip=0.001 eps_done=119 fps=757.9\n",
      "[rollout 68] step=69632 roll_sum=59.833 roll_mean=0.05843 pi_loss=-0.0023 v_loss=31.1409 ent=0.810 kl=-0.0010 clip=0.001 eps_done=120 fps=759.8\n",
      "[rollout 69] step=70656 roll_sum=-42.798 roll_mean=-0.04179 pi_loss=0.0008 v_loss=14.7230 ent=0.703 kl=0.0018 clip=0.012 eps_done=121 fps=761.6\n",
      "[rollout 70] step=71680 roll_sum=63.008 roll_mean=0.06153 pi_loss=-0.0011 v_loss=19.7321 ent=0.876 kl=0.0024 clip=0.002 eps_done=124 fps=763.6\n",
      "  [val] {'episode_reward': 23.7156062463444, 'episode_steps': 189.25, 'num_trades': 1.0, 'win_rate': 0.95, 'avg_trade_return': 25.30256721212439, 'avg_hold_steps': 187.0, 'sum_trade_return': 25.30256721212439}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout70.pt\n",
      "[rollout 71] step=72704 roll_sum=-11.929 roll_mean=-0.01165 pi_loss=0.0002 v_loss=21.6823 ent=0.678 kl=0.0010 clip=0.023 eps_done=125 fps=740.7\n",
      "[rollout 72] step=73728 roll_sum=47.899 roll_mean=0.04678 pi_loss=-0.0013 v_loss=10.0172 ent=0.832 kl=0.0060 clip=0.030 eps_done=127 fps=742.9\n",
      "[rollout 73] step=74752 roll_sum=65.629 roll_mean=0.06409 pi_loss=-0.0019 v_loss=10.5137 ent=0.778 kl=0.0043 clip=0.021 eps_done=128 fps=746.2\n",
      "[rollout 74] step=75776 roll_sum=82.477 roll_mean=0.08054 pi_loss=0.0015 v_loss=17.6103 ent=0.877 kl=0.0021 clip=0.004 eps_done=129 fps=748.7\n",
      "[rollout 75] step=76800 roll_sum=-50.123 roll_mean=-0.04895 pi_loss=0.0003 v_loss=15.5940 ent=0.672 kl=0.0011 clip=0.008 eps_done=130 fps=751.6\n",
      "[rollout 76] step=77824 roll_sum=20.405 roll_mean=0.01993 pi_loss=0.0013 v_loss=52.9782 ent=0.757 kl=0.0024 clip=0.009 eps_done=132 fps=754.5\n",
      "[rollout 77] step=78848 roll_sum=69.875 roll_mean=0.06824 pi_loss=-0.0006 v_loss=14.1700 ent=0.844 kl=0.0014 clip=0.025 eps_done=134 fps=755.2\n",
      "[rollout 78] step=79872 roll_sum=41.871 roll_mean=0.04089 pi_loss=-0.0022 v_loss=8.3598 ent=0.751 kl=0.0003 clip=0.021 eps_done=135 fps=748.0\n",
      "[rollout 79] step=80896 roll_sum=5.495 roll_mean=0.00537 pi_loss=0.0019 v_loss=10.7243 ent=0.746 kl=0.0002 clip=0.004 eps_done=138 fps=749.6\n",
      "[rollout 80] step=81920 roll_sum=1.715 roll_mean=0.00167 pi_loss=0.0014 v_loss=17.9112 ent=0.741 kl=0.0006 clip=0.001 eps_done=140 fps=752.2\n",
      "  [val] {'episode_reward': 30.18732753703423, 'episode_steps': 199.65, 'num_trades': 1.95, 'win_rate': 0.7708333333333333, 'avg_trade_return': 15.24973962034772, 'avg_hold_steps': 129.74166666666665, 'sum_trade_return': 32.17352042905118}\n",
      "  [best] new best val episode_reward=30.1873 -> saves\\ppo_ppo_aapl_final_holdpen_2e-4_best.pt\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout80.pt\n",
      "[rollout 81] step=82944 roll_sum=47.160 roll_mean=0.04605 pi_loss=-0.0049 v_loss=9.2385 ent=0.840 kl=0.0016 clip=0.029 eps_done=142 fps=729.7\n",
      "[rollout 82] step=83968 roll_sum=116.108 roll_mean=0.11339 pi_loss=0.0043 v_loss=29.5909 ent=0.730 kl=0.0027 clip=0.021 eps_done=143 fps=732.1\n",
      "[rollout 83] step=84992 roll_sum=-33.948 roll_mean=-0.03315 pi_loss=-0.0018 v_loss=17.2569 ent=0.670 kl=0.0054 clip=0.009 eps_done=145 fps=735.0\n",
      "[rollout 84] step=86016 roll_sum=249.068 roll_mean=0.24323 pi_loss=0.0105 v_loss=60.5596 ent=0.730 kl=0.0046 clip=0.035 eps_done=146 fps=737.9\n",
      "[rollout 85] step=87040 roll_sum=-113.907 roll_mean=-0.11124 pi_loss=-0.0022 v_loss=46.6371 ent=0.774 kl=0.0049 clip=0.021 eps_done=148 fps=740.7\n",
      "[rollout 86] step=88064 roll_sum=63.074 roll_mean=0.06160 pi_loss=-0.0013 v_loss=29.4688 ent=0.801 kl=0.0074 clip=0.029 eps_done=150 fps=743.3\n",
      "[rollout 87] step=89088 roll_sum=30.602 roll_mean=0.02988 pi_loss=-0.0021 v_loss=8.1100 ent=0.818 kl=0.0032 clip=0.008 eps_done=151 fps=746.0\n",
      "[rollout 88] step=90112 roll_sum=105.715 roll_mean=0.10324 pi_loss=-0.0001 v_loss=12.8781 ent=0.813 kl=0.0010 clip=0.001 eps_done=152 fps=748.6\n",
      "[rollout 89] step=91136 roll_sum=77.273 roll_mean=0.07546 pi_loss=-0.0009 v_loss=19.1458 ent=0.771 kl=0.0010 clip=0.002 eps_done=154 fps=751.2\n",
      "[rollout 90] step=92160 roll_sum=98.767 roll_mean=0.09645 pi_loss=-0.0004 v_loss=46.0509 ent=0.651 kl=0.0010 clip=0.002 eps_done=157 fps=753.8\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 166.55, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout90.pt\n",
      "[rollout 91] step=93184 roll_sum=92.854 roll_mean=0.09068 pi_loss=0.0001 v_loss=31.4714 ent=0.645 kl=0.0019 clip=0.007 eps_done=158 fps=742.4\n",
      "[rollout 92] step=94208 roll_sum=73.659 roll_mean=0.07193 pi_loss=-0.0060 v_loss=16.5786 ent=0.908 kl=0.0040 clip=0.038 eps_done=159 fps=745.0\n",
      "[rollout 93] step=95232 roll_sum=-2.444 roll_mean=-0.00239 pi_loss=-0.0002 v_loss=14.5366 ent=0.904 kl=0.0022 clip=0.034 eps_done=161 fps=747.4\n",
      "[rollout 94] step=96256 roll_sum=29.420 roll_mean=0.02873 pi_loss=-0.0031 v_loss=7.8227 ent=0.814 kl=0.0045 clip=0.025 eps_done=162 fps=750.0\n",
      "[rollout 95] step=97280 roll_sum=93.849 roll_mean=0.09165 pi_loss=-0.0013 v_loss=15.1434 ent=0.812 kl=-0.0002 clip=0.001 eps_done=165 fps=752.5\n",
      "[rollout 96] step=98304 roll_sum=113.427 roll_mean=0.11077 pi_loss=-0.0002 v_loss=26.4144 ent=0.723 kl=-0.0000 clip=0.002 eps_done=166 fps=754.9\n",
      "[rollout 97] step=99328 roll_sum=2.344 roll_mean=0.00229 pi_loss=-0.0030 v_loss=41.9408 ent=0.736 kl=0.0021 clip=0.016 eps_done=169 fps=757.1\n",
      "[rollout 98] step=100352 roll_sum=54.120 roll_mean=0.05285 pi_loss=-0.0019 v_loss=18.6803 ent=0.728 kl=0.0004 clip=0.009 eps_done=170 fps=759.5\n",
      "[rollout 99] step=101376 roll_sum=85.335 roll_mean=0.08333 pi_loss=0.0008 v_loss=26.9137 ent=0.702 kl=0.0016 clip=0.001 eps_done=172 fps=761.7\n",
      "[rollout 100] step=102400 roll_sum=15.011 roll_mean=0.01466 pi_loss=0.0011 v_loss=17.2991 ent=0.783 kl=0.0044 clip=0.023 eps_done=173 fps=763.7\n",
      "  [val] {'episode_reward': 43.61276155018033, 'episode_steps': 178.7, 'num_trades': 1.0, 'win_rate': 0.95, 'avg_trade_return': 43.12038937819849, 'avg_hold_steps': 177.7, 'sum_trade_return': 43.12038937819849}\n",
      "  [best] new best val episode_reward=43.6128 -> saves\\ppo_ppo_aapl_final_holdpen_2e-4_best.pt\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout100.pt\n",
      "[rollout 101] step=103424 roll_sum=53.680 roll_mean=0.05242 pi_loss=-0.0035 v_loss=15.3185 ent=0.783 kl=0.0064 clip=0.042 eps_done=176 fps=751.9\n",
      "[rollout 102] step=104448 roll_sum=34.965 roll_mean=0.03415 pi_loss=-0.0035 v_loss=21.9478 ent=0.734 kl=0.0043 clip=0.021 eps_done=180 fps=754.2\n",
      "[rollout 103] step=105472 roll_sum=26.486 roll_mean=0.02587 pi_loss=0.0008 v_loss=25.8715 ent=0.726 kl=0.0035 clip=0.011 eps_done=182 fps=756.6\n",
      "[rollout 104] step=106496 roll_sum=23.684 roll_mean=0.02313 pi_loss=0.0015 v_loss=21.1104 ent=0.714 kl=0.0034 clip=0.006 eps_done=183 fps=759.0\n",
      "[rollout 105] step=107520 roll_sum=112.241 roll_mean=0.10961 pi_loss=-0.0007 v_loss=28.2516 ent=0.872 kl=-0.0008 clip=0.008 eps_done=185 fps=761.1\n",
      "[rollout 106] step=108544 roll_sum=37.600 roll_mean=0.03672 pi_loss=-0.0038 v_loss=19.7911 ent=0.704 kl=-0.0001 clip=0.007 eps_done=186 fps=763.5\n",
      "[rollout 107] step=109568 roll_sum=-18.261 roll_mean=-0.01783 pi_loss=-0.0004 v_loss=16.1077 ent=0.696 kl=-0.0012 clip=0.004 eps_done=188 fps=765.5\n",
      "[rollout 108] step=110592 roll_sum=31.848 roll_mean=0.03110 pi_loss=0.0002 v_loss=18.8896 ent=0.700 kl=0.0004 clip=0.006 eps_done=190 fps=767.7\n",
      "[rollout 109] step=111616 roll_sum=-23.792 roll_mean=-0.02323 pi_loss=-0.0003 v_loss=15.1667 ent=0.677 kl=0.0006 clip=0.003 eps_done=191 fps=769.6\n",
      "[rollout 110] step=112640 roll_sum=-40.711 roll_mean=-0.03976 pi_loss=-0.0010 v_loss=17.5307 ent=0.705 kl=0.0040 clip=0.000 eps_done=193 fps=771.6\n",
      "  [val] {'episode_reward': 8.794474855844287, 'episode_steps': 204.7, 'num_trades': 0.25, 'win_rate': 0.25, 'avg_trade_return': 9.501774608028244, 'avg_hold_steps': 42.5, 'sum_trade_return': 9.501774608028244}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout110.pt\n",
      "[rollout 111] step=113664 roll_sum=61.662 roll_mean=0.06022 pi_loss=0.0018 v_loss=19.7315 ent=0.698 kl=0.0034 clip=0.005 eps_done=195 fps=758.7\n",
      "[rollout 112] step=114688 roll_sum=82.466 roll_mean=0.08053 pi_loss=0.0001 v_loss=16.3274 ent=0.713 kl=0.0009 clip=0.005 eps_done=196 fps=760.5\n",
      "[rollout 113] step=115712 roll_sum=53.384 roll_mean=0.05213 pi_loss=0.0019 v_loss=30.8079 ent=0.689 kl=0.0047 clip=0.008 eps_done=199 fps=762.5\n",
      "[rollout 114] step=116736 roll_sum=29.057 roll_mean=0.02838 pi_loss=-0.0026 v_loss=24.1333 ent=0.683 kl=0.0024 clip=0.017 eps_done=200 fps=764.7\n",
      "[rollout 115] step=117760 roll_sum=34.787 roll_mean=0.03397 pi_loss=-0.0002 v_loss=18.1596 ent=0.646 kl=0.0001 clip=0.001 eps_done=201 fps=766.6\n",
      "[rollout 116] step=118784 roll_sum=91.860 roll_mean=0.08971 pi_loss=0.0005 v_loss=19.1441 ent=0.707 kl=0.0009 clip=0.015 eps_done=203 fps=768.6\n",
      "[rollout 117] step=119808 roll_sum=-5.027 roll_mean=-0.00491 pi_loss=-0.0036 v_loss=5.4526 ent=0.659 kl=0.0086 clip=0.024 eps_done=204 fps=770.6\n",
      "[rollout 118] step=120832 roll_sum=-14.638 roll_mean=-0.01430 pi_loss=-0.0014 v_loss=19.5710 ent=0.671 kl=0.0050 clip=0.014 eps_done=207 fps=772.5\n",
      "[rollout 119] step=121856 roll_sum=1.060 roll_mean=0.00104 pi_loss=-0.0006 v_loss=14.1428 ent=0.580 kl=0.0025 clip=0.027 eps_done=208 fps=774.3\n",
      "[rollout 120] step=122880 roll_sum=-21.126 roll_mean=-0.02063 pi_loss=-0.0041 v_loss=21.2290 ent=0.635 kl=0.0054 clip=0.034 eps_done=210 fps=776.2\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 186.0, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout120.pt\n",
      "[rollout 121] step=123904 roll_sum=140.730 roll_mean=0.13743 pi_loss=0.0009 v_loss=31.1691 ent=0.538 kl=0.0002 clip=0.005 eps_done=211 fps=765.5\n",
      "[rollout 122] step=124928 roll_sum=175.916 roll_mean=0.17179 pi_loss=0.0133 v_loss=70.2938 ent=0.616 kl=0.0056 clip=0.014 eps_done=212 fps=767.4\n",
      "[rollout 123] step=125952 roll_sum=33.273 roll_mean=0.03249 pi_loss=0.0019 v_loss=23.9626 ent=0.757 kl=0.0017 clip=0.021 eps_done=214 fps=769.2\n",
      "[rollout 124] step=126976 roll_sum=5.004 roll_mean=0.00489 pi_loss=-0.0005 v_loss=14.7162 ent=0.625 kl=0.0010 clip=0.003 eps_done=215 fps=771.1\n",
      "[rollout 125] step=128000 roll_sum=21.500 roll_mean=0.02100 pi_loss=-0.0006 v_loss=11.8145 ent=0.723 kl=0.0011 clip=0.004 eps_done=217 fps=772.9\n",
      "[rollout 126] step=129024 roll_sum=32.609 roll_mean=0.03184 pi_loss=0.0012 v_loss=12.9753 ent=0.856 kl=-0.0003 clip=0.003 eps_done=220 fps=774.7\n",
      "[rollout 127] step=130048 roll_sum=37.422 roll_mean=0.03655 pi_loss=-0.0003 v_loss=15.4761 ent=0.853 kl=0.0021 clip=0.004 eps_done=221 fps=776.6\n",
      "[rollout 128] step=131072 roll_sum=163.637 roll_mean=0.15980 pi_loss=0.0119 v_loss=49.3464 ent=0.701 kl=-0.0011 clip=0.022 eps_done=222 fps=778.4\n",
      "[rollout 129] step=132096 roll_sum=-14.172 roll_mean=-0.01384 pi_loss=0.0041 v_loss=13.3776 ent=0.959 kl=0.0066 clip=0.043 eps_done=223 fps=780.0\n",
      "[rollout 130] step=133120 roll_sum=-37.981 roll_mean=-0.03709 pi_loss=0.0036 v_loss=13.9761 ent=0.825 kl=-0.0003 clip=0.026 eps_done=226 fps=781.7\n",
      "  [val] {'episode_reward': 0.0, 'episode_steps': 223.65, 'num_trades': 0.0, 'win_rate': 0.0, 'avg_trade_return': 0.0, 'avg_hold_steps': 0.0, 'sum_trade_return': 0.0}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout130.pt\n",
      "[rollout 131] step=134144 roll_sum=-32.741 roll_mean=-0.03197 pi_loss=-0.0070 v_loss=12.5436 ent=0.802 kl=0.0116 clip=0.139 eps_done=227 fps=769.0\n",
      "[rollout 132] step=135168 roll_sum=30.680 roll_mean=0.02996 pi_loss=0.0006 v_loss=8.3461 ent=0.799 kl=0.0035 clip=0.007 eps_done=229 fps=770.7\n",
      "[rollout 133] step=136192 roll_sum=2.756 roll_mean=0.00269 pi_loss=0.0010 v_loss=6.4852 ent=0.970 kl=0.0039 clip=0.010 eps_done=230 fps=772.3\n",
      "[rollout 134] step=137216 roll_sum=-33.879 roll_mean=-0.03308 pi_loss=-0.0008 v_loss=21.6295 ent=0.912 kl=0.0025 clip=0.029 eps_done=232 fps=773.9\n",
      "[rollout 135] step=138240 roll_sum=15.681 roll_mean=0.01531 pi_loss=0.0004 v_loss=8.9535 ent=0.793 kl=-0.0003 clip=0.010 eps_done=234 fps=775.6\n",
      "[rollout 136] step=139264 roll_sum=33.218 roll_mean=0.03244 pi_loss=-0.0020 v_loss=3.7523 ent=0.806 kl=0.0042 clip=0.015 eps_done=235 fps=777.2\n",
      "[rollout 137] step=140288 roll_sum=34.413 roll_mean=0.03361 pi_loss=-0.0049 v_loss=7.1128 ent=0.771 kl=0.0035 clip=0.138 eps_done=236 fps=778.8\n",
      "[rollout 138] step=141312 roll_sum=31.656 roll_mean=0.03091 pi_loss=0.0005 v_loss=16.5598 ent=0.713 kl=-0.0007 clip=0.006 eps_done=238 fps=780.5\n",
      "[rollout 139] step=142336 roll_sum=65.947 roll_mean=0.06440 pi_loss=0.0216 v_loss=35.1991 ent=0.750 kl=0.0087 clip=0.038 eps_done=240 fps=782.0\n",
      "[rollout 140] step=143360 roll_sum=-13.891 roll_mean=-0.01357 pi_loss=0.0069 v_loss=21.8519 ent=0.847 kl=0.0138 clip=0.159 eps_done=243 fps=783.7\n",
      "  [val] {'episode_reward': 40.56396402027519, 'episode_steps': 187.2, 'num_trades': 4.05, 'win_rate': 0.8083333333333333, 'avg_trade_return': 9.467062382127157, 'avg_hold_steps': 50.28666666666667, 'sum_trade_return': 40.81787459471025}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout140.pt\n",
      "[rollout 141] step=144384 roll_sum=-0.502 roll_mean=-0.00049 pi_loss=-0.0002 v_loss=8.1752 ent=0.789 kl=-0.0006 clip=0.000 eps_done=245 fps=772.3\n",
      "[rollout 142] step=145408 roll_sum=150.044 roll_mean=0.14653 pi_loss=0.0073 v_loss=42.4110 ent=0.793 kl=0.0039 clip=0.036 eps_done=248 fps=772.3\n",
      "[rollout 143] step=146432 roll_sum=44.505 roll_mean=0.04346 pi_loss=-0.0020 v_loss=12.4021 ent=0.748 kl=0.0017 clip=0.004 eps_done=250 fps=772.7\n",
      "[rollout 144] step=147456 roll_sum=15.655 roll_mean=0.01529 pi_loss=-0.0013 v_loss=13.1242 ent=0.885 kl=0.0084 clip=0.062 eps_done=251 fps=774.2\n",
      "[rollout 145] step=148480 roll_sum=10.391 roll_mean=0.01015 pi_loss=-0.0015 v_loss=20.1790 ent=0.775 kl=0.0028 clip=0.021 eps_done=254 fps=775.6\n",
      "[rollout 146] step=149504 roll_sum=28.162 roll_mean=0.02750 pi_loss=-0.0017 v_loss=7.6825 ent=0.787 kl=0.0037 clip=0.039 eps_done=255 fps=777.0\n",
      "[rollout 147] step=150528 roll_sum=189.858 roll_mean=0.18541 pi_loss=0.0124 v_loss=45.2798 ent=0.750 kl=0.0098 clip=0.067 eps_done=257 fps=778.3\n",
      "[rollout 148] step=151552 roll_sum=-2.092 roll_mean=-0.00204 pi_loss=0.0003 v_loss=41.3753 ent=0.728 kl=0.0041 clip=0.031 eps_done=259 fps=779.5\n",
      "[rollout 149] step=152576 roll_sum=67.556 roll_mean=0.06597 pi_loss=-0.0021 v_loss=13.9533 ent=0.676 kl=-0.0002 clip=0.005 eps_done=261 fps=780.9\n",
      "[rollout 150] step=153600 roll_sum=3.944 roll_mean=0.00385 pi_loss=-0.0022 v_loss=27.1260 ent=0.631 kl=0.0035 clip=0.011 eps_done=263 fps=782.4\n",
      "  [val] {'episode_reward': 27.874764915821338, 'episode_steps': 187.05, 'num_trades': 3.5, 'win_rate': 0.7008333333333333, 'avg_trade_return': 7.224314424171811, 'avg_hold_steps': 64.08666666666667, 'sum_trade_return': 28.004810431078663}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout150.pt\n",
      "[rollout 151] step=154624 roll_sum=-58.160 roll_mean=-0.05680 pi_loss=0.0001 v_loss=14.0571 ent=0.672 kl=0.0030 clip=0.041 eps_done=264 fps=771.8\n",
      "[rollout 152] step=155648 roll_sum=39.620 roll_mean=0.03869 pi_loss=-0.0001 v_loss=5.4738 ent=0.724 kl=0.0009 clip=0.011 eps_done=265 fps=773.2\n",
      "[rollout 153] step=156672 roll_sum=126.139 roll_mean=0.12318 pi_loss=0.0087 v_loss=54.7408 ent=0.847 kl=0.0048 clip=0.094 eps_done=267 fps=774.6\n",
      "[rollout 154] step=157696 roll_sum=34.273 roll_mean=0.03347 pi_loss=-0.0003 v_loss=10.3171 ent=0.704 kl=0.0011 clip=0.003 eps_done=268 fps=776.1\n",
      "[rollout 155] step=158720 roll_sum=41.165 roll_mean=0.04020 pi_loss=0.0016 v_loss=18.4336 ent=0.818 kl=0.0037 clip=0.017 eps_done=269 fps=777.7\n",
      "[rollout 156] step=159744 roll_sum=7.335 roll_mean=0.00716 pi_loss=0.0035 v_loss=10.3122 ent=0.890 kl=0.0179 clip=0.144 eps_done=272 fps=779.2\n",
      "[rollout 157] step=160768 roll_sum=40.978 roll_mean=0.04002 pi_loss=-0.0009 v_loss=9.2629 ent=0.837 kl=0.0009 clip=0.005 eps_done=273 fps=780.5\n",
      "[rollout 158] step=161792 roll_sum=79.550 roll_mean=0.07769 pi_loss=-0.0048 v_loss=9.0090 ent=0.838 kl=0.0036 clip=0.025 eps_done=277 fps=781.9\n",
      "[rollout 159] step=162816 roll_sum=50.569 roll_mean=0.04938 pi_loss=-0.0016 v_loss=7.5913 ent=0.744 kl=0.0003 clip=0.027 eps_done=280 fps=783.3\n",
      "[rollout 160] step=163840 roll_sum=46.562 roll_mean=0.04547 pi_loss=-0.0036 v_loss=21.9370 ent=0.749 kl=0.0041 clip=0.033 eps_done=282 fps=784.8\n",
      "  [val] {'episode_reward': 28.109367199188334, 'episode_steps': 201.4, 'num_trades': 3.35, 'win_rate': 0.7224999999999999, 'avg_trade_return': 7.36986271886259, 'avg_hold_steps': 70.5225, 'sum_trade_return': 28.20462815971059}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout160.pt\n",
      "[rollout 161] step=164864 roll_sum=7.815 roll_mean=0.00763 pi_loss=0.0046 v_loss=7.9615 ent=0.758 kl=0.0076 clip=0.037 eps_done=285 fps=775.1\n",
      "[rollout 162] step=165888 roll_sum=57.115 roll_mean=0.05578 pi_loss=0.0077 v_loss=12.8788 ent=0.771 kl=0.0022 clip=0.029 eps_done=286 fps=776.5\n",
      "[rollout 163] step=166912 roll_sum=-19.536 roll_mean=-0.01908 pi_loss=0.0004 v_loss=18.1972 ent=0.725 kl=0.0039 clip=0.034 eps_done=288 fps=778.0\n",
      "[rollout 164] step=167936 roll_sum=147.677 roll_mean=0.14422 pi_loss=0.0063 v_loss=38.6546 ent=0.701 kl=0.0102 clip=0.039 eps_done=289 fps=779.4\n",
      "[rollout 165] step=168960 roll_sum=169.769 roll_mean=0.16579 pi_loss=0.0023 v_loss=64.1585 ent=0.903 kl=-0.0012 clip=0.072 eps_done=291 fps=780.7\n",
      "[rollout 166] step=169984 roll_sum=22.132 roll_mean=0.02161 pi_loss=0.0045 v_loss=25.2973 ent=0.765 kl=0.0051 clip=0.047 eps_done=292 fps=782.2\n",
      "[rollout 167] step=171008 roll_sum=0.659 roll_mean=0.00064 pi_loss=0.0017 v_loss=11.9128 ent=0.914 kl=0.0026 clip=0.027 eps_done=293 fps=783.5\n",
      "[rollout 168] step=172032 roll_sum=-57.291 roll_mean=-0.05595 pi_loss=-0.0015 v_loss=18.7173 ent=0.736 kl=0.0073 clip=0.031 eps_done=295 fps=784.8\n",
      "[rollout 169] step=173056 roll_sum=200.133 roll_mean=0.19544 pi_loss=0.0085 v_loss=33.0168 ent=0.736 kl=0.0083 clip=0.073 eps_done=296 fps=786.1\n",
      "[rollout 170] step=174080 roll_sum=26.763 roll_mean=0.02614 pi_loss=0.0009 v_loss=6.5970 ent=0.772 kl=0.0025 clip=0.043 eps_done=298 fps=787.5\n",
      "  [val] {'episode_reward': 24.740948735333408, 'episode_steps': 199.1, 'num_trades': 2.45, 'win_rate': 0.7041666666666666, 'avg_trade_return': 9.065638706867041, 'avg_hold_steps': 95.475, 'sum_trade_return': 24.64144431022283}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout170.pt\n",
      "[rollout 171] step=175104 roll_sum=20.490 roll_mean=0.02001 pi_loss=-0.0006 v_loss=36.2549 ent=0.831 kl=0.0029 clip=0.035 eps_done=301 fps=778.2\n",
      "[rollout 172] step=176128 roll_sum=56.437 roll_mean=0.05511 pi_loss=-0.0014 v_loss=15.2036 ent=0.860 kl=0.0008 clip=0.006 eps_done=302 fps=779.4\n",
      "[rollout 173] step=177152 roll_sum=37.895 roll_mean=0.03701 pi_loss=-0.0020 v_loss=17.6524 ent=0.832 kl=0.0003 clip=0.002 eps_done=305 fps=780.8\n",
      "[rollout 174] step=178176 roll_sum=-42.162 roll_mean=-0.04117 pi_loss=-0.0026 v_loss=7.0278 ent=0.696 kl=0.0080 clip=0.060 eps_done=307 fps=782.2\n",
      "[rollout 175] step=179200 roll_sum=45.561 roll_mean=0.04449 pi_loss=-0.0024 v_loss=10.2676 ent=0.727 kl=0.0037 clip=0.042 eps_done=309 fps=783.4\n",
      "[rollout 176] step=180224 roll_sum=44.595 roll_mean=0.04355 pi_loss=0.0075 v_loss=25.5695 ent=0.538 kl=0.0034 clip=0.022 eps_done=311 fps=784.6\n",
      "[rollout 177] step=181248 roll_sum=23.230 roll_mean=0.02269 pi_loss=-0.0001 v_loss=26.3262 ent=0.758 kl=0.0102 clip=0.083 eps_done=312 fps=785.9\n",
      "[rollout 178] step=182272 roll_sum=-14.856 roll_mean=-0.01451 pi_loss=-0.0003 v_loss=15.4493 ent=0.549 kl=-0.0000 clip=0.002 eps_done=314 fps=787.2\n",
      "[rollout 179] step=183296 roll_sum=88.193 roll_mean=0.08613 pi_loss=0.0105 v_loss=22.7928 ent=0.558 kl=0.0099 clip=0.024 eps_done=315 fps=788.6\n",
      "[rollout 180] step=184320 roll_sum=-5.148 roll_mean=-0.00503 pi_loss=0.0087 v_loss=11.3657 ent=0.786 kl=0.0111 clip=0.152 eps_done=317 fps=789.8\n",
      "  [val] {'episode_reward': 23.363836910861504, 'episode_steps': 197.4, 'num_trades': 5.45, 'win_rate': 0.8001190476190476, 'avg_trade_return': 4.085280177479897, 'avg_hold_steps': 38.598214285714285, 'sum_trade_return': 23.888266473619716}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout180.pt\n",
      "[rollout 181] step=185344 roll_sum=96.314 roll_mean=0.09406 pi_loss=0.0074 v_loss=23.6630 ent=0.768 kl=0.0018 clip=0.059 eps_done=318 fps=781.7\n",
      "[rollout 182] step=186368 roll_sum=-40.642 roll_mean=-0.03969 pi_loss=0.0067 v_loss=13.6544 ent=0.682 kl=0.0156 clip=0.121 eps_done=320 fps=783.0\n",
      "[rollout 183] step=187392 roll_sum=-4.143 roll_mean=-0.00405 pi_loss=-0.0029 v_loss=10.4392 ent=0.810 kl=0.0057 clip=0.086 eps_done=322 fps=784.3\n",
      "[rollout 184] step=188416 roll_sum=148.403 roll_mean=0.14493 pi_loss=0.0197 v_loss=49.5903 ent=0.673 kl=0.0244 clip=0.099 eps_done=323 fps=785.6\n",
      "[rollout 185] step=189440 roll_sum=-33.712 roll_mean=-0.03292 pi_loss=0.0125 v_loss=10.0194 ent=0.873 kl=0.0246 clip=0.277 eps_done=325 fps=786.6\n",
      "[rollout 186] step=190464 roll_sum=101.311 roll_mean=0.09894 pi_loss=0.0193 v_loss=16.1878 ent=0.688 kl=0.0166 clip=0.107 eps_done=326 fps=787.7\n",
      "[rollout 187] step=191488 roll_sum=22.093 roll_mean=0.02158 pi_loss=-0.0004 v_loss=32.0819 ent=0.746 kl=0.0068 clip=0.078 eps_done=328 fps=788.9\n",
      "[rollout 188] step=192512 roll_sum=29.696 roll_mean=0.02900 pi_loss=-0.0016 v_loss=11.6693 ent=0.762 kl=0.0004 clip=0.031 eps_done=329 fps=790.1\n",
      "[rollout 189] step=193536 roll_sum=5.997 roll_mean=0.00586 pi_loss=-0.0010 v_loss=21.0540 ent=0.714 kl=0.0008 clip=0.005 eps_done=331 fps=791.3\n",
      "[rollout 190] step=194560 roll_sum=24.836 roll_mean=0.02425 pi_loss=-0.0006 v_loss=9.9735 ent=0.872 kl=0.0071 clip=0.065 eps_done=332 fps=792.4\n",
      "  [val] {'episode_reward': 26.905961642064845, 'episode_steps': 176.95, 'num_trades': 5.55, 'win_rate': 0.7693452380952382, 'avg_trade_return': 4.222690334645075, 'avg_hold_steps': 41.24374999999999, 'sum_trade_return': 27.455327467162014}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout190.pt\n",
      "[rollout 191] step=195584 roll_sum=146.175 roll_mean=0.14275 pi_loss=0.0375 v_loss=26.5407 ent=0.590 kl=0.0202 clip=0.102 eps_done=333 fps=785.5\n",
      "[rollout 192] step=196608 roll_sum=62.646 roll_mean=0.06118 pi_loss=0.0061 v_loss=48.7672 ent=0.814 kl=0.0094 clip=0.172 eps_done=335 fps=786.7\n",
      "[rollout 193] step=197632 roll_sum=4.128 roll_mean=0.00403 pi_loss=0.0095 v_loss=10.6918 ent=0.829 kl=0.0114 clip=0.154 eps_done=337 fps=787.8\n",
      "[rollout 194] step=198656 roll_sum=11.647 roll_mean=0.01137 pi_loss=-0.0033 v_loss=19.8267 ent=0.541 kl=0.0018 clip=0.029 eps_done=339 fps=789.0\n",
      "[rollout 195] step=199680 roll_sum=9.946 roll_mean=0.00971 pi_loss=0.0013 v_loss=10.8457 ent=0.596 kl=-0.0019 clip=0.010 eps_done=340 fps=790.1\n",
      "[rollout 196] step=200704 roll_sum=-21.642 roll_mean=-0.02114 pi_loss=0.0109 v_loss=9.5136 ent=0.789 kl=0.0211 clip=0.210 eps_done=342 fps=791.2\n",
      "[rollout 197] step=201728 roll_sum=34.569 roll_mean=0.03376 pi_loss=-0.0016 v_loss=17.3841 ent=0.668 kl=0.0036 clip=0.014 eps_done=344 fps=792.2\n",
      "[rollout 198] step=202752 roll_sum=19.171 roll_mean=0.01872 pi_loss=0.0014 v_loss=15.4061 ent=0.628 kl=0.0042 clip=0.043 eps_done=345 fps=793.3\n",
      "[rollout 199] step=203776 roll_sum=7.320 roll_mean=0.00715 pi_loss=-0.0008 v_loss=12.2623 ent=0.568 kl=0.0001 clip=0.021 eps_done=346 fps=794.4\n",
      "[rollout 200] step=204800 roll_sum=-35.364 roll_mean=-0.03454 pi_loss=0.0017 v_loss=14.1358 ent=0.548 kl=0.0122 clip=0.074 eps_done=348 fps=795.5\n",
      "  [val] {'episode_reward': 36.221204256578076, 'episode_steps': 181.15, 'num_trades': 4.15, 'win_rate': 0.7616666666666667, 'avg_trade_return': 8.263443036614303, 'avg_hold_steps': 46.28333333333334, 'sum_trade_return': 36.47903725487016}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout200.pt\n",
      "[rollout 201] step=205824 roll_sum=-3.361 roll_mean=-0.00328 pi_loss=0.0042 v_loss=24.3983 ent=0.501 kl=0.0081 clip=0.046 eps_done=351 fps=788.7\n",
      "[rollout 202] step=206848 roll_sum=-3.758 roll_mean=-0.00367 pi_loss=-0.0032 v_loss=6.5219 ent=0.597 kl=0.0117 clip=0.085 eps_done=352 fps=789.5\n",
      "[rollout 203] step=207872 roll_sum=244.581 roll_mean=0.23885 pi_loss=0.0324 v_loss=53.6356 ent=0.564 kl=0.0258 clip=0.128 eps_done=354 fps=790.8\n",
      "[rollout 204] step=208896 roll_sum=-26.135 roll_mean=-0.02552 pi_loss=0.0043 v_loss=7.2737 ent=0.600 kl=0.0031 clip=0.065 eps_done=355 fps=791.9\n",
      "[rollout 205] step=209920 roll_sum=1.681 roll_mean=0.00164 pi_loss=-0.0030 v_loss=9.3099 ent=0.791 kl=0.0046 clip=0.066 eps_done=357 fps=793.0\n",
      "[rollout 206] step=210944 roll_sum=28.205 roll_mean=0.02754 pi_loss=-0.0020 v_loss=10.2579 ent=0.763 kl=0.0050 clip=0.084 eps_done=359 fps=794.1\n",
      "[rollout 207] step=211968 roll_sum=32.060 roll_mean=0.03131 pi_loss=-0.0018 v_loss=9.5437 ent=0.699 kl=-0.0001 clip=0.004 eps_done=360 fps=795.2\n",
      "[rollout 208] step=212992 roll_sum=22.148 roll_mean=0.02163 pi_loss=-0.0010 v_loss=9.6212 ent=0.608 kl=0.0018 clip=0.005 eps_done=362 fps=796.3\n",
      "[rollout 209] step=214016 roll_sum=-37.513 roll_mean=-0.03663 pi_loss=-0.0025 v_loss=28.7067 ent=0.477 kl=-0.0003 clip=0.005 eps_done=363 fps=797.0\n",
      "[rollout 210] step=215040 roll_sum=43.598 roll_mean=0.04258 pi_loss=0.0009 v_loss=16.6492 ent=0.615 kl=0.0040 clip=0.020 eps_done=364 fps=797.9\n",
      "  [val] {'episode_reward': 37.6765482182315, 'episode_steps': 183.15, 'num_trades': 4.1, 'win_rate': 0.7308333333333333, 'avg_trade_return': 8.5778825799403, 'avg_hold_steps': 49.75833333333333, 'sum_trade_return': 37.91986993795733}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout210.pt\n",
      "[rollout 211] step=216064 roll_sum=193.132 roll_mean=0.18861 pi_loss=0.0207 v_loss=51.2502 ent=0.497 kl=0.0221 clip=0.070 eps_done=365 fps=790.9\n",
      "[rollout 212] step=217088 roll_sum=-15.728 roll_mean=-0.01536 pi_loss=0.0091 v_loss=14.2288 ent=0.703 kl=0.0141 clip=0.155 eps_done=367 fps=792.0\n",
      "[rollout 213] step=218112 roll_sum=88.187 roll_mean=0.08612 pi_loss=0.0094 v_loss=34.8595 ent=0.684 kl=0.0059 clip=0.066 eps_done=369 fps=792.9\n",
      "[rollout 214] step=219136 roll_sum=21.398 roll_mean=0.02090 pi_loss=0.0054 v_loss=12.0435 ent=0.569 kl=0.0109 clip=0.063 eps_done=370 fps=793.5\n",
      "[rollout 215] step=220160 roll_sum=-3.116 roll_mean=-0.00304 pi_loss=0.0009 v_loss=13.1218 ent=0.741 kl=0.0046 clip=0.014 eps_done=371 fps=793.9\n",
      "[rollout 216] step=221184 roll_sum=-9.275 roll_mean=-0.00906 pi_loss=0.0002 v_loss=15.8233 ent=0.534 kl=0.0014 clip=0.019 eps_done=373 fps=794.9\n",
      "[rollout 217] step=222208 roll_sum=70.223 roll_mean=0.06858 pi_loss=0.0026 v_loss=18.8163 ent=0.747 kl=0.0005 clip=0.013 eps_done=375 fps=795.6\n",
      "[rollout 218] step=223232 roll_sum=24.783 roll_mean=0.02420 pi_loss=0.0053 v_loss=15.6477 ent=0.730 kl=0.0185 clip=0.138 eps_done=376 fps=796.4\n",
      "[rollout 219] step=224256 roll_sum=24.632 roll_mean=0.02406 pi_loss=-0.0004 v_loss=8.0286 ent=0.487 kl=0.0019 clip=0.013 eps_done=378 fps=797.4\n",
      "[rollout 220] step=225280 roll_sum=-39.120 roll_mean=-0.03820 pi_loss=0.0074 v_loss=3.1817 ent=0.550 kl=0.0006 clip=0.012 eps_done=379 fps=798.3\n",
      "  [val] {'episode_reward': 23.416867445366023, 'episode_steps': 190.0, 'num_trades': 2.45, 'win_rate': 0.8083333333333333, 'avg_trade_return': 8.32055265928182, 'avg_hold_steps': 94.88333333333334, 'sum_trade_return': 23.316544038452452}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout220.pt\n",
      "[rollout 221] step=226304 roll_sum=188.463 roll_mean=0.18405 pi_loss=0.0190 v_loss=70.5847 ent=0.538 kl=0.0183 clip=0.048 eps_done=380 fps=791.7\n",
      "[rollout 222] step=227328 roll_sum=17.313 roll_mean=0.01691 pi_loss=0.0053 v_loss=7.9250 ent=0.722 kl=0.0072 clip=0.057 eps_done=381 fps=792.7\n",
      "[rollout 223] step=228352 roll_sum=37.704 roll_mean=0.03682 pi_loss=-0.0019 v_loss=12.5108 ent=0.734 kl=0.0015 clip=0.016 eps_done=383 fps=793.8\n",
      "[rollout 224] step=229376 roll_sum=94.947 roll_mean=0.09272 pi_loss=-0.0014 v_loss=9.8295 ent=0.684 kl=0.0017 clip=0.015 eps_done=386 fps=794.7\n",
      "[rollout 225] step=230400 roll_sum=-38.684 roll_mean=-0.03778 pi_loss=0.0015 v_loss=14.9820 ent=0.539 kl=0.0029 clip=0.029 eps_done=387 fps=795.7\n",
      "[rollout 226] step=231424 roll_sum=42.976 roll_mean=0.04197 pi_loss=-0.0001 v_loss=8.8373 ent=0.666 kl=0.0012 clip=0.008 eps_done=390 fps=796.7\n",
      "[rollout 227] step=232448 roll_sum=69.141 roll_mean=0.06752 pi_loss=0.0087 v_loss=47.9962 ent=0.627 kl=0.0071 clip=0.077 eps_done=392 fps=797.7\n",
      "[rollout 228] step=233472 roll_sum=-21.673 roll_mean=-0.02116 pi_loss=0.0067 v_loss=12.6970 ent=0.512 kl=0.0195 clip=0.111 eps_done=395 fps=798.7\n",
      "[rollout 229] step=234496 roll_sum=169.765 roll_mean=0.16579 pi_loss=0.0095 v_loss=50.5596 ent=0.364 kl=0.0081 clip=0.044 eps_done=397 fps=799.6\n",
      "[rollout 230] step=235520 roll_sum=11.738 roll_mean=0.01146 pi_loss=0.0012 v_loss=7.4163 ent=0.453 kl=-0.0008 clip=0.020 eps_done=398 fps=800.6\n",
      "  [val] {'episode_reward': 21.528297064489603, 'episode_steps': 183.0, 'num_trades': 2.1, 'win_rate': 0.7625, 'avg_trade_return': 9.073882893916693, 'avg_hold_steps': 95.59583333333333, 'sum_trade_return': 21.358220027713152}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout230.pt\n",
      "[rollout 231] step=236544 roll_sum=-30.168 roll_mean=-0.02946 pi_loss=-0.0030 v_loss=42.4023 ent=0.460 kl=0.0100 clip=0.064 eps_done=399 fps=794.4\n",
      "[rollout 232] step=237568 roll_sum=69.745 roll_mean=0.06811 pi_loss=0.0032 v_loss=25.2537 ent=0.413 kl=-0.0006 clip=0.011 eps_done=400 fps=795.3\n",
      "[rollout 233] step=238592 roll_sum=136.115 roll_mean=0.13292 pi_loss=0.0130 v_loss=41.0489 ent=0.429 kl=0.0085 clip=0.050 eps_done=402 fps=796.2\n",
      "[rollout 234] step=239616 roll_sum=58.583 roll_mean=0.05721 pi_loss=-0.0004 v_loss=14.8025 ent=0.493 kl=0.0012 clip=0.018 eps_done=403 fps=797.0\n",
      "[rollout 235] step=240640 roll_sum=83.235 roll_mean=0.08128 pi_loss=-0.0000 v_loss=19.9746 ent=0.531 kl=0.0011 clip=0.003 eps_done=404 fps=798.0\n",
      "[rollout 236] step=241664 roll_sum=-3.143 roll_mean=-0.00307 pi_loss=-0.0020 v_loss=24.3873 ent=0.435 kl=0.0087 clip=0.040 eps_done=406 fps=799.0\n",
      "[rollout 237] step=242688 roll_sum=18.129 roll_mean=0.01770 pi_loss=0.0005 v_loss=13.9872 ent=0.458 kl=0.0068 clip=0.059 eps_done=408 fps=799.9\n",
      "[rollout 238] step=243712 roll_sum=42.153 roll_mean=0.04116 pi_loss=0.0010 v_loss=25.9244 ent=0.420 kl=0.0017 clip=0.016 eps_done=411 fps=800.8\n",
      "[rollout 239] step=244736 roll_sum=84.840 roll_mean=0.08285 pi_loss=0.0081 v_loss=24.9617 ent=0.417 kl=0.0005 clip=0.019 eps_done=413 fps=801.5\n",
      "[rollout 240] step=245760 roll_sum=61.988 roll_mean=0.06054 pi_loss=0.0025 v_loss=20.2447 ent=0.350 kl=0.0136 clip=0.074 eps_done=418 fps=802.3\n",
      "  [val] {'episode_reward': 22.49019685228985, 'episode_steps': 210.95, 'num_trades': 1.35, 'win_rate': 0.85, 'avg_trade_return': 14.382149697491514, 'avg_hold_steps': 181.65, 'sum_trade_return': 22.126953451847633}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout240.pt\n",
      "[rollout 241] step=246784 roll_sum=65.952 roll_mean=0.06441 pi_loss=0.0002 v_loss=11.2714 ent=0.322 kl=0.0025 clip=0.015 eps_done=419 fps=795.4\n",
      "[rollout 242] step=247808 roll_sum=79.813 roll_mean=0.07794 pi_loss=-0.0019 v_loss=11.9226 ent=0.300 kl=0.0020 clip=0.018 eps_done=421 fps=796.2\n",
      "[rollout 243] step=248832 roll_sum=-10.616 roll_mean=-0.01037 pi_loss=0.0009 v_loss=14.6643 ent=0.273 kl=0.0010 clip=0.010 eps_done=422 fps=797.2\n",
      "[rollout 244] step=249856 roll_sum=45.046 roll_mean=0.04399 pi_loss=0.0004 v_loss=12.4693 ent=0.440 kl=0.0011 clip=0.007 eps_done=423 fps=797.1\n",
      "[rollout 245] step=250880 roll_sum=49.766 roll_mean=0.04860 pi_loss=-0.0014 v_loss=8.2606 ent=0.398 kl=0.0009 clip=0.006 eps_done=425 fps=797.6\n",
      "[rollout 246] step=251904 roll_sum=34.972 roll_mean=0.03415 pi_loss=-0.0037 v_loss=7.4069 ent=0.393 kl=-0.0020 clip=0.012 eps_done=428 fps=798.4\n",
      "[rollout 247] step=252928 roll_sum=192.198 roll_mean=0.18769 pi_loss=0.0045 v_loss=138.3007 ent=0.193 kl=0.0011 clip=0.002 eps_done=429 fps=799.3\n",
      "[rollout 248] step=253952 roll_sum=67.481 roll_mean=0.06590 pi_loss=-0.0005 v_loss=6.2595 ent=0.344 kl=0.0004 clip=0.003 eps_done=430 fps=800.2\n",
      "[rollout 249] step=254976 roll_sum=148.872 roll_mean=0.14538 pi_loss=-0.0015 v_loss=27.3537 ent=0.310 kl=0.0003 clip=0.012 eps_done=432 fps=801.1\n",
      "[rollout 250] step=256000 roll_sum=71.460 roll_mean=0.06979 pi_loss=-0.0004 v_loss=21.8315 ent=0.405 kl=0.0030 clip=0.023 eps_done=434 fps=801.9\n",
      "  [val] {'episode_reward': 28.240190836933277, 'episode_steps': 191.45, 'num_trades': 1.0, 'win_rate': 0.95, 'avg_trade_return': 27.698181199693646, 'avg_hold_steps': 190.45, 'sum_trade_return': 27.698181199693646}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout250.pt\n",
      "[rollout 251] step=257024 roll_sum=-67.624 roll_mean=-0.06604 pi_loss=-0.0017 v_loss=9.9241 ent=0.365 kl=-0.0007 clip=0.009 eps_done=436 fps=796.0\n",
      "[rollout 252] step=258048 roll_sum=-10.653 roll_mean=-0.01040 pi_loss=0.0007 v_loss=18.0208 ent=0.273 kl=0.0012 clip=0.004 eps_done=437 fps=796.9\n",
      "[rollout 253] step=259072 roll_sum=43.447 roll_mean=0.04243 pi_loss=0.0019 v_loss=17.1771 ent=0.116 kl=-0.0007 clip=0.001 eps_done=438 fps=797.8\n",
      "[rollout 254] step=260096 roll_sum=245.766 roll_mean=0.24001 pi_loss=0.0002 v_loss=149.6249 ent=0.035 kl=0.0007 clip=0.001 eps_done=441 fps=798.6\n",
      "[rollout 255] step=261120 roll_sum=96.950 roll_mean=0.09468 pi_loss=-0.0018 v_loss=12.2200 ent=0.301 kl=-0.0001 clip=0.009 eps_done=443 fps=799.4\n",
      "[rollout 256] step=262144 roll_sum=74.057 roll_mean=0.07232 pi_loss=-0.0025 v_loss=7.1669 ent=0.342 kl=-0.0002 clip=0.011 eps_done=444 fps=800.3\n",
      "[rollout 257] step=263168 roll_sum=63.551 roll_mean=0.06206 pi_loss=-0.0007 v_loss=92.6233 ent=0.152 kl=0.0011 clip=0.016 eps_done=447 fps=801.1\n",
      "[rollout 258] step=264192 roll_sum=15.225 roll_mean=0.01487 pi_loss=0.0036 v_loss=16.7235 ent=0.231 kl=0.0017 clip=0.006 eps_done=448 fps=801.8\n",
      "[rollout 259] step=265216 roll_sum=13.794 roll_mean=0.01347 pi_loss=-0.0012 v_loss=6.3805 ent=0.301 kl=0.0011 clip=0.005 eps_done=449 fps=802.6\n",
      "[rollout 260] step=266240 roll_sum=68.166 roll_mean=0.06657 pi_loss=-0.0010 v_loss=6.2608 ent=0.242 kl=0.0004 clip=0.007 eps_done=451 fps=803.4\n",
      "  [val] {'episode_reward': 19.302309381476018, 'episode_steps': 188.65, 'num_trades': 1.0, 'win_rate': 0.95, 'avg_trade_return': 18.802921175466288, 'avg_hold_steps': 187.65, 'sum_trade_return': 18.802921175466288}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout260.pt\n",
      "[rollout 261] step=267264 roll_sum=50.811 roll_mean=0.04962 pi_loss=-0.0010 v_loss=5.2679 ent=0.322 kl=0.0003 clip=0.014 eps_done=453 fps=798.0\n",
      "[rollout 262] step=268288 roll_sum=90.324 roll_mean=0.08821 pi_loss=-0.0016 v_loss=12.1329 ent=0.267 kl=-0.0004 clip=0.011 eps_done=455 fps=798.8\n",
      "[rollout 263] step=269312 roll_sum=-30.847 roll_mean=-0.03012 pi_loss=-0.0012 v_loss=22.6078 ent=0.189 kl=0.0007 clip=0.010 eps_done=457 fps=799.7\n",
      "[rollout 264] step=270336 roll_sum=54.134 roll_mean=0.05287 pi_loss=0.0001 v_loss=28.0296 ent=0.024 kl=-0.0005 clip=0.001 eps_done=458 fps=800.6\n",
      "[rollout 265] step=271360 roll_sum=55.638 roll_mean=0.05433 pi_loss=0.0008 v_loss=28.8435 ent=0.115 kl=-0.0004 clip=0.003 eps_done=461 fps=801.3\n",
      "[rollout 266] step=272384 roll_sum=-0.100 roll_mean=-0.00010 pi_loss=-0.0001 v_loss=15.3647 ent=0.003 kl=-0.0001 clip=0.000 eps_done=462 fps=802.1\n",
      "[rollout 267] step=273408 roll_sum=59.087 roll_mean=0.05770 pi_loss=0.0011 v_loss=12.4851 ent=0.162 kl=0.0003 clip=0.005 eps_done=463 fps=802.9\n",
      "[rollout 268] step=274432 roll_sum=66.757 roll_mean=0.06519 pi_loss=0.0001 v_loss=9.1227 ent=0.254 kl=0.0003 clip=0.005 eps_done=466 fps=803.7\n",
      "[rollout 269] step=275456 roll_sum=62.052 roll_mean=0.06060 pi_loss=0.0003 v_loss=7.9742 ent=0.052 kl=-0.0000 clip=0.004 eps_done=467 fps=804.6\n",
      "[rollout 270] step=276480 roll_sum=68.251 roll_mean=0.06665 pi_loss=-0.0017 v_loss=8.6220 ent=0.346 kl=-0.0005 clip=0.003 eps_done=468 fps=805.4\n",
      "  [val] {'episode_reward': 28.272154230035103, 'episode_steps': 197.3, 'num_trades': 1.0, 'win_rate': 1.0, 'avg_trade_return': 27.725068690867197, 'avg_hold_steps': 196.3, 'sum_trade_return': 27.725068690867197}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout270.pt\n",
      "[rollout 271] step=277504 roll_sum=21.901 roll_mean=0.02139 pi_loss=-0.0030 v_loss=22.4599 ent=0.266 kl=0.0006 clip=0.019 eps_done=470 fps=799.6\n",
      "[rollout 272] step=278528 roll_sum=44.538 roll_mean=0.04349 pi_loss=-0.0001 v_loss=8.2939 ent=0.196 kl=0.0013 clip=0.005 eps_done=471 fps=800.4\n",
      "[rollout 273] step=279552 roll_sum=5.134 roll_mean=0.00501 pi_loss=-0.0006 v_loss=8.7989 ent=0.243 kl=0.0005 clip=0.030 eps_done=472 fps=801.2\n",
      "[rollout 274] step=280576 roll_sum=40.294 roll_mean=0.03935 pi_loss=-0.0008 v_loss=6.2355 ent=0.281 kl=0.0013 clip=0.004 eps_done=473 fps=801.9\n",
      "[rollout 275] step=281600 roll_sum=103.764 roll_mean=0.10133 pi_loss=0.0012 v_loss=11.3765 ent=0.302 kl=0.0014 clip=0.017 eps_done=474 fps=802.8\n",
      "[rollout 276] step=282624 roll_sum=40.386 roll_mean=0.03944 pi_loss=0.0005 v_loss=11.2512 ent=0.155 kl=-0.0009 clip=0.008 eps_done=477 fps=803.6\n",
      "[rollout 277] step=283648 roll_sum=-0.536 roll_mean=-0.00052 pi_loss=-0.0007 v_loss=3.9820 ent=0.253 kl=0.0004 clip=0.002 eps_done=478 fps=804.4\n",
      "[rollout 278] step=284672 roll_sum=62.039 roll_mean=0.06059 pi_loss=-0.0006 v_loss=9.0773 ent=0.212 kl=0.0005 clip=0.012 eps_done=479 fps=805.2\n",
      "[rollout 279] step=285696 roll_sum=9.376 roll_mean=0.00916 pi_loss=0.0015 v_loss=6.9667 ent=0.151 kl=-0.0013 clip=0.002 eps_done=481 fps=806.0\n",
      "[rollout 280] step=286720 roll_sum=-20.751 roll_mean=-0.02026 pi_loss=0.0013 v_loss=3.4153 ent=0.226 kl=-0.0014 clip=0.009 eps_done=482 fps=806.7\n",
      "  [val] {'episode_reward': 26.37692832377176, 'episode_steps': 199.75, 'num_trades': 1.0, 'win_rate': 1.0, 'avg_trade_return': 25.8487714891297, 'avg_hold_steps': 198.75, 'sum_trade_return': 25.8487714891297}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout280.pt\n",
      "[rollout 281] step=287744 roll_sum=-16.025 roll_mean=-0.01565 pi_loss=0.0007 v_loss=4.4707 ent=0.157 kl=-0.0004 clip=0.000 eps_done=484 fps=801.1\n",
      "[rollout 282] step=288768 roll_sum=19.750 roll_mean=0.01929 pi_loss=-0.0000 v_loss=6.3045 ent=0.158 kl=0.0002 clip=0.000 eps_done=486 fps=801.9\n",
      "[rollout 283] step=289792 roll_sum=21.267 roll_mean=0.02077 pi_loss=0.0009 v_loss=6.5060 ent=0.254 kl=-0.0004 clip=0.006 eps_done=487 fps=802.6\n",
      "[rollout 284] step=290816 roll_sum=81.381 roll_mean=0.07947 pi_loss=-0.0015 v_loss=8.6315 ent=0.195 kl=0.0007 clip=0.002 eps_done=489 fps=803.3\n",
      "[rollout 285] step=291840 roll_sum=62.276 roll_mean=0.06082 pi_loss=-0.0015 v_loss=6.0654 ent=0.205 kl=0.0014 clip=0.007 eps_done=491 fps=804.1\n",
      "[rollout 286] step=292864 roll_sum=12.112 roll_mean=0.01183 pi_loss=0.0006 v_loss=3.9403 ent=0.187 kl=0.0000 clip=0.001 eps_done=492 fps=804.9\n",
      "[rollout 287] step=293888 roll_sum=-2.591 roll_mean=-0.00253 pi_loss=0.0003 v_loss=3.9170 ent=0.312 kl=-0.0004 clip=0.002 eps_done=494 fps=805.6\n",
      "[rollout 288] step=294912 roll_sum=-39.567 roll_mean=-0.03864 pi_loss=0.0003 v_loss=6.3726 ent=0.193 kl=-0.0004 clip=0.000 eps_done=495 fps=806.3\n",
      "[rollout 289] step=295936 roll_sum=73.694 roll_mean=0.07197 pi_loss=0.0018 v_loss=11.9216 ent=0.301 kl=0.0030 clip=0.022 eps_done=498 fps=807.0\n",
      "[rollout 290] step=296960 roll_sum=73.889 roll_mean=0.07216 pi_loss=0.0011 v_loss=7.0457 ent=0.209 kl=0.0002 clip=0.008 eps_done=500 fps=807.7\n",
      "  [val] {'episode_reward': 34.666373256084974, 'episode_steps': 184.05, 'num_trades': 1.25, 'win_rate': 1.0, 'avg_trade_return': 30.97846193863313, 'avg_hold_steps': 160.45, 'sum_trade_return': 34.14564073551146}\n",
      "  [save] saves\\ppo_ppo_aapl_final_holdpen_2e-4_rollout290.pt\n",
      "[rollout 291] step=297984 roll_sum=7.171 roll_mean=0.00700 pi_loss=0.0003 v_loss=5.4303 ent=0.079 kl=-0.0000 clip=0.000 eps_done=502 fps=802.6\n",
      "[rollout 292] step=299008 roll_sum=75.217 roll_mean=0.07345 pi_loss=0.0019 v_loss=7.9411 ent=0.270 kl=0.0008 clip=0.010 eps_done=504 fps=803.3\n",
      "[rollout 293] step=300032 roll_sum=15.552 roll_mean=0.01519 pi_loss=0.0006 v_loss=12.0026 ent=0.024 kl=0.0001 clip=0.000 eps_done=505 fps=803.7\n",
      "[rollout 294] step=301056 roll_sum=61.889 roll_mean=0.06044 pi_loss=0.0002 v_loss=7.1567 ent=0.222 kl=0.0017 clip=0.006 eps_done=506 fps=803.0\n",
      "[rollout 295] step=302080 roll_sum=83.770 roll_mean=0.08181 pi_loss=-0.0002 v_loss=8.9850 ent=0.262 kl=-0.0018 clip=0.012 eps_done=509 fps=801.8\n",
      "[rollout 296] step=303104 roll_sum=41.573 roll_mean=0.04060 pi_loss=0.0004 v_loss=5.1253 ent=0.252 kl=-0.0002 clip=0.005 eps_done=511 fps=801.8\n",
      "[rollout 297] step=304128 roll_sum=41.996 roll_mean=0.04101 pi_loss=0.0004 v_loss=6.4877 ent=0.238 kl=-0.0001 clip=0.003 eps_done=514 fps=802.5\n",
      "[rollout 298] step=305152 roll_sum=61.023 roll_mean=0.05959 pi_loss=-0.0003 v_loss=9.3259 ent=0.286 kl=0.0007 clip=0.005 eps_done=516 fps=803.1\n",
      "[rollout 299] step=306176 roll_sum=-20.125 roll_mean=-0.01965 pi_loss=-0.0005 v_loss=6.0680 ent=0.246 kl=0.0020 clip=0.010 eps_done=517 fps=803.8\n",
      "[rollout 300] step=307200 roll_sum=56.478 roll_mean=0.05515 pi_loss=-0.0006 v_loss=23.0515 ent=0.202 kl=0.0010 clip=0.002 eps_done=519 fps=804.6\n",
      "  [val] {'episode_reward': 26.13886437902472, 'episode_steps': 189.05, 'num_trades': 1.75, 'win_rate': 0.925, 'avg_trade_return': 15.604870695298052, 'avg_hold_steps': 124.85, 'sum_trade_return': 25.849215515762467}\n",
      "[PPO] early stopping: no val improvement for 20 validations.\n",
      "\n",
      "✓ Training completed!\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Cell 10: Load best model + final validation"
  },
  {
   "cell_type": "code",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-28T19:17:49.065528Z",
     "start_time": "2025-12-28T19:17:32.203454Z"
    }
   },
   "source": [
    "best_path = os.path.join(\"saves\", f\"ppo_{config['run_name']}_best.pt\")\n",
    "if os.path.exists(best_path):\n",
    "    state = torch.load(best_path, map_location=device)\n",
    "    model.load_state_dict(state)\n",
    "    model.to(device)\n",
    "    print(f\"Loaded best model from: {best_path}\")\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    final_val = validation_run_ppo(env_val, model, episodes=100, device=device, greedy=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Final Validation Results (100 episodes):\")\n",
    "    print(\"=\" * 60)\n",
    "    for k, v in final_val.items():\n",
    "        print(f\"  {k}: {v:.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "else:\n",
    "    print(f\"Best model checkpoint not found: {best_path}\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded best model from: saves\\ppo_ppo_aapl_final_holdpen_2e-4_best.pt\n",
      "\n",
      "============================================================\n",
      "Final Validation Results (100 episodes):\n",
      "============================================================\n",
      "  episode_reward: 30.8059\n",
      "  episode_steps: 187.9800\n",
      "  num_trades: 1.0000\n",
      "  win_rate: 0.9600\n",
      "  avg_trade_return: 30.4833\n",
      "  avg_hold_steps: 186.9800\n",
      "  sum_trade_return: 30.4833\n",
      "============================================================\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes\n",
    "\n",
    "- **TensorBoard**: View training metrics with `tensorboard --logdir runs/`\n",
    "- **Checkpoints**: Best model saved to `saves/ppo_{run_name}_best.pt`\n",
    "- **Configuration**: Adjust hyperparameters in the Configuration cell (Section 5)\n",
    "- **Early Stopping**: Enabled by default based on validation performance\n",
    "- **Data Split**: Chronological split prevents data leakage (recommended)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
